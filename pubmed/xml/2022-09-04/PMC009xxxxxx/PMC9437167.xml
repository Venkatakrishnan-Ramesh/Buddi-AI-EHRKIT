<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Ophthalmol Ther</journal-id><journal-id journal-id-type="iso-abbrev">Ophthalmol Ther</journal-id><journal-title-group><journal-title>Ophthalmology and Therapy</journal-title></journal-title-group><issn pub-type="ppub">2193-8245</issn><issn pub-type="epub">2193-6528</issn><publisher><publisher-name>Springer Healthcare</publisher-name><publisher-loc>Cheshire</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">35882767</article-id><article-id pub-id-type="pmc">PMC9437167</article-id><article-id pub-id-type="publisher-id">548</article-id><article-id pub-id-type="doi">10.1007/s40123-022-00548-1</article-id><article-categories><subj-group subj-group-type="heading"><subject>Original Research</subject></subj-group></article-categories><title-group><article-title>Generating Synthesized Ultrasound Biomicroscopy Images from Anterior Segment Optical Coherent Tomography Images by Generative Adversarial Networks for Iridociliary Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6129-8507</contrib-id><name><surname>Ye</surname><given-names>Hongfei</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Yuan</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Mao</surname><given-names>Kerong</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Yafu</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Hu</surname><given-names>Yiqian</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Yu</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Fei</surname><given-names>Ping</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Lyv</surname><given-names>Jiao</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Li</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Zhao</surname><given-names>Peiquan</given-names></name><address><email>zhaopeiquan@xinhuamed.com.cn</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7146-9138</contrib-id><name><surname>Zheng</surname><given-names>Ce</given-names></name><address><email>zhengce@xinhuamed.com.cn</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="GRID">grid.412987.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 0630 1330</institution-id><institution>Department of Ophthalmology, </institution><institution>Xinhua Hospital Affiliated to Shanghai Jiaotong University School of Medicine, </institution></institution-wrap>No. 1665, Kongjiang Road, Shanghai, 200092 China </aff></contrib-group><pub-date pub-type="epub"><day>26</day><month>7</month><year>2022</year></pub-date><pub-date pub-type="pmc-release"><day>26</day><month>7</month><year>2022</year></pub-date><pub-date pub-type="ppub"><month>10</month><year>2022</year></pub-date><volume>11</volume><issue>5</issue><fpage>1817</fpage><lpage>1831</lpage><history><date date-type="received"><day>18</day><month>5</month><year>2022</year></date><date date-type="accepted"><day>29</day><month>6</month><year>2022</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2022</copyright-statement><license><ali:license_ref specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License, which permits any non-commercial use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">http://creativecommons.org/licenses/by-nc/4.0/</ext-link>.</license-p></license></permissions><abstract id="Abs1"><sec><title>Introduction</title><p id="Par1">The aim of this study was to investigate the feasibility of generating synthesized ultrasound biomicroscopy (UBM) images from swept-source anterior segment optical coherent tomography (SS-ASOCT) images using a cycle-consistent generative adversarial network framework (CycleGAN) for iridociliary assessment on a cohort presenting for primary angle-closure screening.</p></sec><sec><title>Methods</title><p id="Par2">The CycleGAN architecture was adopted to synthesize high-resolution UBM images trained on the SS-ASOCT dataset from the department of ophthalmology, Xinhua Hospital. The performance of the CycleGAN model was further tested in two separate datasets using synthetic UBM images from two different ASOCT modalities (in-distribution and out-of-distribution). We compared the ability of glaucoma specialists to assess the image quality of real and synthetic images. UBM measurements, including anterior chamber, iridociliary parameters, were compared between real and synthetic UBM images. Intra-class correlation coefficients, coefficients of variation, and Bland&#x02013;Altman plots were used to assess the level of agreement. The Fr&#x000e9;chet Inception Distance (FID) was measured to evaluate the quality of the synthetic images.</p></sec><sec><title>Results</title><p id="Par3">The whole trained dataset included anterior chamber angle images, of which 4037 were obtained by SS-ASOCT and 2206 were obtained by UBM. The image quality of real versus synthetic SS-ASOCT images was similar as assessed by two glaucoma specialists. The Bland&#x02013;Altman analysis also suggested high consistency between measurements of real and synthetic UBM images. In addition, there was fair to excellent agreement between real and synthetic UBM measurements for the in-distribution dataset (ICC range 0.48&#x02013;0.97) and the out-of-distribution dataset (ICC range 0.52&#x02013;0.86). The FID was 21.3 and 24.1 for the synthetic UBM images from the in-distribution and out-of-distribution datasets, respectively.</p></sec><sec><title>Conclusion</title><p id="Par4">We developed a CycleGAN model to translate UBM images from non-contact SS-ASOCT images. The CycleGAN synthetic UBM images showed fair to excellent reproducibility when compared with real UBM images. Our results suggest that the CycleGAN technique is a promising tool to evaluate the iridociliary and anterior chamber in an alternative non-contact method.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Anterior segment optical coherence tomography</kwd><kwd>Deep learning</kwd><kwd>Generative adversarial networks</kwd><kwd>Ultrasound biomicroscopy</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>81371010</award-id><award-id>81770963</award-id><principal-award-recipient><name><surname>Zhao</surname><given-names>Peiquan</given-names></name><name><surname>Zheng</surname><given-names>Ce</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Hospital Funded Clinical Research, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine</institution></funding-source><award-id>21XJMR02</award-id><principal-award-recipient><name><surname>Zheng</surname><given-names>Ce</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Interdisciplinary Program of Shanghai Jiao Tong University</institution></funding-source><award-id>YG2021QN52</award-id><principal-award-recipient><name><surname>Ye</surname><given-names>Hongfei</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2022</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="FPar7"><title>Key Summary Points</title><p id="Par5">
<table-wrap id="Taba"><table frame="hsides" rules="groups"><tbody><tr><td align="left"><bold><italic>Why carry out this study?</italic></bold></td></tr><tr><td align="left">Ultrasound biomicroscopy (UBM) and anterior segment optical coherent tomography (ASOCT) are the most widely used instruments to objectively visualize and evaluate anterior segment parameters. Both demonstrate excellent repeatability and reproducibility, while each has its own specific limitations.</td></tr><tr><td align="left">A new technology that combines the advantages of both devices is necessary; such a tool might have the potential to become the gold standard method for measuring anterior segment parameters.</td></tr><tr><td align="left">We have previously shown that a generative adversarial network (GAN) framework of synthetic OCT images provides good quality images for clinical evaluation and can also be used for developing deep learning algorithms. Recently, the cycle-consistent generative adversarial network framework (CycleGAN) was introduced to generate images from different imaging modalities.</td></tr><tr><td align="left">This aim of this study was to investigate the feasibility of generating synthesized UBM from swept-source anterior segment optical coherent tomography (SS-ASOCT) using CycleGAN for iridociliary assessment.</td></tr><tr><td align="left"><bold><italic>What was learned from the study?</italic></bold></td></tr><tr><td align="left">Our results showed that there was good to excellent correlation of anterior segment parameters measured from the synthetic images and those from real UBM images.</td></tr><tr><td align="left">The CycleGAN-based deep learning technique provides a promising strategy to assess iridociliary using easy-to-use and non-contact methods.</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec1"><title>Introduction</title><p id="Par6">Precise ocular biometry is crucial for the diagnosis and treatment of ocular disorders. For anterior ocular biometric measurements, newer imaging instruments have been developed to objectively visualize and evaluate anterior segment parameters instead of traditional subjective techniques, such as slip-lamp examination and gonioscopy. Among these devices, ultrasound biomicroscopy (UBM) and anterior segment optical coherence tomography (ASOCT) are the most widely used imaging modalities. UBM allows high-resolution visualization of the anterior segment and angle structures at an ultrasonic frequency of 35&#x02013;100&#x000a0;MHz, providing additional information on the posterior chamber not otherwise available through clinical examination [<xref ref-type="bibr" rid="CR1">1</xref>]. ASOCT is a computerized imaging technology providing optical cross-sectional images of ocular structures, and when updated with the newer technology of swept-source OCT (SS-OCT), it provides a larger number of non-contact higher resolution images, more detailed information, and precise parameters of the angle, corneal, iris, and anterior chamber volumes; in addition, the fast scan rate effectively decreases motion artifact [<xref ref-type="bibr" rid="CR2">2</xref>].</p><p id="Par7">Research has demonstrated excellent repeatability and reproducibility of UBM and SS-ASOCT [<xref ref-type="bibr" rid="CR3">3</xref>&#x02013;<xref ref-type="bibr" rid="CR5">5</xref>]. However, despite their high value as diagnostic tools, both techniques have limitations. UBM is a contact, non-invasive method that takes a relatively longer time, and capturing diagnostic images and evaluation requires great skill and experience of the operator. SS-ASOCT is based on the optical principle. Since the examining beam cannot penetrate through the iris, certain structures in the posterior chamber, such as ciliary processes, zonule fibers, and cysts or intraocular foreign body behind the iris, can not be captured and imaged. Should a specific new technological method be developed that could combine the advantages of both devices, namely, a device that could automatically capture the non-contact high-resolution images of the anterior segment and angle structures with posterior chamber information, as well as unique anterior chamber parameters under rapid scan, it might have the potential to become the gold standard method for measuring the anterior segment parameters.</p><p id="Par8">The recent development of deep learning methods, especially domain adaption using generative adversarial networks (GANs) [<xref ref-type="bibr" rid="CR6">6</xref>], has attracted much interest in the field of medical imaging analysis. Several studies have demonstrated the capability of GANs to generate synthetic computed tomography (CT) or magnetic resonance (MRI) images of prostate [<xref ref-type="bibr" rid="CR7">7</xref>], liver [<xref ref-type="bibr" rid="CR8">8</xref>], brain [<xref ref-type="bibr" rid="CR9">9</xref>], and head and neck cancer [<xref ref-type="bibr" rid="CR10">10</xref>]. We have previously shown that GANs of synthetic OCT images have good quality for clinical evaluation and can also be used for developing deep learning algorithms [<xref ref-type="bibr" rid="CR11">11</xref>&#x02013;<xref ref-type="bibr" rid="CR13">13</xref>]. More recently, a cycle-consistent generative adversarial network framework (CycleGAN) was introduced to generate images from different imaging modalities. For example, several researchers have demonstrated a CycleGAN-based domain transfer between CT and MRI images [<xref ref-type="bibr" rid="CR14">14</xref>], traditional retinal fundus photographs, and ultra-widefield images [<xref ref-type="bibr" rid="CR15">15</xref>], among others. However, cross-modality image transfer between SS-ASOCT and UBM has not yet been reported. Inspired by this domain transfer, the aim of this study was to build a CycleGAN-based deep learning model for the domain transfer from SS-ASOCT to UBM. We also conducted experiments to demonstrate the effectiveness of the CycleGAN method by qualitatively and quantitatively measuring iridociliary parameters from synthetic UBM using this technique with an independent dataset.</p></sec><sec id="Sec2"><title>Methods</title><sec id="Sec3"><title>Study Design</title><p id="Par9">We retrospectively collected the development datasets from the PACS ((picture archiving and communication system) of the department of ophthalmology, Xinhua Hospital, Medicine School of Shanghai Jiaotong University. The raw development datasets consisted of 2314 SS-ASOCT images and 2417 UBM images from 163 and 612 patients, respectively, between 16 September 2020, and 10 December 2021 (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>a). All subjects had visited the glaucoma clinic to screen for primary angle-closure glaucoma, which is a major cause of blindness in Chinese population. We excluded patients with a prior history of trauma, intraocular tumor, intraocular surgery, and laser iridoplasty. Eyes with gross iris atrophy and uveitis were also excluded. Details of SS-ASOCT and UBM imaging have been described previously [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. In brief, SS-ASOCT (model CASIA2; Tomey, Nagoya, Japan) is a novel imaging modality which, compared to earlier generations of ASOCT, such as time-domain ASOCT (Visante OCT; Carl Zeiss Meditec, Dublin, CA, USA), has a faster scan speed (50,000 vs. 2000 A-scans/s), provides deeper imaging (11 vs. 8&#x000a0;mm), and has a higher resolution (2.4 vs. 10&#x000a0;&#x000b5;m). In the present study, SS-ASOCT (model CASIA2; Tomey Corp., Nagoya, Japan) images were obtained for all participants. A total of 128 two-dimensional cross-sectional SS-ASOCT images were acquired per scan. All UBM images were taken by UBM (model SW-3200L; Suoer Electronic Ltd., Tianjin, China). During the UBM examination, subjects lay supine under standardized dark conditions (illumination 60&#x02013;70&#x000a0;lx). The probe was then placed perpendicular to the ocular surface, and images of all four quadrants were obtained. A single operator imaged all subjects. Only those images were included in the analysis which clearly visualized the scleral spur, drainage angle, ciliary body (for UBM images only), and a half chord of the iris.<fig id="Fig1"><label>Fig. 1</label><caption><p>Schematic of image preprocessing, CycleGAN model development, and evaluation of synthetic UBM images.<italic> ACA</italic> Anterior chamber angle,<italic> AS-OCT</italic> anterior segment optical coherence tomography <italic> CycleGAN</italic> cycle-consistent generative adversarial network,<italic> UBM</italic> ultrasound biomicroscopy</p></caption><graphic xlink:href="40123_2022_548_Fig1_HTML" id="MO1"/></fig></p><p id="Par10">To evaluate the GAN model, we also collected two independent testing datasets as the in-distribution and out-of-distribution testing datasets, respectively. The in-distribution testing dataset included SS-ASOCT images from the same hospital between 1 March 2021 and 4 May 2021. The out-of-distribution testing datasets involved time-domain ASOCT images acquired at a different hospitals from our previous study. All subjects underwent an ophthalmic examination, including best-corrected visual acuity, refraction, slit-lamp examination, and gonioscopic anterior chamber angle (ACA) evaluation by a fellowship-trained glaucoma specialist (Goldmann 2-mirror lens; Haag-Streit AG, Bern, Switzerland). The inclusion and exclusion criteria were identical for the training and independent testing datasets. In the two testing datasets, ASOCT and UBM were performed in terms of the nasal and temporal angle.</p><p id="Par11">This study was approved by the Institutional Review Board of Xinhua Hospital, Medicine School of Shanghai Jiaotong University (identifier: XHEC-D-2021-114). The study was carried out in accordance with the ethical standards set by the Declaration of Helsinki of 1964 as amended in 2008. All SS-ASOCT and UBM images were anonymized and de-identified according to the Health Insurance Portability and Accountability Act Safe Harbor before analysis [<xref ref-type="bibr" rid="CR18">18</xref>]. Informed consent was exempted by the IRB in the retrospectively collected development and validation datasets. In the prospectively collected testing dataset, informed consent for publication was obtained from all enrolled patients or from their guardians.</p></sec><sec id="Sec4"><title>CycleGAN Architecture and Generation of Synthesized UBM Images</title><p id="Par12">To generate the synthesized UBM images, we adopted a CycleGANs model to translate images from the SS-ASOCT domain to UBM using unpaired data (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b). The details of the CycleGAN architecture have been previously described by Zhu et al. In brief, CycleGAN is a type of unsupervised machine learning technique used for mapping different image domains [<xref ref-type="bibr" rid="CR19">19</xref>]. The whole neural network of CycleGAN consists of two generator-discriminator networks. Figure&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>b shows the schematic of the CycleGAN model in the current study. The forward cycle learns a mapping of the first generator to translate SS-ASOCT images to synthetic UBM images and attempts to make synthetic UBM images that are as real as real UBM images to fool the discriminator. On the other hand, the backward cycle learns a mapping of the second generator to transform synthetic UBM images back into synthetic SS-ASOCT images, and to make synthetic SS-ASOCT images as real as the real SS-ASOCT images. This cycle consistency allows CycleGANs to capture the characteristics of two image domains and automatically learn how these characteristics should be translated to transfer the domains without any paired datasets [<xref ref-type="bibr" rid="CR20">20</xref>].</p><p id="Par13">We adopted the codes of CycleGANs from the Tensorflow tutorial webpage (<ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/tutorials/generative/CycleGANs">https://www.tensorflow.org/tutorials/generative/CycleGANs</ext-link>). Since each raw SS-ASOCT image contains two ACA regions, we split each image into two ACA images. Both SS-ASOCT and UBM ACA images were then rescaled to 256&#x02009;&#x000d7;&#x02009;256 pixels, normalized between&#x02009;&#x02212;&#x02009;1 and&#x02009;+&#x02009;1, and augmented by random flip, cropping, and contrast perturbations. For training, we used an Adam optimizer learning rate of 0.0002 and a batch size of 1, as described by the same webpage. The model was trained on the Kaggle platform (<ext-link ext-link-type="uri" xlink:href="http://www.kaggle.com">www.kaggle.com</ext-link>), a free cloud service for deep learning research. Kaggle cloud platform provides free access to NVIDIA TESLA P100 GPUs with 16&#x000a0;GB RAM.</p></sec><sec id="Sec5"><title>Evaluation of CycleGANs Synthetic UBM Images for Iridociliary Assessment</title><p id="Par14">We sought to determine whether the CycleGANs synthetic UBM images could be utilized to assess the iridociliary. First, a visual Turing test [<xref ref-type="bibr" rid="CR21">21</xref>] was performed to evaluate the image quality of synthetic UBM images. Then, we applied the CycleGAN to the independent testing dataset, and 30 synthetic UBM images were generated accordingly. All images were displayed on a laptop screen with 256&#x02009;&#x000d7;&#x02009;256 pixels. Two glaucoma specialists (HFY and YQH with more than 5 and 10&#x000a0;years of glaucoma experience, respectively) from the same center manually assessed the image quality of both the synthetic and real UBM images. To avoid confirmation bias, the glaucoma specialists were not told that there a</p><p id="Par15">were any synthetic images among the UBM images. We have devised an image quality grading scheme for evaluating the GAN synthetic ophthalmic images [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>] that includes: (1) visibility of the scleral spurs, which was defined as the point where there was a change in the curvature of the inner surface of the angle wall [<xref ref-type="bibr" rid="CR22">22</xref>]; (2) presence of continuity in the anterior segment structures, including iridociliary [<xref ref-type="bibr" rid="CR11">11</xref>]; and (3) the absence of motion artifacts [<xref ref-type="bibr" rid="CR23">23</xref>]. After image quality evaluation, two glaucoma specialists were asked to classify every image in that dataset as being either real or fake.</p><p id="Par16">To quantitively assess the synthetic UBM images in the in-distribution (SS-ASOCT) and out-of-distribution (time-domain ASOCT) datasets, imaging analysis in the testing dataset was further processed using q-customized software (Anterior Segment Analysis Program [ASAP] [<xref ref-type="bibr" rid="CR24">24</xref>]) by a single experienced observer (YW) who was masked to the clinical data. ASAP works as a plug-in of public domain software (ImageJ version 1.38x; public domain software: <ext-link ext-link-type="uri" xlink:href="http://imagej.nih.gov/ij">http://imagej.nih.gov/ij</ext-link>) and based on traditional image processing algorithms. The algorithm then automatically calculated the anterior angle and iridociliary parameters. The following parameters were measured as described previously: (1) ciliary body thickness at the point of the scleral spur (CT0) and at the distance of 1000 um (CT1000) from the scleral spur [<xref ref-type="bibr" rid="CR25">25</xref>]; (2) angle opening distance (AOD500) [<xref ref-type="bibr" rid="CR26">26</xref>], which was calculated as the perpendicular distance measured from the trabecular meshwork at 500 um anterior to the scleral spur as described previously; (3) iris thickness (IT500), measured at 500 &#x003bc;m from the scleral spur; (4) trabecular-ciliary process angle (TCA), measured as the angle between the corneal endothelium and superior surface of the ciliary process; and (4) trabecularciliary process distance (TCPD), defined as the length of a line 500 &#x003bc;m from the scleral spur extending from the corneal endothelium, perpendicular through the posterior surface of the iris, to the ciliary process (Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>c).</p><p id="Par17">To further measure the performance of the CycleGAN model, we chose the Fr&#x000e9;chet Inception Distance (FID) to evaluate the synthetic images. FID is a widely used metric for evaluating the distance between the distributions of synthetic data and real data by calculating the Wasserstein-2 distance in the feature space of an Inception-v3 network.</p></sec><sec id="Sec6"><title>Statistical Analysis</title><p id="Par18">The quality of the synthetic and real UBM images&#x02019; was graded by two glaucoma specialists, using Pearson&#x02019;s <italic>&#x003c7;</italic><sup>2</sup> test. To evaluate the agreement between the measurements of the devices, Bland&#x02013;Altman analysis (mean difference and limits of agreement [LoA]) was performed. The measurement correlations were calculated using intra-class correlation coefficients (ICCs) and coefficients of variation (CoVs). An ICC of&#x02009;&#x0003c;&#x02009;0.4 indicated poor reproducibility, ICC between 0.4 and 0.75 indicated fair to good reproducibility, and ICC&#x02009;&#x0003e;&#x02009;0.75 indicated excellent reproducibility. All statistical tests were performed using the SPSS version 26.0 software (SPSS IBM Corp., Armonk, NY, USA).</p></sec></sec><sec id="Sec7"><title>Results</title><p id="Par19">In this study, the CycleGAN model generated UBM images (pixel resolution of 256 &#x000d7; 256) using SS-ASOCT images. After image grading and preprocessing, the trained dataset included 4037 SS-ASOCT ACA and 2206 UBM ACA images. The processing time for one synthetic UBM image was approximately 1.1&#x000a0;s using a commercial laptop (Apple MacBook Air M1; Apple Inc., Cupertino, CA, USA). The examples of synthetic UBM images and corresponding SS-ASOCT images are shown in Fig.&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref>. Overall, our approach is capable of generating OCT images that are realistic. The results of UBM image quality grading by two glaucoma specialists are shown in Table <xref rid="Tab1" ref-type="table">1</xref>. Both glaucoma specialists graded synthetic UBM images as having approximately the same proportion of good quality images as the real UBM images (all <italic>p</italic>&#x02009;&#x0003e;&#x02009;0.05, Pearson Chi-square). Table <xref rid="Tab2" ref-type="table">2</xref> also summarizes the second part of the Turing test. It was notable that the accuracy for distinguishing between true and fake images was 56.7% and 60%, respectively, which is only slightly better than chance.<fig id="Fig2"><label>Fig. 2</label><caption><p>Examples of SS-ASOCT images (<bold>a</bold>, <bold>d</bold>), real UBM images (<bold>b</bold>, <bold>e</bold>), and synthetic UBM images (<bold>c</bold>, <bold>f</bold>).<italic> SS-ASOCT</italic> Swept-source anterior segment optical coherent tomography</p></caption><graphic xlink:href="40123_2022_548_Fig2_HTML" id="MO2"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption><p>Synthetic and real UBM images' quality grading by 2 glaucoma specialists</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">Synthetic UBM</th><th align="left">Real UBM</th><th align="left">p value</th></tr></thead><tbody><tr><td align="left" colspan="4"><bold>Glaucoma specialist 1</bold></td></tr><tr><td align="left">Visibility of the scleral spurs</td><td align="left">27 (90%)</td><td align="left">30 (100%)</td><td char="." align="char">0.076</td></tr><tr><td align="left">Continuity in anterior segment structures</td><td align="left">28 (93%)</td><td align="left">29 (97%)</td><td char="." align="char">0.554</td></tr><tr><td align="left">Absence of motion artifacts</td><td align="left">30 (100%)</td><td align="left">29 (97%)</td><td char="." align="char">0.313</td></tr><tr><td align="left" colspan="4"><bold>Glaucoma specialist 2</bold></td></tr><tr><td align="left">Visibility of the scleral spurs</td><td align="left">23 (77%)</td><td align="left">27 (90%)</td><td char="." align="char">0.166</td></tr><tr><td align="left">Continuity in anterior segment structures</td><td align="left">25 (83%)</td><td align="left">24 (80%)</td><td char="." align="char">0.739</td></tr><tr><td align="left">Absence of motion artifacts</td><td align="left">27 (90%)</td><td align="left">28 (93%)</td><td char="." align="char">0.64</td></tr></tbody></table><table-wrap-foot><p><italic>UBM</italic> Ultrasound biomicroscopy</p></table-wrap-foot></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption><p>Synthetic vs. real UBM images distinguished by 2 glaucoma specialists</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left">True positive ratio (%)</th><th align="left">False positive ratio (%)</th><th align="left">Accuracy (%)</th></tr></thead><tbody><tr><td align="left">Glaucoma specialist 1</td><td char="." align="char">60.0</td><td char="." align="char">46.7</td><td char="." align="char">56.7</td></tr><tr><td align="left">Glaucoma specialist 2</td><td char="." align="char">63.3</td><td char="." align="char">43.3</td><td char="." align="char">60.0</td></tr><tr><td align="left">Overal average</td><td char="." align="char">61.7</td><td char="." align="char">45.0</td><td char="." align="char">58.4</td></tr></tbody></table></table-wrap></p><p id="Par20">In the in-distribution (SS-ASOCT) dataset, there was fair to excellent agreement between the measurements of real and synthetic UBM images of the anterior chamber and of iridociliary parameters (Table <xref rid="Tab3" ref-type="table">3</xref>). ICC values were 0.74 (CT1000), 0.86 (CT0), 0.97 (AOD500), 0.48 (IT500), 0.81 (TCA) and 0.80 (TCPD). Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>a shows the Bland&#x02013;Altman plot for the anterior chamber parameter (AOD500). The mean difference was &#x02212;&#x000a0;0.06&#x000a0;mm at 95% LoA (&#x02212;&#x000a0;0.11 to &#x02212;&#x000a0;0.01&#x000a0;mm). The Bland&#x02013;Altman plots for iridociliary parameters CT0, CT1000, TCA, and TCPD are shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>b, c, e, and f, respectively; the mean difference was &#x02212;&#x000a0;0.10&#x000a0;mm (95% LoA &#x02212;&#x000a0;0.34 to 0.12), &#x02212;&#x000a0;0.01&#x000a0;mm (95% LoA &#x02212;&#x000a0;0.12 to 0.09), &#x02212;&#x000a0;5.1&#x000b0; (95% LoA &#x02212;&#x000a0;21.91&#x000b0; to 11.71&#x000b0;), and 0.02&#x000a0;mm (95% LoA; &#x02212;&#x000a0;0.22 to 0.25), respectively. Figure&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>d shows the Bland&#x02013;Altman plot for the iris parameter (IT500); the mean difference was &#x02212;&#x000a0;0.07&#x000a0;mm at 95% LoA of &#x02212;&#x000a0;0.20 to 0.07&#x000a0;mm (Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>; Table <xref rid="Tab3" ref-type="table">3</xref>). The CoVs were 7.7% (CT1000), 13.3% (CT0), 25.8% (AOD500), 16.5% (IT500), 8.4% (TCA), and 9.6% (TCPD).<table-wrap id="Tab3"><label>Table 3</label><caption><p>Comparison of anterior chamber angle and ciliary body measurements between synthetic and real ultrasound biomicroscopy images</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">UBM parameters<sup>a</sup></th><th align="left">ICC (95% CI)</th><th align="left">Mean difference (mean&#x02009;&#x000b1;&#x02009;SD)</th><th align="left">LoA</th><th align="left">CoV (%)</th></tr></thead><tbody><tr><td align="left">AOD500 (mm)</td><td align="left">0.97 (0.94 to 0.99)</td><td char="(" align="char">&#x02212;&#x000a0;0.06 (&#x02212;&#x000a0;0.08 to &#x02212;&#x000a0;0.05)</td><td align="left">&#x02212;&#x000a0;0.11 to &#x02212;&#x000a0;0.01</td><td char="." align="char">25.8</td></tr><tr><td align="left">CT0 (mm)</td><td align="left">0.86 (0.71 to 0.93)</td><td char="(" align="char">&#x02212;&#x000a0;0.10 (&#x02212;&#x000a0;0.15 to &#x02212;&#x000a0;0.05)</td><td align="left">&#x02212;&#x000a0;0.34 to 0.14</td><td char="." align="char">13.3</td></tr><tr><td align="left">CT1000 (mm)</td><td align="left">0.74 (0.52 to 0.87)</td><td char="(" align="char">&#x02212;&#x000a0;0.01 (&#x02212;&#x000a0;0.04 to 0.01)</td><td align="left">&#x02212;&#x000a0;0.12 to 0.09</td><td char="." align="char">7.7</td></tr><tr><td align="left">IT500 (mm)</td><td align="left">0.48 (&#x02212;&#x000a0;0.12 to 0.76)</td><td char="(" align="char">&#x02212;&#x000a0;0.07(&#x02212;&#x000a0;0.09 to &#x02212;&#x000a0;0.04)</td><td align="left">&#x02212;&#x000a0;0.20 to 0.07</td><td char="." align="char">16.5</td></tr><tr><td align="left">TCA (&#x000b0;)</td><td align="left">0.81 (0.59 to 0.91)</td><td char="(" align="char">&#x02212;&#x000a0;5.1 (&#x02212;&#x000a0;8.4 to &#x02212;&#x000a0;1.77)</td><td align="left">&#x02212;&#x000a0;21.91 to 11.71</td><td char="." align="char">8.4</td></tr><tr><td align="left">TCPD (mm)</td><td align="left">0.80 (0.56 to 0.90)</td><td char="(" align="char">0.02 (&#x02212;&#x000a0;0.22 to 0.25)</td><td align="left">&#x02212;&#x000a0;0.22 to 0.25</td><td char="." align="char">9.6</td></tr></tbody></table><table-wrap-foot><p><italic>CI</italic> Confidence interval,<italic> CoV</italic> coefficient of variance,<italic> ICC</italic> intra-class correlation coefficient,<italic> LoA</italic> limit of agreement,<italic> SD</italic> standard deviation </p><p><sup>a</sup>AOD500, Angle opening distance; CTO, ciliary body thickness at the point of the scleral spur; CT1000 ciliary body thickness at the distance of 1000 um from the scleral spur; IT500, iris thickness, measured at 500 &#x003bc;m from the scleral spur; TCA, trabecular-ciliary process angle; PCPD, trabecularciliary process distance</p></table-wrap-foot></table-wrap><fig id="Fig3"><label>Fig. 3</label><caption><p>Evaluation of agreement between real and synthetic UBM images measurements of anterior chamber and iridociliary parameters: Bland&#x02013;Altman plot for AOD500 (<bold>a</bold>), CT0 (<bold>b</bold>), CT1000 (<bold>c</bold>), IT500 (<bold>d</bold>), TCA (<bold>e</bold>), and TCPD (<bold>f</bold>).<italic> AOD500</italic> Angle opening distance,<italic> CTO</italic> ciliary body thickness at the point of the scleral spur,<italic> CT1000</italic> ciliary body thickness at the distance of 1000 um from the scleral spur,<italic> IT500</italic> iris thickness, measured at 500 &#x003bc;m from the scleral spur.<italic> TCA</italic> trabecular-ciliary process angle,<italic> PCPD</italic> trabecularciliary process distance; for more detail, see text</p></caption><graphic xlink:href="40123_2022_548_Fig3_HTML" id="MO3"/></fig></p><p id="Par21">In the out-of-distribution (time-domain ASOCT) dataset, the fair to excellent agreements were also observed between real and synthetic UBM image measurements of the anterior chamber and iridociliary parameters (Table <xref rid="Tab4" ref-type="table">4</xref>). ICC values were 0.70 (CT1000), 0.82 (CT0), 0.86 (AOD500), 0.52 (IT500), 0.73 (TCA), and 0.81 (TCPD).<table-wrap id="Tab4"><label>Table 4</label><caption><p>Repeatability of anterior chamber angle and ciliary body measurements between synthetic and real ultrasound biomicroscopy images from he out-of-distribution dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left">UBM parameters</th><th align="left">ICC (95% CI)</th></tr></thead><tbody><tr><td align="left">AOD500 (mm)</td><td align="left">0.86 (0.71&#x02013;0.93)</td></tr><tr><td align="left">CT0 (mm)</td><td align="left">0.82 (0.68&#x02013;0.96)</td></tr><tr><td align="left">CT1000 (mm)</td><td align="left">0.70 (0.54&#x02013;0.86)</td></tr><tr><td align="left">IT500 (mm)</td><td align="left">0.52 (0.34&#x02013;0.70)</td></tr><tr><td align="left">TCA (&#x000b0;)</td><td align="left">0.73 (0.57&#x02013;0.89)</td></tr><tr><td align="left">TCPD (mm)</td><td align="left">0.81 (0.67&#x02013;0.95)</td></tr></tbody></table></table-wrap></p><p id="Par22">As no previous data are available on the question addressed in our study, we were unable to determine whether the FID measured in our experiment was good or not. Therefore, we added the synthetic ASOCT images from a precious study performed by our group as reference. Relative to calculations based on the real UBM images, the FID was 21.3, 24.1, and 102.8 for synthetic UBM images from the in-distribution dataset, synthetic UBM images from out-of-distribution dataset, and synthetic ASOCT images, respectively (Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>). It was noted that the FIDs obtained from in-distribution and out-of-distribution UBM images were much smaller than those obtained by synthetic images from different anterior chamber imaging modalities.<fig id="Fig4"><label>Fig. 4</label><caption><p>FID for synthetic UBM images from in-distribution dataset, synthetic UBM images from out-of-distribution dataset, and the synthetic ASOCT images, respectively.<italic> FID</italic> Fr&#x000e9;chet Inception Distance</p></caption><graphic xlink:href="40123_2022_548_Fig4_HTML" id="MO4"/></fig></p><p id="Par23">We also noted several cases of failure, as shown in Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref>. If an input SS-ASOCT image shows an open angle with shallow anterior chamber, the CycleGAN would not be able to generate angle structure correctly.<fig id="Fig5"><label>Fig. 5</label><caption><p>Sample with failed UBM image generation</p></caption><graphic xlink:href="40123_2022_548_Fig5_HTML" id="MO5"/></fig></p></sec><sec id="Sec8"><title>Discussion</title><p id="Par24">In this study, we proposed a CycleGAN-based deep learning technique for generating UBM images using SS-ASOCT images as the inputs. Our experiments suggest that realistic synthetic UBM images can be generated from SS-ASOCT images using CycleGANs. Moreover, the CycleGAN synthetic UBM images can reveal iridociliary structure. The measurements of iridociliary parameters from the CycleGAN synthetic UBM images showed fair to excellent reproducibility compared with those from the real UBM images. Although the measurements of anterior segment parameters obtained with real and synthetic UBM images cannot be considered interchangeable, there is fair to excellent correlation between them. The potential application of this technique is promising, as most clinical researchers still use UBM as the angle image modality in diagnosing angle-closure.</p><p id="Par25">Assessment of angle structure is an essential part of diagnosing and determining the management of individuals with angle-closure [<xref ref-type="bibr" rid="CR17">17</xref>, <xref ref-type="bibr" rid="CR27">27</xref>]. UBM allows visualization of the anterior segment and angle structures at an ultrasonic frequency of 35&#x02013;100&#x000a0;MHz, providing high-resolution (50&#x000a0;&#x003bc;m) images of the angle and iridociliary, which adds valuable information regarding causal mechanisms of angle-closure [<xref ref-type="bibr" rid="CR28">28</xref>]. UBM is characterized to assess the pathologies behind the iris, such as, for example, plateau iris, lens-induced glaucoma, ciliary block, cysts, and solid tumors of the anterior segment. On the other hand, SS-ASOCT has a fast scan speed (50,000 A-scans/s), allows deep imaging (11&#x000a0;mm), and has a higher resolution (2.4&#x000a0;&#x000b5;m); in addition, it provides a simple, user-friendly, and non-contact method of assessing the angle structure and providing more detailed information and precise parameters of the angle and corneal, iris, and anterior chamber volumes. The fast scan rate of SS-ASOCT effectively decreases motion artifacts [<xref ref-type="bibr" rid="CR2">2</xref>]. Unfortunately, SS-ASOCT cannot visualize structures posterior to the iris [<xref ref-type="bibr" rid="CR29">29</xref>], which precludes assessment of the impact of the iridociliary in some angle-closure mechanisms, such as plateau iris which is more common in the Chinese population. Therefore, it would be clinically advantageous to develop a diagnostic modality in which these two anterior segment imaging modalities are integrated. For example, Kwon et al. assessed eyes with primary angle-closure (PAC) using both SS-ASOCT and UBM [<xref ref-type="bibr" rid="CR17">17</xref>]. Based on their results, these authors suggested that by using UBM, clinicians may obtain more clues on the mechanisms of PAC. Moreover, prior SS-ASOCT data can be transferred to UBM and further combined into an anterior segment dataset for medical follow-up, clinical research, and deep learning analysis.</p><p id="Par26">GANs offer a novel method to generate new medical images. Recent studies suggest possible applications of generative methods for retinal image registration and fundus or optical coherence tomography image denoising [<xref ref-type="bibr" rid="CR30">30</xref>]. Traditional GANs require a large training dataset to synthesize realistic medical images. For example, in our previous studies [<xref ref-type="bibr" rid="CR11">11</xref>], we proposed a GAN approach to generate realistic ASOCT images using more than 20,000 ASOCT images. Moreover, traditional GAN models, such as progressively grown GANs or deep convolutional GANs, can only work in images from the same domain. To synthesize medical images from different domains, several generative networks have been suggested, including unsupervised GAN models, such as CycleGAN, and supervised networks, such as Pix2Pix GAN. Pix2Pix techniques have shown good performance in image translation settings. However, the critical shortage of paired dataset restricts the real application of supervised GANs. UBM requires a contact immersion scanning technique that is run by skilled operators. Therefore, it is difficult to collect a sufficiently large paired dataset for supervised GAN training, which is also a common issue in medical imaging research. In addition, it is inevitable that the body or eyes will move during different scanning procedures, such as CT and MRI, ASOCT and UBM, which make it a challenge to match or register two different domain images. CycleGAN is a type of unsupervised machine learning technique with the significant advantage of being able to utilize the unpaired dataset with two different domains. Recently, several researchers have demonstrated a CycleGAN-based domain transfer between different image modalities. For example, Li et al. proposed a CycleGAN architecture to synthesize MRI images from brain CT images for MR-guided radiotherapy [<xref ref-type="bibr" rid="CR31">31</xref>]. In another study, Muhaddisa et al. reported an effective domain adaptation method based on CycleGANs to map MR images from different datasets taken from different hospitals using different scanners and settings [<xref ref-type="bibr" rid="CR32">32</xref>]. Their technique effectively enlarged the MRI dataset, and the proposed scheme was shown to achieve good diagnostic performance for predicting molecular subtypes in low-grade gliomas. Using a similar CycleGAN model, Yoo et al. showed this technique can synthesize traditional fundus photography images directly from ultra-widefield fundus photography without a manual pre-conditioning process [<xref ref-type="bibr" rid="CR15">15</xref>]. However, cross-modality domain transfer between SS-ASOCT and UBM is challenging due to the high variability of angle tissue or structure appearance caused by different imaging mechanisms [<xref ref-type="bibr" rid="CR33">33</xref>]. In the current study, we built a CycleGAN-based deep learning model for the domain transfer from SS-ASOCT to UBM. Our result was encouraging in that we found that the CycleGAN model can generate realistic UBM images of high quality, as assessed by human experts in our Turing test.</p><p id="Par27">Despite the above, our study has a few limitations. First, our CycleGAN model can only generate UBM images with 256 &#x000d7; 256 pixel resolution, which is lower than that of the Casia2 SS-ASOCT system (2129 &#x000d7; 1464 pixel resolution) and the Suoer UBM (1024 &#x000d7; 655 pixel resolution) system. For the evaluation of some small lesions, such as ciliary body cysts or tumors, a higher resolution may be needed. As only part of the iris and lens were synthesized in the UBM image, we also could not assess the reproducibility of some lens or iris parameters (lens vault or iris convexity). On the other hand, some angle structure, such as iris thickness measured at 500 &#x003bc;m from the scleral spur, only has a length of a few pixels which could cause lower ICC in our study. The aim of the present study was to build a CycleGAN-based deep learning model for the domain transfer from SS-ASOCT to UBM. It is possible to generate higher resolutions (e.g., 1024&#x02009;&#x000d7;&#x02009;1024 or above), as reported in our previous study [<xref ref-type="bibr" rid="CR12">12</xref>]. Future work will involve generating small lesions or angle structures, which might help improve the clinical application of GAN models. Second, most of the subjects in our testing datasets had an open-angle. Other angle-closure mechanisms, like pupil block, plateau iris configuration, thick peripheral iris roll, and exaggerated lens vault, were not evaluated in the current study [<xref ref-type="bibr" rid="CR34">34</xref>]. Generating the iridociliary for different angle-closure mechanisms might help improve our understanding of angle-closure. Third, although we assessed the generalizability of the CycleGAN model by testing it within independent external datasets, the whole dataset is collected from the same center, using the same devices, following the same process. This will result in the method being vulnerable to the domain shift and will also cause over-optimism about the results. The latest module of SS-ASOCT, CASIA2, is only available in very few centers in mainland China, which prevented us from collecting more data from other centers. The application of the CycleGAN technique requires further validation in multi-center and multi-ethnic trials. Fourth, the CycleGAN model does not work in a pairwise fashion. For paired image-to-image translation (i.e., ASOCT images to UBM images with the same corresponding clock-hour position), Pix2Pix architecture (a supervised GAN) is the better model to learn a mapping from input images to corresponding output images. However, supervised GAN needs paired datasets that are not available in the current study. Overlapped examinations of both SS-ASOCT and UBM may cost the patients more and increase the use of human resources. The objective of this study was to evaluate the feasibility of CycleGANs to generate synthesized UBM images from SS-ASOCT images. The real-world applications of UBM include tumors, trauma, and surgical complications, all of which cannot be easily synthesized using the CycleGAN model. Future work is need to evaluate the iridociliary and angle pathologies using other GAN techniques, such as meta-learning-based GANs or semi-supervised GANs, by collecting pairs of ASOCT and UBM images of different pathologies.</p></sec><sec id="Sec9"><title>Conclusion</title><p id="Par28">In conclusion, we developed a CycleGAN model for generating UBM images using SS-ASOCT images as the inputs. Our preliminary results showed that there has fair to excellent correlation between anterior segment parameters measured from the synthetic and real UBM images. The CycleGAN-based deep learning technique presents a promising way to assess the iridociliary with non-contact methods.</p></sec></body><back><ack><title>Acknowledgements</title><sec id="d32e1135"><title>Funding</title><p id="Par29">This study was supported by the National Natural Science Foundation of China (81371010, 81770963), Hospital Funded Clinical Research, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine (21XJMR02), and Interdisciplinary Program of Shanghai Jiao Tong University (YG2021QN52). No funding or sponsorship was received for publication of this article. The journal&#x02019;s Rapid Service Fee was funded by the authors.</p></sec><sec id="d32e1140"><title>Authorship</title><p id="Par30">All named authors meet the International Committee of Medical Journal Editors (ICMJE) criteria for authorship for this article, take responsibility for the integrity of the work as a whole, and have given their approval for this version to be published.</p></sec><sec id="d32e1145"><title>Author Contributions</title><p id="Par31">Hongfei Ye and Ce Zheng conceived and designed the analysis. Hongfei Ye, Yuan Yang, Kerong Mao, and Yafu Wang collected the data. Yiqian Hu, Yu Xu, Ping Fei, Jiao Lyv, Li Chen, and Ce Zheng contributed data. Ce Zheng performed the analysis. Hongfei Ye and Ce Zheng wrote the paper. Peiquan Zhao and Ce Zheng did the final editing and approved the final version. All authors agree to be accountable for the content of the work.</p></sec><sec id="d32e1150"><title>Disclosures</title><p id="Par32">All authors have nothing to disclose and declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec id="d32e1155"><title>Compliance with Ethics Guidelines</title><p id="Par33">This study was approved by the Institutional Review Board of Xinhua Hospital, Medicine School of Shanghai Jiaotong University (identifier: XHEC-D-2021-114). The study was carried out in accordance with the ethical standards set by the Declaration of Helsinki of 1964 as amended in 2008. All SS-ASOCT and UBM images were anonymized and de-identified according to the Health Insurance Portability and Accountability Act Safe Harbor before analysis [<xref ref-type="bibr" rid="CR18">18</xref>]. Informed consent was exempted by the IRB in the retrospectively collected development and validation datasets. In the prospectively collected testing dataset, informed consent for publication was obtained from all enrolled patients or from their guardians.</p></sec><sec id="d32e1163" sec-type="data-availability"><title>Data Availability</title><p id="Par34">The datasets generated during and/or analyzed during the current study are available from the corresponding author on reasonable request.</p></sec></ack><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silverman</surname><given-names>RH</given-names></name></person-group><article-title>High-resolution ultrasound imaging of the eye&#x02014;a review</article-title><source>Clin Exp Ophthalmol</source><year>2009</year><volume>37</volume><issue>1</issue><fpage>54</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1111/j.1442-9071.2008.01892.x</pub-id><pub-id pub-id-type="pmid">19138310</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ang</surname><given-names>M</given-names></name><name><surname>Baskaran</surname><given-names>M</given-names></name><name><surname>Werkmeister</surname><given-names>RM</given-names></name><etal/></person-group><article-title>Anterior segment optical coherence tomography</article-title><source>Prog Retin Eye Res</source><year>2018</year><volume>66</volume><fpage>132</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/j.preteyeres.2018.04.002</pub-id><pub-id pub-id-type="pmid">29635068</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chansangpetch</surname><given-names>S</given-names></name><name><surname>Nguyen</surname><given-names>A</given-names></name><name><surname>Mora</surname><given-names>M</given-names></name><etal/></person-group><article-title>Agreement of anterior segment parameters obtained from swept-source fourier-domain and time-domain anterior segment optical coherence tomography</article-title><source>Invest Ophthalmol Vis Sci</source><year>2018</year><volume>59</volume><issue>3</issue><fpage>1554</fpage><lpage>1561</lpage><pub-id pub-id-type="doi">10.1167/iovs.17-23574</pub-id><pub-id pub-id-type="pmid">29625479</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montes-Mico</surname><given-names>R</given-names></name><name><surname>Pastor-Pascual</surname><given-names>F</given-names></name><name><surname>Ruiz-Mesa</surname><given-names>R</given-names></name><name><surname>Tana-Rivero</surname><given-names>P</given-names></name></person-group><article-title>Ocular biometry with swept-source optical coherence tomography</article-title><source>J Cataract Refract Surg</source><year>2021</year><volume>47</volume><issue>6</issue><fpage>802</fpage><lpage>814</lpage><pub-id pub-id-type="doi">10.1097/j.jcrs.0000000000000551</pub-id><pub-id pub-id-type="pmid">33315731</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Q</given-names></name><name><surname>Jin</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name></person-group><article-title>Repeatability, reproducibility, and agreement of central anterior chamber depth measurements in pseudophakic and phakic eyes: optical coherence tomography versus ultrasound biomicroscopy</article-title><source>J Cataract Refract Surg</source><year>2010</year><volume>36</volume><issue>6</issue><fpage>941</fpage><lpage>946</lpage><pub-id pub-id-type="doi">10.1016/j.jcrs.2009.12.038</pub-id><pub-id pub-id-type="pmid">20494765</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D. Generative adversarial nets. In: Ghahramani Z, Welling M, Cortes C, Lawrence N, Weinberger KQ, editors. Advances in neural information processing systems 27 (NIPS 2014), 8&#x02013;13 Dec 2014, Montreal.</mixed-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Qin</surname><given-names>A</given-names></name><name><surname>Zhou</surname><given-names>D</given-names></name><name><surname>Yan</surname><given-names>D</given-names></name></person-group><article-title>Technical Note: U-net-generated synthetic CT images for magnetic resonance imaging-only prostate intensity-modulated radiation therapy treatment planning</article-title><source>Med Phys</source><year>2018</year><volume>45</volume><issue>12</issue><fpage>5659</fpage><lpage>5665</lpage><pub-id pub-id-type="doi">10.1002/mp.13247</pub-id><pub-id pub-id-type="pmid">30341917</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Lei</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><etal/></person-group><article-title>MRI-based treatment planning for liver stereotactic body radiotherapy: validation of a deep learning-based synthetic CT generation method</article-title><source>Br J Radiol</source><year>2019</year><volume>92</volume><issue>1100</issue><fpage>20190067</fpage><pub-id pub-id-type="doi">10.1259/bjr.20190067</pub-id><pub-id pub-id-type="pmid">31192695</pub-id></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kazemifar</surname><given-names>S</given-names></name><name><surname>McGuire</surname><given-names>S</given-names></name><name><surname>Timmerman</surname><given-names>R</given-names></name><etal/></person-group><article-title>MRI-only brain radiotherapy: assessing the dosimetric accuracy of synthetic CT images generated using a deep learning approach</article-title><source>Radiother Oncol</source><year>2019</year><volume>136</volume><fpage>56</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/j.radonc.2019.03.026</pub-id><pub-id pub-id-type="pmid">31015130</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Deng</surname><given-names>W</given-names></name></person-group><article-title>Synthetic CT generation based on T2 weighted MRI of nasopharyngeal carcinoma (NPC) using a deep convolutional neural network (DCNN)</article-title><source>Front Oncol</source><year>2019</year><volume>9</volume><fpage>1333</fpage><pub-id pub-id-type="doi">10.3389/fonc.2019.01333</pub-id><pub-id pub-id-type="pmid">31850218</pub-id></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Bian</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><etal/></person-group><article-title>Assessment of generative adversarial networks for synthetic anterior segment optical coherence tomography images in closed-angle detection</article-title><source>Transl Vis Sci Technol</source><year>2021</year><volume>10</volume><issue>4</issue><fpage>34</fpage><pub-id pub-id-type="doi">10.1167/tvst.10.4.34</pub-id><pub-id pub-id-type="pmid">34004012</pub-id></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Xie</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>K</given-names></name><etal/></person-group><article-title>Assessment of generative adversarial networks model for synthetic optical coherence tomography images of retinal disorders</article-title><source>Transl Vis Sci Technol</source><year>2020</year><volume>9</volume><issue>2</issue><fpage>29</fpage><pub-id pub-id-type="doi">10.1167/tvst.9.2.29</pub-id><pub-id pub-id-type="pmid">32832202</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Koh</surname><given-names>V</given-names></name><name><surname>Bian</surname><given-names>F</given-names></name><etal/></person-group><article-title>Semi-supervised generative adversarial networks for closed-angle detection on anterior segment optical coherence tomography images: an empirical study with a small training dataset</article-title><source>Ann Transl Med</source><year>2021</year><volume>9</volume><issue>13</issue><fpage>1073</fpage><pub-id pub-id-type="doi">10.21037/atm-20-7436</pub-id><pub-id pub-id-type="pmid">34422985</pub-id></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Khosravan</surname><given-names>N</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Cross-modality knowledge transfer for prostate segmentation from CT scans</article-title><source>Domain adaptation and representation transfer and medical image learning with less labels and imperfect data</source><year>2019</year><publisher-loc>Cham</publisher-loc><publisher-name>Springer</publisher-name><fpage>63</fpage><lpage>71</lpage></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoo</surname><given-names>TK</given-names></name><name><surname>Ryu</surname><given-names>IH</given-names></name><name><surname>Kim</surname><given-names>JK</given-names></name><etal/></person-group><article-title>Deep learning can generate traditional retinal fundus photographs using ultra-widefield images via generative adversarial networks</article-title><source>Comput Methods Programs Biomed</source><year>2020</year><volume>197</volume><fpage>105761</fpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2020.105761</pub-id><pub-id pub-id-type="pmid">32961385</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verma</surname><given-names>S</given-names></name><name><surname>Nongpiur</surname><given-names>ME</given-names></name><name><surname>Oo</surname><given-names>HH</given-names></name><etal/></person-group><article-title>Plateau Iris distribution across anterior segment optical coherence tomography defined subgroups of subjects with primary angle closure glaucoma</article-title><source>Invest Ophthalmol Vis Sci</source><year>2017</year><volume>58</volume><issue>12</issue><fpage>5093</fpage><lpage>5097</lpage><pub-id pub-id-type="doi">10.1167/iovs.17-22364</pub-id><pub-id pub-id-type="pmid">28986594</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwon</surname><given-names>J</given-names></name><name><surname>Sung</surname><given-names>KR</given-names></name><name><surname>Han</surname><given-names>S</given-names></name><name><surname>Moon</surname><given-names>YJ</given-names></name><name><surname>Shin</surname><given-names>JW</given-names></name></person-group><article-title>Subclassification of primary angle closure using anterior segment optical coherence tomography and ultrasound biomicroscopic parameters</article-title><source>Ophthalmology</source><year>2017</year><volume>124</volume><issue>7</issue><fpage>1039</fpage><lpage>1047</lpage><pub-id pub-id-type="doi">10.1016/j.ophtha.2017.02.025</pub-id><pub-id pub-id-type="pmid">28385302</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Office for Civil Rights (OCR). Guidance regarding methods for de-identification of protected health information in accordance with the Health Insurance Portability and Accountability Act (HIPAA) privacy rule. <ext-link ext-link-type="uri" xlink:href="https://www.hhs.gov/guidance/document/guidance-regarding-methods-de-identification-protected-health-information-accordance-0">https://www.hhs.gov/guidance/document/guidance-regarding-methods-de-identification-protected-health-information-accordance-0</ext-link>. Accessed 12 Mar 2017.</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Zhu J-Y, Park T, Isola P, Efros AA. Unpaired image-to-image translation using cycle-consistent adversarial networks. In: Proceedings of the IEEE international conference on computer vision, 22&#x02013;19 Oct 2017, Venice<italic>.</italic> p. 2223&#x02013;32.</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Liu Y, Guo Y, Chen W, Lew MS. An extensive study of cycle-consistent generative networks for image-to-image translation. In: 24th international conference on pattern recognition (ICPR), 20&#x02013;14 Aug, 2018, Beijing. p. 219&#x02013;24.</mixed-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlegl</surname><given-names>T</given-names></name><name><surname>Seebock</surname><given-names>P</given-names></name><name><surname>Waldstein</surname><given-names>SM</given-names></name><name><surname>Langs</surname><given-names>G</given-names></name><name><surname>Schmidt-Erfurth</surname><given-names>U</given-names></name></person-group><article-title>f-AnoGAN: fast unsupervised anomaly detection with generative adversarial networks</article-title><source>Med Image Anal</source><year>2019</year><volume>54</volume><fpage>30</fpage><lpage>44</lpage><pub-id pub-id-type="doi">10.1016/j.media.2019.01.010</pub-id><pub-id pub-id-type="pmid">30831356</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakata</surname><given-names>LM</given-names></name><name><surname>Lavanya</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>DS</given-names></name><etal/></person-group><article-title>Assessment of the scleral spur in anterior segment optical coherence tomography images</article-title><source>Arch Ophthalmol</source><year>2008</year><volume>126</volume><issue>2</issue><fpage>181</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1001/archophthalmol.2007.46</pub-id><pub-id pub-id-type="pmid">18268207</pub-id></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>RY</given-names></name><name><surname>Kasuga</surname><given-names>T</given-names></name><name><surname>Cui</surname><given-names>QN</given-names></name><etal/></person-group><article-title>Association between baseline iris thickness and prophylactic laser peripheral iridotomy outcomes in primary angle-closure suspects</article-title><source>Ophthalmology</source><year>2014</year><volume>121</volume><issue>6</issue><fpage>1194</fpage><lpage>1202</lpage><pub-id pub-id-type="doi">10.1016/j.ophtha.2013.12.027</pub-id><pub-id pub-id-type="pmid">24534754</pub-id></element-citation></ref><ref id="CR24"><label>24.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Cheung</surname><given-names>CY</given-names></name><name><surname>Aung</surname><given-names>T</given-names></name><etal/></person-group><article-title>In vivo analysis of vectors involved in pupil constriction in Chinese subjects with angle closure</article-title><source>Invest Ophthalmol Vis Sci</source><year>2012</year><volume>53</volume><issue>11</issue><fpage>6756</fpage><lpage>6762</lpage><pub-id pub-id-type="doi">10.1167/iovs.12-10415</pub-id><pub-id pub-id-type="pmid">22930726</pub-id></element-citation></ref><ref id="CR25"><label>25.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Huang</surname><given-names>W</given-names></name><etal/></person-group><article-title>Difference of uveal parameters between the acute primary angle closure eyes and the fellow eyes</article-title><source>Eye (Lond)</source><year>2018</year><volume>32</volume><issue>7</issue><fpage>1174</fpage><lpage>1182</lpage><pub-id pub-id-type="doi">10.1038/s41433-018-0056-9</pub-id><pub-id pub-id-type="pmid">29491485</pub-id></element-citation></ref><ref id="CR26"><label>26.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Guzman</surname><given-names>CP</given-names></name><name><surname>Cheung</surname><given-names>CY</given-names></name><etal/></person-group><article-title>Analysis of anterior segment dynamics using anterior segment optical coherence tomography before and after laser peripheral iridotomy</article-title><source>JAMA Ophthalmol</source><year>2013</year><volume>131</volume><issue>1</issue><fpage>44</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1001/jamaophthalmol.2013.567</pub-id><pub-id pub-id-type="pmid">23307207</pub-id></element-citation></ref><ref id="CR27"><label>27.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quigley</surname><given-names>HA</given-names></name></person-group><article-title>Angle-closure glaucoma-simpler answers to complex mechanisms: LXVI Edward Jackson Memorial Lecture</article-title><source>Am J Ophthalmol</source><year>2009</year><volume>148</volume><issue>5</issue><fpage>657</fpage><lpage>691.e1</lpage><pub-id pub-id-type="doi">10.1016/j.ajo.2009.08.009</pub-id><pub-id pub-id-type="pmid">19878757</pub-id></element-citation></ref><ref id="CR28"><label>28.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>YE</given-names></name><name><surname>Huang</surname><given-names>G</given-names></name><etal/></person-group><article-title>Prevalence and characteristics of plateau iris configuration among American Caucasian, American Chinese and mainland Chinese subjects</article-title><source>Br J Ophthalmol</source><year>2014</year><volume>98</volume><issue>4</issue><fpage>474</fpage><lpage>478</lpage><pub-id pub-id-type="doi">10.1136/bjophthalmol-2013-303792</pub-id><pub-id pub-id-type="pmid">24429278</pub-id></element-citation></ref><ref id="CR29"><label>29.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nongpiur</surname><given-names>ME</given-names></name><name><surname>Atalay</surname><given-names>E</given-names></name><name><surname>Gong</surname><given-names>T</given-names></name><etal/></person-group><article-title>Anterior segment imaging-based subdivision of subjects with primary angle-closure glaucoma</article-title><source>Eye (Lond)</source><year>2017</year><volume>31</volume><issue>4</issue><fpage>572</fpage><lpage>577</lpage><pub-id pub-id-type="doi">10.1038/eye.2016.267</pub-id><pub-id pub-id-type="pmid">27935603</pub-id></element-citation></ref><ref id="CR30"><label>30.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pekala</surname><given-names>M</given-names></name><name><surname>Joshi</surname><given-names>N</given-names></name><name><surname>Liu</surname><given-names>TYA</given-names></name><name><surname>Bressler</surname><given-names>NM</given-names></name><name><surname>DeBuc</surname><given-names>DC</given-names></name><name><surname>Burlina</surname><given-names>P</given-names></name></person-group><article-title>Deep learning based retinal OCT segmentation</article-title><source>Comput Biol Med</source><year>2019</year><volume>114</volume><fpage>103445</fpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2019.103445</pub-id><pub-id pub-id-type="pmid">31561100</pub-id></element-citation></ref><ref id="CR31"><label>31.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Qin</surname><given-names>W</given-names></name><etal/></person-group><article-title>Magnetic resonance image (MRI) synthesis from brain computed tomography (CT) images based on deep learning methods for magnetic resonance (MR)-guided radiotherapy</article-title><source>Quant Imaging Med Surg</source><year>2020</year><volume>10</volume><issue>6</issue><fpage>1223</fpage><lpage>1236</lpage><pub-id pub-id-type="doi">10.21037/qims-19-885</pub-id><pub-id pub-id-type="pmid">32550132</pub-id></element-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Ali MB, Gu IY, Berger MS, et al. Domain mapping and deep learning from multiple MRI clinical datasets for prediction of molecular subtypes in low grade gliomas. Brain Sci. 2020;0(7):463. 10.3390/brainsci10070463.</mixed-citation></ref><ref id="CR33"><label>33.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolan</surname><given-names>WP</given-names></name><name><surname>See</surname><given-names>JL</given-names></name><name><surname>Chew</surname><given-names>PT</given-names></name><etal/></person-group><article-title>Detection of primary angle closure using anterior segment optical coherence tomography in Asian eyes</article-title><source>Ophthalmology</source><year>2007</year><volume>114</volume><issue>1</issue><fpage>33</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.ophtha.2006.05.073</pub-id><pub-id pub-id-type="pmid">17070597</pub-id></element-citation></ref><ref id="CR34"><label>34.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shabana</surname><given-names>N</given-names></name><name><surname>Aquino</surname><given-names>MC</given-names></name><name><surname>See</surname><given-names>J</given-names></name><etal/></person-group><article-title>Quantitative evaluation of anterior chamber parameters using anterior segment optical coherence tomography in primary angle closure mechanisms</article-title><source>Clin Exp Ophthalmol</source><year>2012</year><volume>40</volume><issue>8</issue><fpage>792</fpage><lpage>801</lpage><pub-id pub-id-type="doi">10.1111/j.1442-9071.2012.02805.x</pub-id><pub-id pub-id-type="pmid">22594402</pub-id></element-citation></ref></ref-list></back></article>
