<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="review-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">iScience</journal-id><journal-id journal-id-type="iso-abbrev">iScience</journal-id><journal-title-group><journal-title>iScience</journal-title></journal-title-group><issn pub-type="epub">2589-0042</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmc">PMC9437858</article-id><article-id pub-id-type="pii">S2589-0042(22)01139-7</article-id><article-id pub-id-type="doi">10.1016/j.isci.2022.104867</article-id><article-id pub-id-type="publisher-id">104867</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title>3D visualization processes for recreating and studying organismal form</article-title></title-group><contrib-group><contrib contrib-type="author" id="au1"><name><surname>Irschick</surname><given-names>Duncan J.</given-names></name><email>duncan@umass.edu</email><xref rid="aff1" ref-type="aff">1</xref><xref rid="cor1" ref-type="corresp">&#x02217;</xref></contrib><contrib contrib-type="author" id="au2"><name><surname>Christiansen</surname><given-names>Fredrik</given-names></name><xref rid="aff3" ref-type="aff">3</xref></contrib><contrib contrib-type="author" id="au3"><name><surname>Hammerschlag</surname><given-names>Neil</given-names></name><xref rid="aff4" ref-type="aff">4</xref><xref rid="aff5" ref-type="aff">5</xref><xref rid="aff6" ref-type="aff">6</xref></contrib><contrib contrib-type="author" id="au4"><name><surname>Martin</surname><given-names>Johnson</given-names></name><xref rid="aff7" ref-type="aff">7</xref></contrib><contrib contrib-type="author" id="au5"><name><surname>Madsen</surname><given-names>Peter T.</given-names></name><xref rid="aff2" ref-type="aff">2</xref><xref rid="aff3" ref-type="aff">3</xref></contrib><contrib contrib-type="author" id="au6"><name><surname>Wyneken</surname><given-names>Jeanette</given-names></name><xref rid="aff8" ref-type="aff">8</xref></contrib><contrib contrib-type="author" id="au7"><name><surname>Brooks</surname><given-names>Annabelle</given-names></name><xref rid="aff9" ref-type="aff">9</xref></contrib><contrib contrib-type="author" id="au8"><name><surname>Gleiss</surname><given-names>Adrian</given-names></name><xref rid="aff10" ref-type="aff">10</xref><xref rid="aff11" ref-type="aff">11</xref></contrib><contrib contrib-type="author" id="au9"><name><surname>Fossette</surname><given-names>Sabrina</given-names></name><xref rid="aff12" ref-type="aff">12</xref></contrib><contrib contrib-type="author" id="au10"><name><surname>Siler</surname><given-names>Cameron</given-names></name><xref rid="aff13" ref-type="aff">13</xref></contrib><contrib contrib-type="author" id="au11"><name><surname>Gamble</surname><given-names>Tony</given-names></name><xref rid="aff14" ref-type="aff">14</xref><xref rid="aff15" ref-type="aff">15</xref><xref rid="aff16" ref-type="aff">16</xref></contrib><contrib contrib-type="author" id="au12"><name><surname>Fish</surname><given-names>Frank</given-names></name><xref rid="aff17" ref-type="aff">17</xref></contrib><contrib contrib-type="author" id="au13"><name><surname>Siebert</surname><given-names>Ursula</given-names></name><xref rid="aff18" ref-type="aff">18</xref></contrib><contrib contrib-type="author" id="au14"><name><surname>Patel</surname><given-names>Jaymin</given-names></name><xref rid="aff19" ref-type="aff">19</xref></contrib><contrib contrib-type="author" id="au15"><name><surname>Xu</surname><given-names>Zhan</given-names></name><xref rid="aff20" ref-type="aff">20</xref></contrib><contrib contrib-type="author" id="au16"><name><surname>Kalogerakis</surname><given-names>Evangelos</given-names></name><xref rid="aff20" ref-type="aff">20</xref></contrib><contrib contrib-type="author" id="au17"><name><surname>Medina</surname><given-names>Joshua</given-names></name><xref rid="aff1" ref-type="aff">1</xref></contrib><contrib contrib-type="author" id="au18"><name><surname>Mukherji</surname><given-names>Atreyi</given-names></name><xref rid="aff21" ref-type="aff">21</xref></contrib><contrib contrib-type="author" id="au19"><name><surname>Mandica</surname><given-names>Mark</given-names></name><xref rid="aff22" ref-type="aff">22</xref></contrib><contrib contrib-type="author" id="au20"><name><surname>Zotos</surname><given-names>Savvas</given-names></name><xref rid="aff23" ref-type="aff">23</xref><xref rid="aff24" ref-type="aff">24</xref></contrib><contrib contrib-type="author" id="au21"><name><surname>Detwiler</surname><given-names>Jared</given-names></name><xref rid="aff25" ref-type="aff">25</xref></contrib><contrib contrib-type="author" id="au22"><name><surname>Perot</surname><given-names>Blair</given-names></name><xref rid="aff26" ref-type="aff">26</xref></contrib><contrib contrib-type="author" id="au23"><name><surname>Lauder</surname><given-names>George</given-names></name><xref rid="aff27" ref-type="aff">27</xref></contrib><aff id="aff1"><label>1</label>Department of Biology, 221 Morrill Science Center, University of Massachusetts, Amherst, MA 01003, USA</aff><aff id="aff2"><label>2</label>Zoophysiology, Department of Biology, Aarhus University, Aarhus, Denmark</aff><aff id="aff3"><label>3</label>Aarhus Institute of Advanced Studies, H&#x000f8;egh-Guldbergs Gade 6B, Aarhus C, Denmark</aff><aff id="aff4"><label>4</label>Rosenstiel School of Marine and Atmospheric Science, University of Miami, Miami, FL, USA</aff><aff id="aff5"><label>5</label>Leonard and Jayne Abess Center for Ecosystem Science and Policy, University of Miami, Coral Gables, FL, USA</aff><aff id="aff6"><label>6</label>Shark Research and Conservation Program, University of Miami, Miami, FL, USA</aff><aff id="aff7"><label>7</label>329 E Main Street, Wilmore, KY 40390, USA</aff><aff id="aff8"><label>8</label>Department of Biological Sciences, Florida Atlantic University, 777 Glades Road, Boca Raton, FL 33431-0991, USA</aff><aff id="aff9"><label>9</label>Cape Eleuthera Institute, PO Box EL-26029, Rock Sound, Eleuthera, The Bahamas</aff><aff id="aff10"><label>10</label>Centre for Sustainable Aquatic Ecosystems, Harry Butler Institute, Murdoch University, 90 South Street, Murdoch, WA 6150, Australia</aff><aff id="aff11"><label>11</label>College of Science, Health, Engineering and Education, Murdoch University, 90 South St, Murdoch, WA 6150, Australia</aff><aff id="aff12"><label>12</label>Biodiversity and Conservation Science, Department of Biodiversity, Conservation and Attractions, 17 Dick Perry Avenue, Kensington, WA 6151, Australia</aff><aff id="aff13"><label>13</label>Sam Noble Oklahoma Museum of Natural History and Department of Biology, University of Oklahoma, 2401 Chautauqua Avenue, Norman, OK 73072-7029, USA</aff><aff id="aff14"><label>14</label>Marquette University, Wehr Life Sciences 109, 1428 W. Clybourn Street, Milwaukee, WI 53233, USA</aff><aff id="aff15"><label>15</label>Milwaukee Public Museum, 800 W. Wells Street, Milwaukee, WI 53233, USA</aff><aff id="aff16"><label>16</label>Bell Museum of Natural History, University of Minnesota, 1987 Upper Buford Circle, St. Paul, Minnesota 551088, USA</aff><aff id="aff17"><label>17</label>Department of Biology, West Chester University, West Chester, PA 19383, USA</aff><aff id="aff18"><label>18</label>Institute for Terrestrial and Aquatic Wildlife Research, University of Veterinary Medicine, Hannover, Werftstrasse 6, 25761 Buesum, Germany</aff><aff id="aff19"><label>19</label>Temple University Kornberg School of Dentistry, 3223 North Broad Street Philadelphia, PE 19140, USA</aff><aff id="aff20"><label>20</label>College of Information and Computer Sciences, University of Massachusetts, 140 Governors Dr., Amherst, MA, USA</aff><aff id="aff21"><label>21</label>University of Texas Southwestern Medical Center, 5323 Harry Hines Boulevard, Dallas, TX 75390, USA</aff><aff id="aff22"><label>22</label>Amphibian Foundation, 4055 Roswell Road NE, Atlanta, GA 30342, USA</aff><aff id="aff23"><label>23</label>Terra Cypria&#x02013;the Cyprus Conservation Foundation, Agiou Andreou 341, 3035 Limassol, Cyprus</aff><aff id="aff24"><label>24</label>School of Pure and Applied Sciences, Open University of Cyprus, PO Box 12794, 2252 Nicosia, Cyprus</aff><aff id="aff25"><label>25</label>22 Hooker Avenue Unit 1, Northampton, MA 01060, USA</aff><aff id="aff26"><label>26</label>Department of Mechanical and Industrial Engineering, University of Massachusetts, Amherst, MA 01003-2210, USA</aff><aff id="aff27"><label>27</label>Museum of Comparative Zoology, Harvard University, Cambridge, MA 02138, USA</aff></contrib-group><author-notes><corresp id="cor1"><label>&#x02217;</label>Corresponding author <email>duncan@umass.edu</email></corresp></author-notes><pub-date pub-type="pmc-release"><day>04</day><month>8</month><year>2022</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="collection"><day>16</day><month>9</month><year>2022</year></pub-date><pub-date pub-type="epub"><day>04</day><month>8</month><year>2022</year></pub-date><volume>25</volume><issue>9</issue><elocation-id>104867</elocation-id><permissions><copyright-statement>&#x000a9; 2022.</copyright-statement><copyright-year>2022</copyright-year><copyright-holder/><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p></license></permissions><abstract id="abs0010"><title>Summary</title><p>The study of biological form is a vital goal of evolutionary biology and functional morphology. We review an emerging set of methods that allow scientists to create and study accurate 3D models of living organisms and animate those models for biomechanical and fluid dynamic analyses. The methods for creating such models include 3D photogrammetry, laser and CT scanning, and 3D software. New multi-camera devices can be used to create accurate 3D models of living animals in the wild and captivity. New websites and virtual reality/augmented reality devices now enable the visualization and sharing of these data. We provide examples of these approaches for animals ranging from large whales to lizards and show applications for several areas: Natural history collections; body condition/scaling, bioinspired robotics, computational fluids dynamics (CFD), machine learning, and education. We provide two datasets to demonstrate the efficacy of CFD and machine learning approaches and conclude with a prospectus.</p></abstract><abstract abstract-type="graphical" id="abs0015"><title>Graphical abstract</title><fig id="undfig1" position="anchor"><graphic xlink:href="fx1"/></fig></abstract><kwd-group id="kwrds0010"><title>Subject area</title><kwd>Ecology</kwd><kwd>Biological sciences</kwd><kwd>Zoology</kwd><kwd>Evolutionary biology</kwd></kwd-group></article-meta></front><body><sec id="sec1"><title>Introduction</title><p id="p0010">For as long as humans have existed on the earth, we have engaged in new ways to visualize and depict the living world around us. This characteristic is reflected in some of the earliest cave drawings as well as the first forays into the formal scientific depiction of organisms in natural history drawings. The earliest studies of microscopy revealed new forms of life that were not then known to exist, beginning in earnest with innovations in lens manufacturing in the 1500s in Europe and the Middle East. Further innovations in camera technology enabled new 2D and 3D techniques for visualizing shape and color in living organisms (e.g., <xref rid="bib9" ref-type="bibr">Baqersad et&#x000a0;al., 2017</xref>; <xref rid="bib24" ref-type="bibr">Chiari et&#x000a0;al., 2008</xref>). Today, we boast a wide range of visualization techniques for organisms of varying sizes, from the smallest bacteria to massive whales (e.g., <xref rid="bib16" ref-type="bibr">Bot and Irschick, 2019</xref>; <xref rid="bib44" ref-type="bibr">Gignac and Kley, 2014</xref>; <xref rid="bib45" ref-type="bibr">Gignac et&#x000a0;al., 2016</xref>; <xref rid="bib65" ref-type="bibr">Kim and Kim, 1999</xref>; <xref rid="bib74" ref-type="bibr">Laha et&#x000a0;al., 2014</xref>; <xref rid="bib103" ref-type="bibr">Silverstein et&#x000a0;al., 2006</xref>; <xref rid="bib108" ref-type="bibr">Szaflik, 2007</xref>). The urgency of visualizing the shape of life is felt perhaps more deeply today than ever before because of the extinction crisis for biodiversity during the Anthropocene (<xref rid="bib112" ref-type="bibr">Turvey and Crees 2019</xref>). Simply put, many organisms are disappearing, and thus, there is increased interest in their documentation. More broadly, defining the &#x0201c;shape&#x0201d; or &#x0201c;form&#x0201d; of organisms is central to all concepts of biodiversity (<xref rid="bib1" ref-type="bibr">Adams et&#x000a0;al., 2009</xref>; <xref rid="bib22" ref-type="bibr">Brown et&#x000a0;al., 2013</xref>; <xref rid="bib38" ref-type="bibr">Falkingham, 2012</xref>; <xref rid="bib43" ref-type="bibr">Gaston, 2003</xref>; <xref rid="bib88" ref-type="bibr">Pimm et&#x000a0;al., 1995</xref>). As one of the primary goals of ecology and evolution is to understand the origins and maintenance of biodiversity, methods for defining the shape of organisms and phenotypic diversity are important.</p><p id="p0015">Several scientific fields have aimed to describe the mechanistic, ecological, and evolutionary causes of variation in body shape. First, the field of evolutionary developmental biology (i.e., evo-devo) has focused on elucidating the genetic and developmental basis of organismal body form (<xref rid="bib2" ref-type="bibr">Albertson et&#x000a0;al., 2003</xref>; <xref rid="bib54" ref-type="bibr">Irschick et&#x000a0;al., 2013</xref>; <xref rid="bib84" ref-type="bibr">M&#x000fc;ller, 2007</xref>; <xref rid="bib95" ref-type="bibr">Raff, 2000</xref>). Second, the field of biometry has employed quantitative and statistical methods for describing differences in body shape among and within species (<xref rid="bib17" ref-type="bibr">Boyer et&#x000a0;al., 2015</xref>; <xref rid="bib124" ref-type="bibr">Zelditch et&#x000a0;al., 2012</xref>). Third, evolutionary ecologists and eco-morphologists have examined the origins of diversity in the context of resource use (<xref rid="bib15" ref-type="bibr">Bock, 1994</xref>; <xref rid="bib53" ref-type="bibr">Irschick et&#x000a0;al., 1997</xref>; <xref rid="bib87" ref-type="bibr">Pianka, 2001</xref>; <xref rid="bib116" ref-type="bibr">Wainwright and Reilly, 1994</xref>). The common thread for these lines of inquiry is explaining why and how variation in animal form exists, with the bulk of this interest focused on external features of organisms, such as dimensions of the head, limbs, and body axis. This interest is justified not only because it is the external shape that humans most obviously witness, but also because of the importance of external shape for how animals interact with their environment, such as the importance of wing shape for flight, or limb and body dimensions for locomotion (<xref rid="bib55" ref-type="bibr">Irschick and Higham, 2016</xref>). Although the internal structures and anatomy of organisms also play a vital role in all the above aspects, quantifying external aspects of shape represents a valuable goal and is especially important for emerging areas such as bioinspired robotics and computational analyses of animal locomotion.</p><p id="p0020">Here, we discuss an exciting new approach to the study of organismal shape, the creation of 3D models of living organisms, which provides a new platform for the study of body shape. The focus here is on relatively non-invasive methods of 3D scanning and reconstruction, such as creating a 3D digital photogrammetry scan from an animal eating while at a zoo, or in the wild. This non-invasive angle opens opportunities for working with rare and elusive megafauna that are challenging or impossible to keep in captive situations, such as most whales. Our focus is on approaches taken by several of us, but we note that the broader goal of creating life-like 3D organisms is built on a foundation from many other scientists that we discuss and cite. It is important to contrast this scientific goal with the already established practice of computer graphics animal modeling within the entertainment industry, which blends artistic and scientific approaches and is often proprietary. We emphasize that while the techniques used to recreate 3D digital animals, such as CT scanning, 3D digital photogrammetry, and laser scanning, are well-established, how these tools, in combination with software (e.g., Blender, Maya) are used to recreate living animals in a scientifically accurate manner, remains a work in progress.</p><p id="p0025">Rather than supplanting prior scientific methods, such as biometry, this approach provides a new venue for this and other methods to operate in. We argue that in an ideal world, scientists would have access to a digital replica of living animals so that they could iteratively measure their shape and color. However, is such an approach feasible? Furthermore, what additional value do such 3D models provide for ecology, evolution, and functional morphology that is not already being met with current methods? We aim to answer these questions and argue that the above-stated goal is now within the reach of scientists, with far-reaching implications. We discuss how new techniques and conceptual methods in 3D reconstruction are opening new opportunities for scientists in a wide range of disciplines.</p></sec><sec id="sec2"><title>New methods for creating digital avatars of living organisms</title><p id="p0030">To appreciate the potential value of creating digital avatars of living animals, let us imagine a few of the numerous scientific applications of such an approach. Before doing so, it is important to state that we focus this review on the external shape of living animals, and not on color or internal anatomy. The quantification of color and internal anatomy each have a long and rich history, and while the 3D modeling approach has implications for each, such treatment is beyond the scope of this perspective. There are ongoing efforts to use 3D photogrammetry approaches to studying color. Like any photograph, a 3D model can be analyzed in terms of its color profile in the same manner as a 2D digital photograph, and it is also possible to link the physical position of colors onto a shape to investigate basic ecological and evolutionary questions. Also, our coverage here is germane to all living organisms, although the taxonomic sampling described in this article is relatively limited, and consists mostly of reptiles, amphibians, fish, and marine mammals. Modifications of the techniques and concepts discussed here could be, in theory, applied to the vast bulk of living organisms.</p><p id="p0035">There are several potential scientific applications for digital animal avatars. First, a digital avatar would provide biometricians unprecedented ability to measure body shape with a range of innovative techniques. Instead of relying only on 2D images, or a preserved specimen, or perhaps brief access to a live specimen, a digital avatar could enable a nearly infinite range of measurement opportunities at the convenience of the investigator (<xref rid="fig1" ref-type="fig">Figure&#x000a0;1</xref>, <xref rid="fig2" ref-type="fig">Figure&#x000a0;2</xref> and <xref rid="fig2" ref-type="fig">2</xref>). Importantly, a digital avatar could comfortably dwell on an investigator&#x02019;s computer or hard drive for over a lifetime, whereas a preserved specimen might have to be returned to a museum. Second, engineers interested in creating bioinspired robots would have direct access to a digital replica from which they could gain inspiration, and for which they could test the functional value of specific body parts, such as using flow-tank, or computational fluid-dynamics, simulations (<xref rid="bib40" ref-type="bibr">Fish and Lauder, 2017</xref>; <xref rid="bib69" ref-type="bibr">Laforsch et al., 2012</xref>; <xref rid="bib79" ref-type="bibr">Liu, 2002</xref>; <xref rid="bib83" ref-type="bibr">Miller et&#x000a0;al., 2012</xref>). Third, scientists interested in body condition would be able to directly measure volumes and estimate mass, which is especially important for large or elusive megafauna (e.g., marine mammals, sharks, large herbivores, and so forth), which often travel long distances and expend large amounts of energy (<xref rid="bib25" ref-type="bibr">Christiansen et&#x000a0;al., 2018</xref>, <xref rid="bib26" ref-type="bibr">2019</xref>; <xref rid="bib60" ref-type="bibr">Jakob et&#x000a0;al., 1996</xref>). Finally, functional morphologists and biomechanists would be able to digitally manipulate the motions and shapes of these digital avatars in computational simulations, which would allow them to better understand the adaptive basis of morphological variation (<xref rid="bib11" ref-type="bibr">Biewener, 2003</xref>; <xref rid="bib12" ref-type="bibr">Biewener and Patek, 2017</xref>; <xref rid="bib55" ref-type="bibr">Irschick and Higham, 2016</xref>; <xref rid="bib70" ref-type="bibr">Lauder, 1990</xref>). Behaviorists could also use the 3D models as playbacks for manipulating receivers (<xref rid="bib41" ref-type="bibr">Fleishman and Endler, 2000</xref>).<fig id="fig1"><label>Figure&#x000a0;1</label><caption><p>3D photogrammetry methods for reconstructing live animals</p><p>Multi-camera scanning methods for photo-scanning various kinds of animals using 3D digital photogrammetry, such as small reptiles; (A)&#x000a0;a tokay gecko, <italic>Gekko gecko</italic>, taken in the Irschick lab at the University of Massachusetts at Amherst, USA); (B)&#x000a0;a medium sized sea turtle (a green sea turtle, <italic>Chelonia mydas</italic>, taken at the Loggerhead MarineLife Center, USA); (C)&#x000a0;a large mammal (a southern white rhino (<italic>Ceratotherium simum</italic>, taken at the Perth zoo, Australia); A typical process for 3D photogrammetry of live specimens, including multiple photos (D), creation of a 3D surface using 3D software (E), and finally a full-color version (F)&#x000a0;which closely replicates the original specimen. Photos are of a marine toad (<italic>Rhinella marina</italic>) taken in the Philippines.</p></caption><graphic xlink:href="gr1"/></fig><fig id="fig2"><label>Figure&#x000a0;2</label><caption><p>3D shapes of various animal species</p><p>3D whole body meshes of the 14 described species in Table&#x000a0;1, which are: (A)&#x000a0;southern right whale (<italic>Eubalaena australis</italic>), (B)&#x000a0;southern white rhino (<italic>Ceratotherium simum simum</italic>), (C)&#x000a0;blacktip shark (<italic>Carcharhinus limbatus</italic>), (D)&#x000a0;harbor porpoise (<italic>Phocoena phocoena</italic>), (E) loggerhead sea turtle, <italic>Caretta caretta</italic>, (F)&#x000a0;flatback sea turtle (<italic>Natator depressus</italic>), (G)&#x000a0;green sea turtle (<italic>Chelonia mydas</italic>), (H)&#x000a0;kemps Ridley sea turtle (<italic>Lepidochelys kempii,</italic> (I), hawksbill sea turtle (<italic>Eretmochelys imbricata</italic>), (J) Cyprus racer snake (<italic>Dolichophis jugularis</italic>), (K)&#x000a0;sling-tailed agama (<italic>Stellagama stellio cypriaca</italic>), (L)&#x000a0;tokay gecko (<italic>Gekko gecko</italic>), (M)&#x000a0;flying gecko (<italic>Ptychozoon kuhli</italic>), (N)&#x000a0;house gecko (<italic>Hemidactylus platyurus</italic>).</p></caption><graphic xlink:href="gr2"/></fig></p><p id="p0040">New elaborations on previously established methods and software now make these applications possible. It is noteworthy that the techniques for creating these digital avatars often do not fall neatly into one method (e.g., 3D photogrammetry), but represent a combination of hardware and software methods that continue to evolve. Indeed, there are exciting new opportunities for combining data from different scanning methods. On the hardware end, dramatic improvements in computers, camera, video, and laser and white-light scanning systems now provide an unparalleled set of tools that can be used to create 3D models (<xref rid="bib9" ref-type="bibr">Baqersad et&#x000a0;al., 2017</xref>; <xref rid="bib24" ref-type="bibr">Chiari et&#x000a0;al., 2008</xref>; <xref rid="bib35" ref-type="bibr">Egels and Kasser, 2003</xref>; <xref rid="bib38" ref-type="bibr">Falkingham, 2012</xref>; <xref rid="bib56" ref-type="bibr">Irschick et&#x000a0;al., 2020a</xref>, <xref rid="bib57" ref-type="bibr">2020b</xref>; <xref rid="bib92" ref-type="bibr">Postma et&#x000a0;al., 2015</xref>; <xref rid="bib108" ref-type="bibr">Szaflik, 2007</xref>). Three frequently used techniques are computed tomography (CT) scanning, laser/white-light scanning, and 3D photogrammetry. CT scanning is very useful for providing high-resolution 3D scans at a range of animal sizes under standardized laboratory conditions, although it cannot be conducted easily on living animals, and the devices are expensive. This method can potentially provide accurate estimates of body shape for rigid-bodied organisms that undergo little physical alteration after death. Laser and white-light scanning both operate with the same basic principles of projecting either a laser or white light onto a surface and then measuring the degree of reflectance in 3D. The resulting surface map is used to create a 3D point cloud. Additional processing can be used to measure the intensity of color from the object and then projecting a color map onto the 3D model. One limitation of laser and white-light scanning is that both processes are relatively time-consuming and typically take multiple seconds to several minutes to scan a medium-sized object (&#x0223c;10&#x000a0;cm). Scanning larger objects (e.g., &#x0003e;500&#x000a0;cm) can take much longer. Thus, laser and white light scanning are possibly more suitable for immobile living organisms but are less effective for larger or mobile organisms. 3D photogrammetry is the process by which 2D images are translated into 3D data (<xref rid="bib37" ref-type="bibr">Evin et&#x000a0;al., 2016</xref>; <xref rid="bib77" ref-type="bibr">Linder, 2009</xref>). The history of 3D photogrammetry goes back hundreds of years, but on a practical level, this method has most rapidly matured and developed within the last decade (<xref rid="bib8" ref-type="bibr">Amado et&#x000a0;al., 2019</xref>; <xref rid="bib77" ref-type="bibr">Linder, 2009</xref>). The modern incarnation of this method, 3D digital photogrammetry, consists of the direct conversion of 2D digital images, such as those captured by a handheld camera, to a 3D point cloud that is used to generate a texture map with overlying photographic color maps. Modern user-friendly software such as Meshroom, Colmap, and RealityCapture (e.g., <xref rid="bib6" ref-type="bibr">Alicvision, 2018</xref>; <xref rid="bib28" ref-type="bibr">Community, 2018</xref>), enable these reconstructions in relatively short time spans (e.g., minutes to hours).</p><p id="p0045">Unlike laser and white-light scanning, 3D photogrammetry works effectively across a range of size scales (e.g., mouse to elephant). Also, unlike CT scanning, 3D photogrammetry can be applied to mobile organisms and in remote areas. Limitations of 3D photogrammetry have been noted elsewhere (<xref rid="bib9" ref-type="bibr">Baqersad et&#x000a0;al., 2017</xref>), and include variability owing to camera resolution, lighting, photographic technique, and the choice of software for post-processing. For these reasons, many of the examples provided herein focused on various uses of 3D photogrammetry, but as noted later, combining various scanning techniques, such as laser/white light scanning, 3D photogrammetry, and CT scanning, offer exciting possibilities for live animal 3D reconstruction.</p><p id="p0050">Multi-camera systems offer great potential to scan live animals, and the Digital Life team (<ext-link ext-link-type="uri" xlink:href="http://www.digitallife3d.org" id="intref0010">www.digitallife3d.org</ext-link>) at the University of Massachusetts at Amherst has used the Beastcam technology platform, which is a set of portable, flexible multi-camera systems to successfully scan a large range of organisms. The original Beastcam was a handheld, four-armed system, in which a single camera was mounted on each of four arms. Wires from each camera went to a central trigger which triggered all four cameras simultaneously. Because of the small number of cameras, however, this system proved to be too limited for the photocapture of a wider range of organisms. Other systems are the Beastcam ARRAY (15-arm static system), the Beastcam MACRO (12-arm static system), and the STAND system (movable, tripod system) which were for mid-sized (4-8 inches), smaller (1-4 inches), and larger (&#x0003e;8 inches) organisms, respectively. 3D photogrammetry is highly accurate under certain conditions (<xref rid="bib3" ref-type="bibr">Aldridge et&#x000a0;al., 2005</xref>; Bagersad et&#x000a0;al., 2017; <xref rid="bib14" ref-type="bibr">Bythell et&#x000a0;al., 2001</xref>; <xref rid="bib29" ref-type="bibr">Dai and Mu, 2010</xref>; <xref rid="bib51" ref-type="bibr">Huising and Pereira, 1998</xref>; <xref rid="bib119" ref-type="bibr">Weinberg et&#x000a0;al., 2004</xref>). Some of these conditions include the avoidance of extreme wide-angle (fisheye) lenses and creating high-quality (in focus) images that allow the object to be reconstructed at a high level of detail. Our own analyses confirm this high level of accuracy when comparing live specimens versus their digital avatars, with accuracy levels of over 98% (<xref rid="bib57" ref-type="bibr">Irschick et&#x000a0;al., 2020b</xref>).</p><p id="p0055">The Digital Life team has successfully used various kinds of multi-camera rigs to scan a range of live mobile animals, including sea turtles, sharks, lizards, frogs, scorpions, and cockroaches, among others (<xref rid="tbl1" ref-type="table">Table&#x000a0;1</xref>, see also <ext-link ext-link-type="uri" xlink:href="https://sketchfab.com/DigitalLife3D" id="intref0015">https://sketchfab.com/DigitalLife3D</ext-link>). These specimens include reconstructions of whole live specimens, as well as 3D scans of organisms such as frogs or lizards sitting on branches or a leaf. Notably, it was possible to use the same techniques across a range of sizes, as demonstrated through similar methods of multi-camera scanning of animals ranging in size from a five g gecko (<italic>Hemidactylus frenatus</italic>) to a &#x0003e;37 million g southern right whale (<italic>Eubalaena australis</italic>, Table&#x000a0;1, <xref rid="fig2" ref-type="fig">Figure&#x000a0;2</xref>). In some cases, a combination of techniques was used to accurately recreate the body shapes of animals. For example, <xref rid="bib56" ref-type="bibr">Irschick et&#x000a0;al. (2020a)</xref> included images from an aerial drone and multiple GoPro cameras to create 3D scans of a live harbor porpoise (<italic>Phocoena phocoena</italic>). As shown in <xref rid="bib56" ref-type="bibr">Irschick et&#x000a0;al. (2020a</xref>, <xref rid="bib57" ref-type="bibr">2020b</xref>, <xref rid="bib58" ref-type="bibr">2020c)</xref>, the 3D reconstructions were very accurate, with error rates overall less than about 1%, once the digital avatar and the original live animal were compared.<table-wrap position="float" id="tbl1"><label>Table 1</label><caption><p>14 animal species used for 3D reconstruction</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Common name</th><th>Species</th><th>Link</th></tr></thead><tbody><tr><td>Southern right whale<xref rid="tblfn1" ref-type="table-fn">a</xref></td><td><italic>Eubalaena australis</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6Ryut" id="intref0010a">https://skfb.ly/6Ryut</ext-link></td></tr><tr><td>Southern white rhinoceros</td><td><italic>Ceratotherium simum</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6WRvC" id="intref0015a">https://skfb.ly/6WRvC</ext-link></td></tr><tr><td>Blacktip shark</td><td><italic>Carcharhinus limbatus</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6UpvY" id="intref0020a">https://skfb.ly/6UpvY</ext-link></td></tr><tr><td>Harbor porpoise<xref rid="tblfn2" ref-type="table-fn">b</xref></td><td><italic>Phocoena phocoena</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6VWZx" id="intref0025a">https://skfb.ly/6VWZx</ext-link></td></tr><tr><td>Loggerhead sea turtle</td><td><italic>Caretta</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6R7JL" id="intref0030a">https://skfb.ly/6R7JL</ext-link></td></tr><tr><td>Flatback sea turtle</td><td><italic>Natator depressus</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6R7JK" id="intref0035a">https://skfb.ly/6R7JK</ext-link></td></tr><tr><td>Green sea turtle</td><td><italic>Chelonia mydas</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6R7JJ" id="intref0040a">https://skfb.ly/6R7JJ</ext-link></td></tr><tr><td>Kemps ridley sea turtle</td><td><italic>Lepidochelys kempii</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6R7JM" id="intref0045a">https://skfb.ly/6R7JM</ext-link></td></tr><tr><td>Hawksbill sea turtle</td><td><italic>Eretmochelys imbricata</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6R7JN" id="intref0050a">https://skfb.ly/6R7JN</ext-link></td></tr><tr><td>Cyprus racer snake</td><td><italic>Dolichophis jugularis</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6RMRn" id="intref0055a">https://skfb.ly/6RMRn</ext-link></td></tr><tr><td>Sling-tailed agama lizard</td><td><italic>Stellagama stellio cypriaca</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6ROpL" id="intref0060a">https://skfb.ly/6ROpL</ext-link></td></tr><tr><td>Tokay gecko lizard</td><td><italic>Gekko gecko</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6SNEn" id="intref0065a">https://skfb.ly/6SNEn</ext-link></td></tr><tr><td>Flying gecko lizard</td><td><italic>Ptychozoon kuhli</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6ROpW" id="intref0070a">https://skfb.ly/6ROpW</ext-link></td></tr><tr><td>House gecko</td><td><italic>Hemidactylus platyurus</italic></td><td><ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6ROpX" id="intref0075a">https://skfb.ly/6ROpX</ext-link></td></tr></tbody></table><table-wrap-foot><fn><p>See links for models, and more information on methods of reconstruction. With the exception of the harbor porpoise and the southern right whale, all species were reconstructed using multi-camera 3D photogrammetry rigs (see Irschick et al. 2020b,c for details).</p></fn></table-wrap-foot><table-wrap-foot><fn id="tblfn1"><label>a</label><p id="ntpara0020a">Reconstructed using a combination of top and side images from drones.</p></fn></table-wrap-foot><table-wrap-foot><fn id="tblfn2"><label>b</label><p id="ntpara0025a">Reconstructed through a combination of drone photos and a 3D photoscan of animal while jumping.</p></fn></table-wrap-foot></table-wrap></p><p id="p0060">Like all digital data, 3D photogrammetry models can be conveniently shared in different ways, such as by exporting.OBJ or FBX files, which can be viewed in various Virtual Reality and Augmented Reality software and hardware because of their interest in the general public, the issues of licensing and proper data deposition for researchers are especially germane. A general principle of scientific publishing is that the raw data should be made publically accessible for non-profit use both for verification purposes, but also so that other scientists and educators can use and learn from the data. Because of the rise of commercial 3D sharing platforms such as Sketchfab (<ext-link ext-link-type="uri" xlink:href="http://Sketchfab.com" id="intref0020">Sketchfab.com</ext-link>), it is now possible to easily share 3D models. However, unlike most scientific data, accurate 3D models of living organisms also have potential commercial value, so finding an appropriate license structure that allows unfettered non-profit use, but not commercial use, is challenging. For example, the standard creative commons (cc) license typically allows free-usage, but certain restrictions on commercial use can be stipulated. However, because some educational usage is also technically commercial, this is an additional complication. Clearly, scientists as a community need to establish perhaps a new kind of license structure that is tailored for 3D models with commercial value, which include 3D organisms. A final issue is a proper repository structure. Ideally, scientific data should be stored as part of an existing non-profit structure (e.g., Website, or natural history museum collection database) that is stable, and which supports the mission of sharing scientific data. Especially important is basic standards on metadata, such as methods of model creation, model parameters (e.g., model size, number of triangles, and so forth), and details on the scientific specimen (see <xref rid="bib46" ref-type="bibr">Grayburn et&#x000a0;al., 2019</xref> for more discussion).</p></sec><sec id="sec3"><title>Natural history collections</title><p id="p0065">One of the many applications of life-like 3D models is for complementing traditional specimen-based collections. The tradition of using physical museum samples for studies of systematics, anatomy, taxonomy, and ecomorphology dates back hundreds of years (<xref rid="bib7" ref-type="bibr">Allmon 1994</xref>; <xref rid="bib19" ref-type="bibr">Bradley et&#x000a0;al., 2014</xref>; <xref rid="bib30" ref-type="bibr">Davis 1996</xref>; <xref rid="bib82" ref-type="bibr">Medina et&#x000a0;al., 2020</xref>; <xref rid="bib89" ref-type="bibr">Pleijel et&#x000a0;al., 2008</xref>; <xref rid="bib98" ref-type="bibr">Schindel and Cook, 2018</xref>; <xref rid="bib100" ref-type="bibr">Shaffer et&#x000a0;al., 1998</xref>; <xref rid="bib107" ref-type="bibr">Suarez and Tsutsui, 2004</xref>). This practice typically involves the collection of live specimens or portions of dead specimens (e.g., skeletons) alongside deposition of metadata. Live specimens are typically euthanized, and then preserved in some fashion (e.g., formalin, ethanol) (<xref rid="bib7" ref-type="bibr">Allmon, 1994</xref>; <xref rid="bib30" ref-type="bibr">Davis, 1996</xref>; <xref rid="bib50" ref-type="bibr">Huber, 2007</xref>; <xref rid="bib107" ref-type="bibr">Suarez and Tsutsui, 2004</xref>) and maintained in museums as a resource for scientists and educators. The value of keeping physical whole (i.e., whole organism) specimens is clear. First, they typically enable species identification, such as through visual inspection, or through measurements of key parameters (e.g., morphometrics). Second, physical specimens can be shared with other scientists, and their status can, therefore, be compared among different individuals. Third, they provide the ability to access genetic information (e.g., tissue) as well as internal anatomy. However, there is also a growing digitization movement within natural history collections, which is also raising questions about how other kinds of data other than physical specimens can be stored&#x000a0;in various kinds of natural history repositories, such as iDigBio (<ext-link ext-link-type="uri" xlink:href="http://www.idigbio.org" id="intref0025">www.idigbio.org</ext-link>) or Morphosource (<ext-link ext-link-type="uri" xlink:href="http://www.morphosource.org" id="intref0030">www.morphosource.org</ext-link>).</p><p id="p0070">In this regard, life-like 3D digital models offer a new tool for field-based collections, especially in areas where permits for collecting and exporting whole specimens are challenging or impossible to obtain. Increased effort toward field collection of 3D data would be a welcome addition to the ongoing efforts to catalog preserved museum specimens using 3D digital photogrammetry and CT scanning. Examples of ongoing efforts include the Florida Museum of Natural History (<ext-link ext-link-type="uri" xlink:href="https://sketchfab.com/FloridaMuseum" id="intref0035">https://sketchfab.com/FloridaMuseum</ext-link>) and the Moore Lab of Zoology at Occidential College (<ext-link ext-link-type="uri" xlink:href="https://sketchfab.com/MooreLab" id="intref0040">https://sketchfab.com/MooreLab</ext-link>), among others. <xref rid="bib82" ref-type="bibr">Medina et&#x000a0;al. (2020)</xref> discuss in detail how 3D photogrammetry can be used as a tool for digitally preserving preserved field specimens, and many of those same lessons can be applied to field-based 3D life-like specimens. This broader effort of collecting 3D scans of living field specimens would fit in nicely with the priorities of museums for high-throughput digitization and sharing (<xref rid="bib13" ref-type="bibr">Blagoderov et&#x000a0;al., 2012</xref>; <xref rid="bib47" ref-type="bibr">Hebert et&#x000a0;al., 2013</xref>). One possibility would be to combine 3D digital scans of organisms taken in the field, such as a flower, frog, or insect, together with the collection of DNA or other tissue. In such a case, specimens could then be released live, which would allow sampling in areas where traditional collection methods are challenging. Given that the bulk of characters that define many species can be accurately quantified through the inspection of life-like 3D digital models, as well as a cross-reference with DNA and metadata, it should be possible to identify species from each sample. As an example, for a sample of 3D models of various species of Philippine frogs, and frogs from the Atlanta Zoo, we used a set of characters from <xref rid="bib118" ref-type="bibr">Watters et&#x000a0;al. (2016)</xref>, which reviewed species identification characters for frogs. We used the 16 most common characters, among 42 in all, which <xref rid="bib118" ref-type="bibr">Watters et&#x000a0;al. (2016)</xref> have identified as most useful for frog species descriptions. Using these characters, we were able to correctly identify and measure the vast bulk of them from 3D models alone (85%, see <xref rid="mmc3" ref-type="supplementary-material">Data S1</xref>).</p><p id="p0075">We note that the physical and digital 3D scans complement each other. For example, the physical specimens provide valuable information on anatomy, whereas the living 3D digital specimens provide the ability to more easily share and visualize the specimen, as well as to visualize its overall posture and color. As an example, it was possible to obtain high-quality scans of five different frog species from several islands in the Philippines, and a snake and lizard species from different locations in Cyprus (<xref rid="bib58" ref-type="bibr">Irschick et&#x000a0;al., 2020c</xref>, <xref rid="mmc4" ref-type="supplementary-material">Data S2</xref>, see <ext-link ext-link-type="uri" xlink:href="https://sketchfab.com/DigitalLife3D" id="intref0045">https://sketchfab.com/DigitalLife3D</ext-link> for models). As the infrastructure, training, and protocols for this kind of 3D field-based documentation proceeds, it should be possible to achieve higher throughput. Moreover, the creation of 3D &#x0201c;digital specimens&#x0201d; would also open new opportunities for museum-based outreach and education. Although museums regularly house thousands of physical specimens of various species, the ability to create &#x0201c;digital specimens&#x0201d;, especially for larger megafauna (e.g., marine mammals, sharks), that are not well-represented in museums, is enticing (<xref rid="fig5" ref-type="fig">Figure&#x000a0;5</xref>). Although many scientists are currently creating 3D scans of various kinds of existing museum specimens (e.g., bones), there is also the possibility of a museum hosting purely digital specimens, or working with existing online databases (e.g., <ext-link ext-link-type="uri" xlink:href="http://morphosource.com" id="intref0050">morphosource.com</ext-link>, <ext-link ext-link-type="uri" xlink:href="http://idigbio.com" id="intref0055">idigbio.com</ext-link>) to share such digital specimens.</p></sec><sec id="sec4"><title>Body condition and scaling</title><p id="p0080">A significant challenge in working with living organisms is estimating basic parameters related to overall body size, which can be important for estimating body condition, and for understanding scaling, which examines how organisms change in various physical parameters as they grow, or as species evolve and become larger over evolutionary time. One challenge relates to the study of body condition, which can be defined as an animal&#x02019;s overall mass or volume relative to its structural size (often body length, <xref rid="bib60" ref-type="bibr">Jakob et&#x000a0;al., 1996</xref>). In general, wild animals are rarely afforded opportunities to regularly consume more food than they need, and hence, most live on a dietary tightrope, with fuel input being barely sufficient (or insufficient) to cope with energetic demands. This problem is especially important for animals that migrate long distances and consequently, expend large amounts of locomotor energy, such as birds and some marine mammals and fishes (<xref rid="bib68" ref-type="bibr">Labocha and Hayes, 2012</xref>). Body condition is commonly measured as body mass relative to body length, and while body mass is easily measured for smaller, more common organisms such as reptiles, fish, or insects, for larger megafauna such as marine mammals, sharks, and some terrestrial mammals, estimating mass accurately is challenging. The 3D reconstruction methods described here provide a new tool for scientists to estimate mass in such elusive megafauna, in combination with tools such as drones or remotely operated underwater vehicles (ROVs). For example, <xref rid="bib26" ref-type="bibr">Christiansen et&#x000a0;al. (2019)</xref> used aerial drones and photogrammetry methods to estimate the size and body mass of southern right whales. These images were used to create an accurate 3D model of one whale (<ext-link ext-link-type="uri" xlink:href="https://skfb.ly/6Ryut" id="intref0060">https://skfb.ly/6Ryut</ext-link>), from which one can estimate volumetrics. Although volume is not typically a direct measure of condition, gaining data on volume could represent an important tool for more accurately measuring condition, especially in animals that cannot be easily weighed. It is important to note that even if one were able to directly weigh a large animal such as a whale, one could not gain estimates of volume or density from mass data. Some of the 3D models included in Table&#x000a0;1 are currently being used to investigate similar, new methods of reconstructing body shapes, masses, and volumes, especially for sea turtles, sharks, and marine mammals (Irschick, ms in prep., Christiansen, ms in prep). As more data emerge on the body shape and estimated mass of megafauna such as whales, it should be possible to estimate volumetrics and surface areas of a wide range of species (<xref rid="fig3" ref-type="fig">Figure&#x000a0;3</xref>).<fig id="fig3"><label>Figure&#x000a0;3</label><caption><p>Scaled volumetric 3D models of the 14 animal species in Table&#x000a0;1, showing the variation in body size, volume and surface area</p><p>Animals are shown from the largest to smallest. The southern right whale is not shown, as it is too large.</p></caption><graphic xlink:href="gr3"/></fig></p><p id="p0085">With such data in hand, it would then be possible to test basic theories of energetics and life history, such as the theory that metabolic rate should scale in a particular manner with surface area and volume in mammals (<xref rid="bib20" ref-type="bibr">Brown and Sibly, 2006</xref>; <xref rid="bib36" ref-type="bibr">Enquist et&#x000a0;al., 1999</xref>; <xref rid="bib64" ref-type="bibr">Kleiber, 1961</xref>). Max <xref rid="bib64" ref-type="bibr">Kleiber (1961)</xref> argued that for the majority of mammals, basal metabolic rate B tends to increase as the 3/4 power when scaled against body mass. Various theories for why this pattern occurs exist, but one popular theory suggests that smaller animals will respire more per unit of body mass than larger animals as a greater proportion of their body mass consists of metabolically active tissues that impose higher maintenance costs. This idea in turn is based on assumptions about an organism&#x02019;s surface area to volume. Although there have been some efforts to estimate these parameters for a range of organisms, there are few good estimates of the surface areas and volumes of living animals, as measuring these on dead animals is likely to be often inaccurate. Although some elements of this theory have been tested across a small range of body sizes, expanding these data to larger marine mammals for example, and providing more accurate data on volumetrics, surface areas, and masses, would provide a stronger test of the theory. As an example of this approach, we estimated volumes, surface areas from 12 of the 14 species listed in Table&#x000a0;1 (for the flatback sea turtle, <italic>Natator depressus</italic>, and blacktip shark, <italic>Carcharhinus limbatus)</italic>, masses were not available, see <xref rid="tbl2" ref-type="table">Table&#x000a0;2</xref>). Volumes and surface areas were calculated from the calibrated 3D meshes within Blender. By plotting volume versus mass (<xref rid="fig4" ref-type="fig">Figure&#x000a0;4</xref>), we can gain an understanding of the basic relationship among these variables for a disparate group of animals ranging from small geckos to a massive whale. In plotting surface area versus mass, one observes an estimated slope of 0.64 (y&#x000a0;= 0.64x+1.12, F&#x000a0;= 547.8, R<sup>2</sup>&#x000a0;= 0.98, p&#x000a0;&#x0003c;&#x000a0;0.001), whereas for volume versus mass, the estimated slope is 1.01 (y&#x000a0;= 1.01x-0.03, F&#x000a0;= 959.1, R<sup>2</sup>&#x000a0;= 0.99, p&#x000a0;&#x0003c;&#x000a0;0.001). Although this plot intermixes vastly different organisms, such as endotherms, ectotherms, and terrestrial and aquatic animals, it provides a glimpse of future opportunities for testing theories of scaling and energetics, ideally for a much wider group of species.<table-wrap position="float" id="tbl2"><label>Table 2</label><caption><p>Masses, volumes, and surface areas for the 14 animal species in <xref rid="tbl1" ref-type="table">Table 1</xref></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Common name</th><th>Mass (g)</th><th>Volume (cm<sup>3</sup>)</th><th>Surface area (cm<sup>2</sup>)</th></tr></thead><tbody><tr><td>Southern Right Whale<sup>&#x02217;</sup></td><td>37,041,435</td><td>49,085,723</td><td>1,414,121</td></tr><tr><td>Southern white rhinoceros</td><td>2,225,000</td><td>1,499,727</td><td>100,202</td></tr><tr><td>Blacktip shark</td><td>N/A</td><td>25,108</td><td>8,086</td></tr><tr><td>Harbor porpoise<sup>&#x00026;</sup></td><td>67,100</td><td>51,303</td><td>10,675</td></tr><tr><td>Loggerhead sea turtle</td><td>37,000</td><td>39,824</td><td>9,636</td></tr><tr><td>Flatback sea turtle</td><td>N/A</td><td>103,319</td><td>19,334</td></tr><tr><td>Green sea turtle</td><td>14,240</td><td>17,576</td><td>5,517</td></tr><tr><td>Kemps ridley sea turtle</td><td>2,850</td><td>3,775</td><td>2,053</td></tr><tr><td>Hawksbill sea turtle</td><td>220</td><td>243</td><td>335</td></tr><tr><td>Cyprus racer snake</td><td>479</td><td>534</td><td>1,130</td></tr><tr><td>Sling-tailed agama lizard</td><td>78</td><td>18</td><td>87</td></tr><tr><td>Tokay gecko lizard</td><td>52</td><td>104</td><td>236</td></tr><tr><td>Flying gecko lizard</td><td>14</td><td>17</td><td>100</td></tr><tr><td>House gecko</td><td>5</td><td>3</td><td>28</td></tr></tbody></table></table-wrap><fig id="fig4"><label>Figure&#x000a0;4</label><caption><p>Scaling parameters from 3D models</p><p>A plot of estimated mass versus surface area (A), and mass versus volume (B)&#x000a0;for 12 of the 14 animal species in Table&#x000a0;1 (no mass data were available for the flatback sea turtle or the blacktip shark).</p></caption><graphic xlink:href="gr4"/></fig></p></sec><sec id="sec5"><title>Applications to bioinspiration and robotics</title><p id="p0090">One of the most exciting implications of recreating living animals accurately is for the development of bioinspired materials and robotics (<xref rid="bib40" ref-type="bibr">Fish and Lauder, 2017</xref>; <xref rid="bib66" ref-type="bibr">Kim et&#x000a0;al., 2013</xref>; <xref rid="bib72" ref-type="bibr">Lauder et&#x000a0;al., 2011</xref>; <xref rid="bib115" ref-type="bibr">Vargas et&#x000a0;al., 2008</xref>; <xref rid="bib122" ref-type="bibr">Yang et&#x000a0;al., 2016</xref>). On the one hand, an important part of bioinspired material research is the investigation of substrate properties and structural elements of surfaces at a nanoscale level. This aspect of bioinspired materials will continue to be relevant, but another aspect is the investigation of animal form and movement at a whole-organism level. 3D models allow researchers to not only test the efficacy of biological designs, but also &#x0201c;forbidden&#x0201d; phenotypes that don&#x02019;t exist in the natural world, but which could exist, such as a ten-armed octopus. Over the past decade, there has been a large growth in the number and kinds of bioinspired robots, including those designed after manta rays, pike, snakes, flies, and kangaroos, among others (e.g., <xref rid="bib115" ref-type="bibr">Vargas et&#x000a0;al., 2008</xref>). The goal of such mimicry is to recreate some of the performance capacities of these animals, which include high levels of energetic efficiency, velocity, and acceleration. For example, one class of robots is largely inspired by fast-moving fish such as pike (<italic>Esox Lucius)</italic>, tuna species, and sharks (<xref rid="bib72" ref-type="bibr">Lauder et&#x000a0;al., 2011</xref>; <xref rid="bib71" ref-type="bibr">Lauder, 2015</xref>). Extensive research on the interaction between body shape and locomotor ability in fish has shown that certain body shapes are ideal for high-speed movements, such as the fusiform shapes of laminid sharks (<xref rid="bib71" ref-type="bibr">Lauder, 2015</xref>). Therefore, recreating the body shapes of sharks provides a platform for scientists to test and recreate these shapes in physical (e.g., flow-tank) and computational environments. Indeed, it is possible to think of a base accurate 3D model for a particular species of a known size, age, and sex, as serving as a valuable template for many other studies. Although there is some attempt to replicate the accurate body shapes of animals using CT scanning or 3D photogrammetry, much of the field of bioinspired robots rely on approximations of animal shape. Although this approach has some validity, animal bodies often contain subtle features whose function may not be obvious upon first inspection. For example, computational and flow-tank studies showed that the scalloped edges of the pectoral fins of humpback whales (<italic>Megaptera novaeangliae</italic>) enable them to move more efficiently through the water (<xref rid="bib40" ref-type="bibr">Fish and Lauder, 2017</xref>). It is also useful to consider that the widths, depths, and thicknesses of animal bodies, and various appendages can have significant advantages for certain locomotor tasks. Indeed, artistic estimation of body shapes has the potential to alter basic body proportions misleadingly. Furthermore, animal morphology often varies across different life stages, and thus creating a &#x0201c;generic&#x0201d; design that is not clearly tied to a particular life stage or sex, is problematic. The 3D models discussed here can, therefore, be used as testing prototypes for further development as possible shapes for robots. Because of the flexible nature of these models, they can also be easily altered in terms of their shape, which would allow investigators to visualize new phenotypes that have not yet evolved, and which may possess novel functional capacities.</p></sec><sec id="sec6"><title>Computational fluids dynamics</title><p id="p0095">One of the most exciting research opportunities for the usage of 3D modeling is computational fluids dynamics (CFD) research and more generally, computer simulation (<xref rid="bib39" ref-type="bibr">Fauci and Peskin, 1988</xref>; <xref rid="bib40" ref-type="bibr">Fish and Lauder, 2017</xref>; <xref rid="bib69" ref-type="bibr">Laforsch et&#x000a0;al., 2012</xref>; <xref rid="bib71" ref-type="bibr">Lauder, 2015</xref>). The use of CFD is widespread within the realm of mechanical engineering and animal locomotion and is viewed as a vital tool for understanding the consequences of variation in body shape on locomotor efficiency in a range of animals, primarily those that move in air or water, such as fish, marine mammals, and birds, among others (<xref rid="bib27" ref-type="bibr">Cohen and Cleary 2010</xref>; <xref rid="bib33" ref-type="bibr">Dong et&#x000a0;al., 2020</xref>; <xref rid="bib79" ref-type="bibr">Liu, 2002</xref>; <xref rid="bib85" ref-type="bibr">Nakata et&#x000a0;al., 2011</xref>; <xref rid="bib96" ref-type="bibr">Ravi et&#x000a0;al., 2020</xref>). It is also critical for better tag designs in biologging research, where the interplay between body shape and form factor of the tag is critical for the developing drag when the tagged animals move in air and water (<xref rid="bib102" ref-type="bibr">Shorter et&#x000a0;al., 2014</xref>; <xref rid="bib114" ref-type="bibr">Vandenabeele et&#x000a0;al., 2014</xref>).</p><p id="p0100">A typical workflow involves the creation of a basic anatomical shape of interest and then testing how various aspects, such as the speed of air or water flow, or other factors in either a computational simulated environment or in a flow-tank, impact aspects such as drag. In addition, there may be particular interest paid to particular body parts, such as that of a fin or a limb, for example. Some investigators go to great lengths to accurately reconstruct the proper shapes of animal bodies, or their constituent parts, such as by using CT-reconstruction or 3D photogrammetry (e.g., <xref rid="bib80" ref-type="bibr">Liu et&#x000a0;al., 2017</xref>; <xref rid="bib117" ref-type="bibr">Wang et&#x000a0;al., 2020</xref>). However, in many cases, an approximate model is created that may not be anatomically correct or may neglect aspects of variation related to age or sex. The value of modeling particular individuals, or possibly multiple individuals, as opposed to the generic reconstruction of a particular species, is that it is possible to examine the consequences of variation among individuals, ages, and sexes, among other aspects. As an example, ontogenetic changes are well-documented for many animals, including lizards, sharks, and sea turtles, and in some cases, some of these changes, such as a relatively enlarged caudal fin in younger sharks, are likely to have a significant impact on locomotor efficiency (<xref rid="bib23" ref-type="bibr">Carrier, 1996</xref>; <xref rid="bib42" ref-type="bibr">Fu et&#x000a0;al., 2016</xref>). The utility of this approach was demonstrated by <xref rid="bib34" ref-type="bibr">Dudley et&#x000a0;al. (2016)</xref>, who used 3D models of various sea turtle species to examine the potential impacts of climate change in nesting leatherback sea turtle females (<italic>Dermochelys coriacea).</italic> As an example of the potential value of CFD work with our models, we used a 3D scan from a subadult green sea turtle (<italic>Chelonia mydas</italic>), and then examined locomotor aspects such as drag, power, and power/volume for three sizes (hatchling, subadult, and adult, <xref rid="mmc4" ref-type="supplementary-material">Data S2</xref>). Drag increased markedly with sea turtle size, with a noticeable jump between a hatchling and a subadult (<xref rid="fig5" ref-type="fig">Figure&#x000a0;5</xref>). Similarly, the power required for swimming was also noticeably larger for larger sea turtles compared to smaller ones. Interestingly, however, the power/volume was roughly two orders of magnitude higher for the smallest sea turtles than for the largest sea turtle when they are traveling at the same speed. Obviously, it would be beneficial to rerun these analyses in more detail with more specific models (i.e., one hatchling, one subadult, one adult), ideally from the same breeding population. However, the data seem to show that there are additional power requirements for smaller sea turtles for locomotion that could be ecologically relevant. Also, the drag and power estimates now offer some values that could be used in a larger analysis that considers the total distance moved by sea turtles and could result in a total estimated energetic budget. This analysis points toward how one could use the base 3D model and alter basic aspects, such as carapace shape, or limb or head shape. Finally, one could perform similar analyses with all the 3D models of various sea turtle species, such as comparing the same life-stage (e.g., adults, subadults) during various kinds of flow regimes (e.g., steady, unsteady flow dynamics).<fig id="fig5"><label>Figure&#x000a0;5</label><caption><p>Swimming parameters for scaled green sea turtle 3D model</p><p>Plots of swimming speed (x axis) versus swimming thrust force (A), swimming power (B), and standardized swimming power (C)&#x000a0;based on computational fluids dynamics (CFD) simulations for three simulated age classes from 3D models of green sea turtles (<italic>Chelonia mydas</italic>).</p></caption><graphic xlink:href="gr5"/></fig></p></sec><sec id="sec7"><title>Machine learning applications for locomotion</title><p id="p0105">Studies of animal motion have informed us on the evolution of form and function (<xref rid="bib12" ref-type="bibr">Biewener and Patek, 2017</xref>; <xref rid="bib55" ref-type="bibr">Irschick and Higham, 2016</xref>), and most research on animal locomotion has focused on empirical studies of kinematics, kinetics, or anatomy (<xref rid="bib4" ref-type="bibr">Alexander, 2003</xref>; <xref rid="bib12" ref-type="bibr">Biewener and Patek, 2017</xref>; <xref rid="bib55" ref-type="bibr">Irschick and Higham, 2016</xref>; <xref rid="bib71" ref-type="bibr">Lauder, 2015</xref>). Although such studies are valuable, they are only one way to study animal movement. One complement to this approach is computational research (e.g., <xref rid="bib32" ref-type="bibr">Dong et&#x000a0;al., 2010</xref>; <xref rid="bib39" ref-type="bibr">Fauci and Peskin, 1988</xref>; <xref rid="bib52" ref-type="bibr">Hutchinson et&#x000a0;al., 2007</xref>; <xref rid="bib79" ref-type="bibr">Liu, 2002</xref>; <xref rid="bib115" ref-type="bibr">Vargas et&#x000a0;al., 2008</xref>) which can provide scientists, educators, and the public, with tools and techniques to analyze and visualize complex datasets and processes related to locomotion. For example, machine learning approaches offer exciting potential for accelerating the pace of the creation of 3D models. Another application is to integrate the movement of living organisms into static 3D reconstructions of these organisms. Currently, animated movements of 3D models are created by hand, and while metrics (e.g., frequency, amplitude) can be inserted into these models, these represent only rough surrogates of the motion of live animals.</p><p id="p0110">Integration of movement with shape would be beneficial for scientists, educators, and 3D artists. As one example, scientists interested in biodiversity could use such methods to create digital online &#x0201c;museums&#x0201d; of accurate, full-color, and moving animals from different regions to compare against one another. For science educators, showcasing accurate and moving 3D models of animals would be beneficial for demonstrating principles of biomechanics. With the emergence of new virtual reality (VR) and augmented reality (AR) hardware, students could perceive moving animals and their surroundings in immersive environments. Finally, such a tool would have commercial applications, as there is a demand for realistic 3D animal models in the animation industry, especially for films and video games.</p><p id="p0115">The typical workflow to create an animated 3D model is for artists and modelers to manually design the geometric representation of a 3D model, texture it (i.e., add color to the geometry), and finally create a rig. A rig is a hierarchical set of interconnected bones, called an animation skeleton in the computer graphics literature (<xref rid="bib81" ref-type="bibr">Magnenat-Thalmann et&#x000a0;al., 1988</xref>), which is used as a control data structure to animate the geometric representation. Machine learning approaches would be valuable here, as manually creating rigs requires expertise and significant training. An alternative approach is to use 3D photogrammetry methods to capture animal body shapes and motion to reconstruct a rig. However, these methods are limited to reconstruction from static snapshots. Although one can capture several different snapshots representing key body shapes and deformations that could be combined into a final moving shape (<xref rid="bib5" ref-type="bibr">Anguelov et&#x000a0;al., 2005</xref>), such an approach is time-consuming.</p><p id="p0120">As an example of how machine learning approaches can be integrated with the creation and study of life-like 3D models for scientific applications, we describe a neural net-based computer algorithm that automatically translates animal motion captured in the video into animated 3D models of animals that can be readily visualized in graphics, augmented and virtual reality environments (see <xref rid="fig6" ref-type="fig">Figure&#x000a0;6</xref>, <xref rid="fig7" ref-type="fig">Figure&#x000a0;7</xref> and <xref rid="fig7" ref-type="fig">7</xref> and <xref rid="mmc6" ref-type="supplementary-material">Video S1</xref>). The contribution of this machine learning software (ANATAR, from ANimal AuTomated Animation and Rigging) is 2-fold (<xref rid="mmc5" ref-type="supplementary-material">Data S3</xref>). First, we introduce an automatic algorithm that creates an internal motion rig for a given body shape. Our algorithm uses the shape of the object, in this case, 3D models of fish, as a case example, to estimate the rig. Second, we integrate this method with harvesting motion data from live moving organisms and place them into our automated rigs. Together, these methods provide the foundations of a method for creating motion rigs for a variety of living organisms in a standardized, time- and cost-efficient manner. As a case example of our video-to-rig method, we use previously published video movement from a live chain catshark (<italic>Scyliorhinus retifer</italic>) which was placed into a previously created 3D model of a blacktip shark (see <xref rid="fig6" ref-type="fig">Figures&#x000a0;6</xref> and <xref rid="fig7" ref-type="fig">7</xref>). Although these are different species, the motion of sharks shares some commonality and offers a proof-of-concept that could ultimately be applied to motion on the same animals for which 3D models are created. Applying kinematic models measured for one species to a diversity of 3D body shapes in other species allows the investigation of the effect of body shape on locomotor performance using a controlled experimental method.<fig id="fig6"><label>Figure&#x000a0;6</label><caption><p>Input and output of the machine learning method outlined in <xref rid="mmc5" ref-type="supplementary-material">Data S3</xref></p><p>Given an input video capturing the motion of a live chain catshark (<italic>Scyliorhinus retifer</italic>).</p><p>(A) Our method automatically generates animations of several fish-like models closely following the captured motion.</p><p>(B) We also refer to our <xref rid="mmc1" ref-type="supplementary-material">Video S1</xref> showing the whole motion.</p></caption><graphic xlink:href="gr6"/></fig><fig id="fig7"><label>Figure&#x000a0;7</label><caption><p>Pipeline of the machine learning method outlined in <xref rid="mmc5" ref-type="supplementary-material">Data S3</xref></p><p>(A&#x02013;D) given an input 3D model, created through photogrammetry or modeled by an artist, (B)&#x000a0;our method employs a neural network to rig it with an animation skeleton, then (C)&#x000a0;using an input video capturing the locomotion of a real animal, (D)&#x000a0;another neural network controls the skeleton and animates the 3D model so that its motion closely follows the input video.</p></caption><graphic xlink:href="gr7"/></fig></p><p id="p0125">
<supplementary-material content-type="local-data" id="mmc6"><caption><title>Video S1. A visual explanation of a neural network for integrating animal movement with shape</title></caption><media xlink:href="mmc6.mp4"/></supplementary-material>
</p><p id="p0130">Our method provides a mechanism for creating an automated rig for a certain body shape. Normally, the process of creating rigs for motion is time-consuming and subjective. In the context of the broader workflow of creating 3D models of living animals and creating rigs that represent their actual motion automatically, our method is a first step. Ultimately, a fuller workflow should be (1) recreate an accurate 3D model of living animal, (2) use a version of our automated motion rig for an animal based on its 3D shape, (3) film living animal moving (e.g., swimming, running), and (4) use information from the motion of the animal to inform movements of the rig created in step (2). Our method integrates all four steps, with a special focus on elucidating steps (2) and (4). Biologists can use this algorithm to create rigs for fish-like organisms for modeling research, such as for computational fluid dynamics research. As our algorithm continues to develop with new training data, it could be used for a wider range of shapes and organisms. This thus represents a good example of how machine learning approaches the creation of 3D models to create both accurate shapes and motions.</p></sec><sec id="sec8"><title>Educational and conservation uses</title><p id="p0135">The growth of hardware and software for visualizing objects in 3D presents new opportunities for incorporating 3D models for education, both at the K-12 (i.e., early education) and University levels (<xref rid="bib31" ref-type="bibr">Donalek et&#x000a0;al., 2014</xref>; <xref rid="bib61" ref-type="bibr">Jang et&#x000a0;al., 2017</xref>; <xref rid="bib78" ref-type="bibr">Lischer-Katz and Cook, 2017</xref>; <xref rid="bib86" ref-type="bibr">Pantelidis, 2010</xref>; <xref rid="bib91" ref-type="bibr">Pober and Cook, 2016</xref>; <xref rid="bib99" ref-type="bibr">Seth et&#x000a0;al., 2011</xref>). Over the past decade, there have been substantial improvements in virtual reality (VR) headsets, which now allow people to visualize 3D scenes with remarkable clarity and realism. As the cost of these systems goes down, they are now increasingly accessible to a wider range of entities, including elementary and secondary K-12 schools. As an example of a public resource that is tailored to educational needs, one of us (Savvas Zotos and colleagues), has used many of the methods described here to create 3D models of the reptiles of Cyprus (<ext-link ext-link-type="uri" xlink:href="http://3dreptiles.cs.ucy.ac.cy/public/our-reptiles" id="intref0065">http://3dreptiles.cs.ucy.ac.cy/public/our-reptiles</ext-link>), which offer a tremendous educational and scientific tool. There is also growing evidence that such visualization tools offer a learning benefit for students with some learning disabilities, compared to more traditional teaching methods (e.g., textbooks, PowerPoint slides, <xref rid="bib63" ref-type="bibr">Ke and Im, 2013</xref>). For natural history museums that are seeking novel ways to engage visitors, visualizing 3D models using a range of techniques such as integrated video projectors, or VR headsets, offers tantalizing opportunities, especially if there is the ability to visualize how a living animal looks and moves, as opposed to an artistic recreation. At a different level, there have been increasing concerns raised by the public about the presence of megafauna such as killer whales (<italic>Orcinus orca)</italic>, elephants, rhinos, and tigers within aquaria and zoos (<xref rid="bib101" ref-type="bibr">Shapiro, 2018</xref>), and it is likely that many organizations will forgo the acquisition of new animals once their current individuals have died. The rise of high-fidelity virtual 3D experiences may take the place of many of these live animal experiences, as there will likely remain an intense interest in these animals moving forward. If supplemented with audio or text-driven content, these kinds of virtual experiences could prove easily as motivational and educational as witnessing a live animal in a zoo setting.</p></sec><sec id="sec9"><title>Prospectus for the future</title><p id="p0140">Over the past 20 years, there have been tremendous advances in computer processing power, 3D scanning techniques, and software for processing these 3D data. As discussed here, these methods are now being applied to living animals, but we are only at the beginning of this process. New techniques and workflows are needed to both effectively scan some kinds of organisms, such as plants or small invertebrates, but also to do so in a time-efficient manner. Currently, the workflows for the 3D scanning of live animals are overly time-consuming, and new efficiencies will need to be accomplished before larger taxonomic sampling is possible. The sheer diversity of most animal groups stands as a significant challenge, as 4000&#x000a0;+ species of lizards and 11,000&#x000a0;+ species of birds indicate. As an example of the kind of innovation that is needed, some research groups have shown great progress in 3D scanning methods for preserved insects (<xref rid="bib90" ref-type="bibr">Plum and Labonte, 2021</xref>), with the potential promise of ultimately scanning live insects. New innovations in 3D techniques for scanning electron microscopy could prove useful for creating 3D scans of various kinds of microorganisms (<xref rid="bib93" ref-type="bibr">Reiss and Eulitz, 2015</xref>). Plants represent perhaps one of the greatest challenges for proper 3D reconstruction owing to their tremendous variation in size and shape and developing physical scanning protocols for this group will be important. One of the most promising avenues for accelerating the creation of accurate life-like 3D scans is machine-learning techniques (<xref rid="bib73" ref-type="bibr">Lawrance, 1994</xref>). Such techniques might be able to overcome the time-consuming process of fixing small imperfections in 3D meshes, a process that currently has to be done by hand. Although technical innovation in hardware and software has transformed how we view 3D data, there is a need for more standardization of how the scientific community creates, views, deposits, and shares 3D data, especially in the context of traditional natural history collections, such as museums. <xref rid="bib46" ref-type="bibr">Grayburn et&#x000a0;al., 2019</xref> discuss these issues in detail in the context of academic libraries, and perhaps some of those lessons can be applied to 3D life-like models. The opportunity to gather 3D life-like specimens rapidly and then disseminate them in a standardized manner is an exciting opportunity for scientists and natural history collections, and we are eagerly anticipating this future.</p></sec></body><back><ref-list id="cebib0010"><title>References</title><ref id="bib1"><element-citation publication-type="journal" id="sref1"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>D.C.</given-names></name><name><surname>Berns</surname><given-names>C.M.</given-names></name><name><surname>Kozak</surname><given-names>K.H.</given-names></name><name><surname>Wiens</surname><given-names>J.J.</given-names></name></person-group><article-title>Are rates of species diversification correlated with rates of morphological evolution?</article-title><source>Proc. Royal Soc. London B</source><volume>276</volume><year>2009</year><fpage>2729</fpage></element-citation></ref><ref id="bib2"><element-citation publication-type="journal" id="sref2"><person-group person-group-type="author"><name><surname>Albertson</surname><given-names>R.C.</given-names></name><name><surname>Streelman</surname><given-names>J.T.</given-names></name><name><surname>Kocher</surname><given-names>T.D.</given-names></name></person-group><article-title>Directional selection has shaped&#x000a0;the oral jaws of lake Malawi cichlid fishes</article-title><source>Proc. Nat. Acad. Sci. USA</source><volume>100</volume><year>2003</year><fpage>5252</fpage><lpage>5257</lpage><pub-id pub-id-type="pmid">12704237</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal" id="sref3"><person-group person-group-type="author"><name><surname>Aldridge</surname><given-names>K.</given-names></name><name><surname>Boyadjiev</surname><given-names>S.A.</given-names></name><name><surname>Capone</surname><given-names>G.T.</given-names></name><name><surname>Deleon</surname><given-names>V.B.</given-names></name><name><surname>Richtsmeier</surname><given-names>J.T.</given-names></name></person-group><article-title>Precision and error of three-dimensional phenotypic measures acquired from 3dmd photogrammetric images</article-title><source>Am. J. Med. Gen.</source><volume>13</volume><year>2005</year><fpage>247</fpage><lpage>253</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="book" id="sref4"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>M.N.</given-names></name></person-group><part-title>Animal Locomotion</part-title><year>2003</year><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal" id="sref5"><person-group person-group-type="author"><name><surname>Anguelov</surname><given-names>D.</given-names></name><name><surname>Srinivasan</surname><given-names>P.</given-names></name><name><surname>Koller</surname><given-names>D.</given-names></name><name><surname>Thrun</surname><given-names>S.</given-names></name><name><surname>Rodgers</surname><given-names>J.</given-names></name><name><surname>Davis</surname><given-names>J.</given-names></name></person-group><article-title>Scape: shape completion and animation of people</article-title><source>ACM Trans. Graph.</source><volume>24</volume><year>2005</year><fpage>408</fpage><lpage>416</lpage></element-citation></ref><ref id="bib6"><element-citation publication-type="other" id="sref6"><person-group person-group-type="author"><collab>Alicevision</collab></person-group><article-title>Meshroom: A 3d reconstruction software</article-title><ext-link ext-link-type="uri" xlink:href="https://github.com/alicevision/meshroom" id="intref0075">https://github.com/alicevision/meshroom</ext-link><year>2018</year></element-citation></ref><ref id="bib7"><element-citation publication-type="journal" id="sref7"><person-group person-group-type="author"><name><surname>Allmon</surname><given-names>W.D.</given-names></name></person-group><article-title>The value of natural history collections</article-title><source>Curator</source><volume>37</volume><year>1994</year><fpage>83</fpage><lpage>89</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal" id="sref8"><person-group person-group-type="author"><name><surname>Amado</surname><given-names>T.</given-names></name><name><surname>Pinto</surname><given-names>M.</given-names></name><name><surname>Olalla-T&#x000e1;rraga</surname><given-names>M.</given-names></name></person-group><article-title>Anuran 3d models reveal the relationship between surface area-to-volume ratio and climate</article-title><source>J.&#x000a0;Biogeogr.</source><volume>46</volume><year>2019</year><fpage>1429</fpage><lpage>1437</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="journal" id="sref9"><person-group person-group-type="author"><name><surname>Baqersad</surname><given-names>J.</given-names></name><name><surname>Poozesh</surname><given-names>P.</given-names></name><name><surname>Niezrecki</surname><given-names>C.</given-names></name><name><surname>Avitabile</surname><given-names>P.</given-names></name></person-group><article-title>Photogrammetry and optical methods in structural dynamics &#x02013; a review</article-title><source>Mech. Syst. Syst. Proc.</source><volume>86</volume><year>2017</year><fpage>17</fpage><lpage>34</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal" id="sref10"><person-group person-group-type="author"><name><surname>Baran</surname><given-names>I.</given-names></name><name><surname>Popovi&#x00107;</surname><given-names>J.</given-names></name></person-group><article-title>Automatic rigging and animation of 3d characters</article-title><source>ACM Trans. Graphics</source><volume>26</volume><year>2007</year><fpage>72</fpage></element-citation></ref><ref id="bib11"><element-citation publication-type="book" id="sref11"><person-group person-group-type="author"><name><surname>Biewener</surname><given-names>A.</given-names></name></person-group><part-title>Animal Locomotion</part-title><year>2003</year><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="book" id="sref12"><person-group person-group-type="author"><name><surname>Biewener</surname><given-names>A.</given-names></name><name><surname>Patek</surname><given-names>S.</given-names></name></person-group><part-title>Animal Locomotion</part-title><year>2017</year><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib13"><element-citation publication-type="journal" id="sref13"><person-group person-group-type="author"><name><surname>Blagoderov</surname><given-names>V.</given-names></name><name><surname>Kitching</surname><given-names>I.J.</given-names></name><name><surname>Livermore</surname><given-names>L.</given-names></name><name><surname>Simonsen</surname><given-names>T.J.</given-names></name><name><surname>Smith</surname><given-names>V.S.</given-names></name></person-group><article-title>No specimen left behind: industrial scale digitization of natural history collections</article-title><source>ZooKeys</source><volume>209</volume><year>2012</year><fpage>133</fpage><lpage>146</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="journal" id="sref14"><person-group person-group-type="author"><name><surname>Bythell</surname><given-names>J.</given-names></name><name><surname>Pan</surname><given-names>P.</given-names></name><name><surname>Lee</surname><given-names>J.</given-names></name></person-group><article-title>Three-dimensional morphometric measurements of reef corals using underwater photogrammetry techniques</article-title><source>Coral Reefs</source><volume>20</volume><year>2001</year><fpage>193</fpage><lpage>199</lpage></element-citation></ref><ref id="bib15"><element-citation publication-type="journal" id="sref15"><person-group person-group-type="author"><name><surname>Bock</surname><given-names>W.J.</given-names></name></person-group><article-title>Concepts and methods in Ecomorphology</article-title><source>J.&#x000a0;Biosci.</source><volume>19</volume><year>1994</year><fpage>403</fpage><lpage>413</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="book" id="sref16"><person-group person-group-type="author"><name><surname>Bot</surname><given-names>J.</given-names></name><name><surname>Irschick</surname><given-names>D.J.</given-names></name></person-group><part-title>Using 3d photogrammetry to create open-access models of live animals using open source 2d And 3d software solutions open source 2d &#x00026; 3d software solutions</part-title><person-group person-group-type="editor"><name><surname>Grayburn</surname><given-names>J.</given-names></name><name><surname>Lischer-Katz</surname><given-names>Z.</given-names></name><name><surname>Golubiewski-Davis</surname><given-names>K.</given-names></name><name><surname>Ikeshoji-Orlati</surname><given-names>V.</given-names></name></person-group><source>3d/Vr In The Academic Library: Emerging Practices And Trends</source><year>2019</year><fpage>54</fpage><lpage>72</lpage><comment>Clir Reports, Clir Publication No. 176</comment></element-citation></ref><ref id="bib17"><element-citation publication-type="journal" id="sref17"><person-group person-group-type="author"><name><surname>Boyer</surname><given-names>D.M.</given-names></name><name><surname>Puente</surname><given-names>J.</given-names></name><name><surname>Gladman</surname><given-names>J.T.</given-names></name><name><surname>Glynn</surname><given-names>C.</given-names></name><name><surname>Mukherjee</surname><given-names>S.</given-names></name><name><surname>Yapuncich</surname><given-names>G.S.</given-names></name><name><surname>Daubechies</surname><given-names>I.</given-names></name></person-group><article-title>A new fully automated approach for aligning and comparing shapes</article-title><source>Anat. Record.</source><volume>298</volume><year>2015</year><fpage>249</fpage><lpage>276</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="book" id="sref18"><person-group person-group-type="author"><name><surname>Boykov</surname><given-names>Y.</given-names></name><name><surname>Veksler</surname><given-names>O.</given-names></name><name><surname>Zabih</surname><given-names>R.</given-names></name></person-group><part-title>Fast approximate energy minimization via graph cuts</part-title><source>Proceedings of The Seventh IEEE International Conference on Computer Vision 1</source><year>1999</year><fpage>377</fpage><lpage>384</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="journal" id="sref19"><person-group person-group-type="author"><name><surname>Bradley</surname><given-names>R.D.</given-names></name><name><surname>Bradley</surname><given-names>L.C.</given-names></name><name><surname>Garner</surname><given-names>H.J.</given-names></name><name><surname>Baker</surname><given-names>R.J.</given-names></name></person-group><article-title>Assessing the value of natural history collections and addressing issues regarding long-term growth and care</article-title><source>Bioscience</source><volume>64</volume><year>2014</year><fpage>1150</fpage><lpage>1158</lpage></element-citation></ref><ref id="bib20"><element-citation publication-type="journal" id="sref20"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>J.H.</given-names></name><name><surname>Sibly</surname><given-names>R.M.</given-names></name></person-group><article-title>Life-history evolution under A production constraint</article-title><source>Proc. Nat. Acad. Sci. USA</source><volume>47</volume><year>2006</year><fpage>17595</fpage><lpage>17599</lpage></element-citation></ref><ref id="bib21"><element-citation publication-type="journal" id="sref21"><person-group person-group-type="author"><name><surname>Bronstein</surname><given-names>M.M.</given-names></name><name><surname>Bruna</surname><given-names>J.</given-names></name><name><surname>Lecun</surname><given-names>Y.</given-names></name><name><surname>Szlam</surname><given-names>A.</given-names></name><name><surname>Vandergheynst</surname><given-names>P.</given-names></name></person-group><article-title>Geometric deep learning: going beyond Euclidean data</article-title><source>IEEE Signal Proc. Mag.</source><year>2017</year><fpage>18</fpage><lpage>42</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="journal" id="sref22"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>R.M.</given-names></name><name><surname>Siler</surname><given-names>C.D.</given-names></name><name><surname>Oliveros</surname><given-names>C.H.</given-names></name><name><surname>Esselstyn</surname><given-names>J.A.</given-names></name><name><surname>Diesmos</surname><given-names>A.C.</given-names></name><name><surname>Hosner</surname><given-names>P.A.</given-names></name><name><surname>Linkem</surname><given-names>C.W.</given-names></name><name><surname>Barley</surname><given-names>A.J.</given-names></name><name><surname>Oaks</surname><given-names>J.R.</given-names></name><name><surname>Sanguila</surname><given-names>M.B.</given-names></name><etal/></person-group><article-title>Evolutionary processes of diversification in a model island archipelago</article-title><source>Ann. Rev. Ecol. Syst.</source><volume>44</volume><year>2013</year><fpage>411</fpage><lpage>435</lpage></element-citation></ref><ref id="bib23"><element-citation publication-type="journal" id="sref23"><person-group person-group-type="author"><name><surname>Carrier</surname><given-names>D.R.</given-names></name></person-group><article-title>Ontogenetic limits on locomotor performance</article-title><source>Phys. Zool.</source><volume>69</volume><year>1996</year><fpage>467</fpage><lpage>488</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal" id="sref24"><person-group person-group-type="author"><name><surname>Chiari</surname><given-names>Y.</given-names></name><name><surname>Wang</surname><given-names>B.</given-names></name><name><surname>Rushmeier</surname><given-names>H.</given-names></name><name><surname>Caccone</surname><given-names>A.</given-names></name></person-group><article-title>Using digital images to reconstruct three-dimensional biological forms: a new tool for morphological studies</article-title><source>Biol. J. Linn. Soc.</source><volume>95</volume><year>2008</year><fpage>425</fpage><lpage>436</lpage></element-citation></ref><ref id="bib25"><element-citation publication-type="journal" id="sref25"><person-group person-group-type="author"><name><surname>Christiansen</surname><given-names>F.</given-names></name><name><surname>Vivier</surname><given-names>F.</given-names></name><name><surname>Charlton</surname><given-names>C.</given-names></name><name><surname>Ward</surname><given-names>R.</given-names></name><name><surname>Amerson</surname><given-names>A.</given-names></name><name><surname>Burnell</surname><given-names>S.</given-names></name><name><surname>Bejder</surname><given-names>L.</given-names></name></person-group><article-title>Maternal body size and condition determine calf growth rates in southern right whales</article-title><source>Mar. Ecol. Prog. Series</source><volume>592</volume><year>2018</year><fpage>267</fpage><lpage>281</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal" id="sref26"><person-group person-group-type="author"><name><surname>Christiansen</surname><given-names>F.</given-names></name><name><surname>Sironi</surname><given-names>M.</given-names></name><name><surname>Moore</surname><given-names>M.J.</given-names></name><name><surname>Di Martino</surname><given-names>M.</given-names></name><name><surname>Ricciardi</surname><given-names>M.</given-names></name><name><surname>Warick</surname><given-names>H.</given-names></name><name><surname>Irschick</surname><given-names>D.J.</given-names></name><name><surname>Gutierrez</surname><given-names>R.</given-names></name><name><surname>Uhart</surname><given-names>M.M.</given-names></name></person-group><article-title>Estimating body mass of free-living whales using aerial photogrammetry and 3d volumetrics</article-title><source>Meth. Ecol. Evol.</source><volume>10</volume><year>2019</year><fpage>2034</fpage><lpage>2044</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal" id="sref27"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>R.C.Z.</given-names></name><name><surname>Cleary</surname><given-names>P.W.</given-names></name></person-group><article-title>Computational studies of the locomotion of dolphins and sharks using smoothed particle hydrodynamics</article-title><source>IFMBE Proc.</source><volume>31</volume><year>2010</year></element-citation></ref><ref id="bib28"><element-citation publication-type="other" id="sref28"><person-group person-group-type="author"><name><surname>Community</surname><given-names>B.O.</given-names></name></person-group><article-title>Blender - a 3d modelling and rendering package. Stichting blender foundation</article-title><ext-link ext-link-type="uri" xlink:href="http://www.blender.org" id="intref0080">http://www.blender.org</ext-link><year>2018</year></element-citation></ref><ref id="bib29"><element-citation publication-type="journal" id="sref29"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>F.</given-names></name><name><surname>Mu</surname><given-names>L.</given-names></name></person-group><article-title>Assessing the accuracy of applying photogrammetry to take geometric measurements on building products</article-title><source>J.&#x000a0;Const. Eng. Manag.</source><volume>136</volume><year>2010</year><fpage>242</fpage><lpage>250</lpage></element-citation></ref><ref id="bib30"><element-citation publication-type="book" id="sref30"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>P.</given-names></name></person-group><part-title>Museums and the natural environment: the role of natural history museums</part-title><source>Biological Conservation</source><year>1996</year><publisher-name>Leicester University Press</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="book" id="sref31"><person-group person-group-type="author"><name><surname>Donalek</surname><given-names>C.</given-names></name><name><surname>Djorgovski</surname><given-names>S.G.</given-names></name><name><surname>Cioc</surname><given-names>A.</given-names></name><name><surname>Wang</surname><given-names>A.</given-names></name><name><surname>Zhang</surname><given-names>J.</given-names></name><name><surname>Lawler</surname><given-names>E.</given-names></name><name><surname>Yeh</surname><given-names>S.</given-names></name><name><surname>Mahabal</surname><given-names>A.</given-names></name><name><surname>Graham</surname></name><name><surname>Drake</surname><given-names>M.A.</given-names></name></person-group><part-title>Immersive and collaborative data visualization using virtual reality platforms</part-title><source>2014 IEEE International Conference on Big Data</source><year>2014</year><fpage>609</fpage><lpage>614</lpage></element-citation></ref><ref id="bib32"><element-citation publication-type="journal" id="sref32"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>H.</given-names></name><name><surname>Bozkurttas</surname><given-names>M.</given-names></name><name><surname>Mittal</surname><given-names>R.</given-names></name><name><surname>Madden</surname><given-names>P.</given-names></name><name><surname>Lauder</surname><given-names>G.V.</given-names></name></person-group><article-title>Computational modelling and analysis of the hydrodynamics of A highly deformable fish pectoral fin</article-title><source>J.&#x000a0;Fluid Mech.</source><volume>645</volume><year>2010</year><fpage>345</fpage><lpage>373</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal" id="sref33"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>H.</given-names></name><name><surname>Wu</surname><given-names>Z.</given-names></name><name><surname>Tan</surname><given-names>M.</given-names></name><name><surname>Junzhi</surname><given-names>Y.</given-names></name></person-group><article-title>Hydrodynamic analysis and verification of an innovative whale shark-like underwater glider</article-title><source>J.&#x000a0;Bionic Eng.</source><volume>17</volume><year>2020</year><fpage>123</fpage><lpage>133</lpage></element-citation></ref><ref id="bib34"><element-citation publication-type="journal" id="sref34"><person-group person-group-type="author"><name><surname>Dudley</surname><given-names>P.</given-names></name><name><surname>Bonazza</surname><given-names>R.</given-names></name><name><surname>Porter</surname><given-names>W.P.</given-names></name></person-group><article-title>Climate change impacts on nesting and interesting leatherback sea turtles using 3d animated computational fluid dynamics and finite heat transfer</article-title><source>Ecol. Model.</source><volume>320</volume><year>2016</year><fpage>231</fpage><lpage>240</lpage></element-citation></ref><ref id="bib35"><element-citation publication-type="book" id="sref35"><person-group person-group-type="author"><name><surname>Egels</surname><given-names>Y.</given-names></name><name><surname>Kasser</surname><given-names>M.</given-names></name></person-group><part-title>Digital photogrammetry</part-title><year>2003</year></element-citation></ref><ref id="bib36"><element-citation publication-type="journal" id="sref36"><person-group person-group-type="author"><name><surname>Enquist</surname><given-names>B.</given-names></name><name><surname>West</surname><given-names>G.</given-names></name><name><surname>Charnov</surname><given-names>E.</given-names></name><name><surname>Brown</surname><given-names>J.</given-names></name></person-group><article-title>Allometric scaling of production and life-history variation in vascular plants</article-title><source>Nature</source><volume>401</volume><year>1999</year><fpage>907</fpage><lpage>911</lpage></element-citation></ref><ref id="bib37"><element-citation publication-type="journal" id="sref37"><person-group person-group-type="author"><name><surname>Evin</surname><given-names>A.</given-names></name><name><surname>Souter</surname><given-names>T.</given-names></name><name><surname>Hulme-Beaman</surname><given-names>A.</given-names></name><name><surname>Ameen</surname><given-names>C.</given-names></name><name><surname>Allen</surname><given-names>R.</given-names></name><name><surname>Viacava</surname><given-names>P.</given-names></name><name><surname>Larson</surname><given-names>G.</given-names></name><name><surname>Cucchi</surname><given-names>T.</given-names></name><name><surname>Dobney</surname><given-names>K.</given-names></name></person-group><article-title>The use of close-range photogrammetry in Zooarchaeology: creating accurate 3d models of wolf crania to study dog domestication</article-title><source>J.&#x000a0;Arch. Sci. Rep.</source><volume>9</volume><year>2016</year><fpage>87</fpage><lpage>93</lpage></element-citation></ref><ref id="bib38"><element-citation publication-type="journal" id="sref38"><person-group person-group-type="author"><name><surname>Falkingham</surname><given-names>P.</given-names></name></person-group><article-title>Acquisition of high-resolution three-dimensional models using free open-source, photogrammetric software</article-title><source>Palaeontol. Electron.</source><volume>15</volume><year>2012</year><fpage>1</fpage><lpage>15</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal" id="sref39"><person-group person-group-type="author"><name><surname>Fauci</surname><given-names>L.J.</given-names></name><name><surname>Peskin</surname><given-names>C.S.</given-names></name></person-group><article-title>A computational model of aquatic animal locomotion</article-title><source>J.&#x000a0;Comp. Phys.</source><volume>77</volume><year>1988</year><fpage>85</fpage><lpage>108</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal" id="sref40"><person-group person-group-type="author"><name><surname>Fish</surname><given-names>F.</given-names></name><name><surname>Lauder</surname><given-names>G.V.</given-names></name></person-group><article-title>Control surfaces of aquatic vertebrates: active and passive design and function</article-title><source>J.&#x000a0;Exp. Biol.</source><volume>220</volume><year>2017</year><fpage>4351</fpage><lpage>4363</lpage><pub-id pub-id-type="pmid">29187618</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal" id="sref41"><person-group person-group-type="author"><name><surname>Fleishman</surname><given-names>L.J.</given-names></name><name><surname>Endler</surname><given-names>J.A.</given-names></name></person-group><article-title>Some comments on visual perception and the Use of video playback in animal behavior studies</article-title><source>Acta Ethol.</source><volume>3.1</volume><year>2000</year><fpage>15</fpage><lpage>27</lpage></element-citation></ref><ref id="bib42"><element-citation publication-type="journal" id="sref42"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>A.</given-names></name><name><surname>Lauder</surname><given-names>G.</given-names></name><name><surname>Wilga</surname><given-names>C.</given-names></name><name><surname>Hammerschlag</surname><given-names>N.</given-names></name><name><surname>Irschick</surname><given-names>D.J.</given-names></name></person-group><article-title>Ontogeny of head and caudal fin shape of an apex marine predator: the tiger shark (Galeocerdo cuvier)</article-title><source>J.&#x000a0;Morph</source><volume>277</volume><year>2016</year><fpage>556</fpage><lpage>564</lpage><pub-id pub-id-type="pmid">26869274</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal" id="sref43"><person-group person-group-type="author"><name><surname>Gaston</surname><given-names>K.</given-names></name></person-group><article-title>The how and why of biodiversity</article-title><source>Nature</source><volume>421</volume><year>2003</year><fpage>900</fpage><lpage>901</lpage><pub-id pub-id-type="pmid">12606980</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal" id="sref44"><person-group person-group-type="author"><name><surname>Gignac</surname><given-names>P.M.</given-names></name><name><surname>Kley</surname><given-names>N.J.</given-names></name></person-group><article-title>Iodine-enhanced micro-ct imaging: methodological refinements for the study of the soft-tissue anatomy of post-embryonic vertebrates</article-title><source>J.&#x000a0;Exp. Zool. B.</source><volume>322</volume><year>2014</year><fpage>166</fpage><lpage>176</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="journal" id="sref45"><person-group person-group-type="author"><name><surname>Gignac</surname><given-names>P.M.</given-names></name><name><surname>Kley</surname><given-names>N.J.</given-names></name><name><surname>Clarke</surname><given-names>J.A.</given-names></name><name><surname>Colbert</surname><given-names>M.W.</given-names></name><name><surname>Morhardt</surname><given-names>A.C.</given-names></name><name><surname>Cerio</surname><given-names>D.</given-names></name><name><surname>Cost</surname><given-names>I.</given-names></name><name><surname>Cox</surname><given-names>P.G.</given-names></name><name><surname>Daza</surname><given-names>J.D.</given-names></name><name><surname>Early</surname><given-names>C.M.</given-names></name><etal/></person-group><article-title>Diffusible iodine-based contrast-enhanced computed tomography (dicect): an emerging tool for rapid, high-resolution, 3-D imaging of metazoan soft tissues</article-title><source>J.&#x000a0;Anat.</source><volume>228</volume><year>2016</year><fpage>889</fpage><lpage>909</lpage><pub-id pub-id-type="pmid">26970556</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal" id="sref46"><person-group person-group-type="author"><name><surname>Grayburn</surname><given-names>J.</given-names></name><name><surname>Lisher-Katz</surname><given-names>Z.</given-names></name><name><surname>Golubiewski-Davis</surname><given-names>K.</given-names></name><name><surname>Ikeshoji-Orlati</surname><given-names>V.</given-names></name></person-group><article-title>3d/Vr in the academic library: emerging practices and trends</article-title><source>Clir Rep.</source><year>2019</year><comment>clir publication No. 176</comment></element-citation></ref><ref id="bib47"><element-citation publication-type="journal" id="sref47"><person-group person-group-type="author"><name><surname>Hebert</surname><given-names>P.D.N.</given-names></name><name><surname>Waard</surname><given-names>D.</given-names></name><name><surname>Zakharov</surname><given-names>E.V.</given-names></name><name><surname>Prosser</surname><given-names>S.W.J.</given-names></name><name><surname>Sones</surname><given-names>J.E.</given-names></name><name><surname>Mckeown</surname><given-names>J.T.A.</given-names></name><name><surname>Mantle</surname><given-names>B.</given-names></name><name><surname>Lasalle</surname><given-names>J.</given-names></name></person-group><article-title>A dna &#x02018;Barcode Blitz&#x02019;: rapid digitization and sequencing of a natural history collection</article-title><source>PLoS One</source><volume>8</volume><year>2013</year><fpage>E68535</fpage><pub-id pub-id-type="pmid">23874660</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="book" id="sref48"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><part-title>Deep residual learning for image recognition</part-title><source>Proc. IEEE Conf. Comp. Vision and Pattern Recognition</source><year>2016</year><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal" id="sref49"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>H.</given-names></name><name><surname>Wu</surname><given-names>S.</given-names></name><name><surname>Cohen-Or</surname><given-names>D.</given-names></name><name><surname>Gong</surname><given-names>M.</given-names></name><name><surname>Zhang</surname><given-names>H.</given-names></name><name><surname>Li</surname><given-names>G.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name></person-group><article-title>L1-Medial skeleton of point cloud</article-title><source>ACM Trans. Graphics</source><volume>32</volume><year>2013</year><fpage>65</fpage></element-citation></ref><ref id="bib50"><element-citation publication-type="journal" id="sref50"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>J.T.</given-names></name></person-group><article-title>The importance of voucher specimens, with practical guidelines for preserving specimens of the major invertebrate phyla for identification</article-title><source>J.&#x000a0;Nat. Hist.</source><volume>32</volume><year>2007</year><fpage>367</fpage><lpage>385</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="journal" id="sref51"><person-group person-group-type="author"><name><surname>Huising</surname><given-names>E.J.</given-names></name><name><surname>Pereira</surname><given-names>L.M.G.</given-names></name></person-group><article-title>Errors and accuracy estimates of laser data acquired by various laser scanning systems for topographic applications</article-title><source>ISPRS J. Photogramm. Remote Sens.</source><volume>53</volume><year>1998</year><fpage>245</fpage><lpage>261</lpage></element-citation></ref><ref id="bib52"><element-citation publication-type="journal" id="sref52"><person-group person-group-type="author"><name><surname>Hutchinson</surname><given-names>J.R.</given-names></name><name><surname>Ng-Throw-Hing</surname></name><name><surname>Anderson</surname><given-names>F.C.</given-names></name></person-group><article-title>A 3d interactive method for estimating body segmental parameters in animals: application to the turning and running performance of tyrannosaurus rex</article-title><source>J.&#x000a0;Theo. Biol.</source><volume>246</volume><year>2007</year><fpage>660</fpage><lpage>680</lpage></element-citation></ref><ref id="bib53"><element-citation publication-type="journal" id="sref53"><person-group person-group-type="author"><name><surname>Irschick</surname><given-names>D.J.</given-names></name><name><surname>Vitt</surname><given-names>L.J.</given-names></name><name><surname>Zani</surname><given-names>P.</given-names></name><name><surname>Losos</surname><given-names>J.B.</given-names></name></person-group><article-title>A comparison of evolutionary radiations in mainland and west Indian Anolis lizards</article-title><source>Ecology</source><volume>78</volume><year>1997</year><fpage>2191</fpage><lpage>2203</lpage></element-citation></ref><ref id="bib54"><element-citation publication-type="journal" id="sref54"><person-group person-group-type="author"><name><surname>Irschick</surname><given-names>D.J.</given-names></name><name><surname>Podos</surname><given-names>J.</given-names></name><name><surname>Albertson</surname><given-names>C.</given-names></name><name><surname>Brennan</surname><given-names>P.</given-names></name><name><surname>Johnson</surname><given-names>N.</given-names></name><name><surname>Patek</surname><given-names>S.P.</given-names></name><name><surname>Dumont</surname><given-names>B.</given-names></name></person-group><article-title>Evo-devo beyond morphology: from genes to resource use</article-title><source>Trends Ecol. Evol.</source><volume>28</volume><year>2013</year><fpage>267</fpage><lpage>273</lpage><pub-id pub-id-type="pmid">23337185</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="book" id="sref55"><person-group person-group-type="author"><name><surname>Irschick</surname><given-names>D.J.</given-names></name><name><surname>Higham</surname><given-names>T.</given-names></name></person-group><part-title>Animal athletes: an ecological and evolutionary approach</part-title><year>2016</year><publisher-name>Oxford University Press</publisher-name><fpage>272</fpage></element-citation></ref><ref id="bib56"><element-citation publication-type="journal" id="sref56"><person-group person-group-type="author"><name><surname>Irschick</surname><given-names>D.J.</given-names></name><name><surname>Martin</surname><given-names>J.</given-names></name><name><surname>Siebert</surname><given-names>U.</given-names></name><name><surname>Kristensen</surname><given-names>J.</given-names></name><name><surname>Madsen</surname><given-names>P.T.</given-names></name><name><surname>Christiansen</surname><given-names>F.</given-names></name></person-group><article-title>Creation of accurate 3d models of harbor porpoises (phocoena phocoena) using 3d photogrammetry</article-title><source>Mar. Mamm. Sci.</source><volume>37</volume><year>2020</year><fpage>482</fpage><lpage>491</lpage></element-citation></ref><ref id="bib57"><element-citation publication-type="journal" id="sref57"><person-group person-group-type="author"><name><surname>Irschick</surname><given-names>D.J.</given-names></name><name><surname>Bot</surname><given-names>J.</given-names></name><name><surname>Brooks</surname><given-names>A.</given-names></name><name><surname>Bresette</surname><given-names>M.</given-names></name><name><surname>Fossette</surname><given-names>S.</given-names></name><name><surname>Gleiss</surname><given-names>A.</given-names></name><name><surname>Gutierrez</surname><given-names>R.</given-names></name><name><surname>Manire</surname><given-names>C.</given-names></name><name><surname>Merigo</surname><given-names>C.</given-names></name><name><surname>Martin</surname><given-names>J.</given-names></name><etal/></person-group><article-title>Using 3d photogrammetry to create accurate 3d models of sea turtle species as digital voucher specimens</article-title><source>Herp. Rev.</source><volume>51</volume><year>2020</year><fpage>709</fpage><lpage>715</lpage></element-citation></ref><ref id="bib58"><element-citation publication-type="journal" id="sref58"><person-group person-group-type="author"><name><surname>Irschick</surname><given-names>D.J.</given-names></name><name><surname>Corriveau</surname><given-names>Z.</given-names></name><name><surname>Mayhan</surname><given-names>T.</given-names></name><name><surname>Siler</surname><given-names>C.</given-names></name><name><surname>Mandica</surname><given-names>M.</given-names></name><name><surname>Gamble</surname><given-names>T.</given-names></name><name><surname>Martin</surname><given-names>J.</given-names></name><name><surname>Bot</surname><given-names>J.</given-names></name><name><surname>Zotos</surname><given-names>S.</given-names></name></person-group><article-title>Devices and methods for rapid 3d photo-capture and photogrammetry of small reptiles and Amphibians in the laboratory and the field</article-title><source>Herp. Rev.</source><volume>51</volume><year>2020</year><fpage>716</fpage><lpage>725</lpage></element-citation></ref><ref id="bib59"><element-citation publication-type="journal" id="sref59"><person-group person-group-type="author"><name><surname>Igarashi</surname><given-names>T.</given-names></name><name><surname>Moscovich</surname><given-names>T.</given-names></name><name><surname>Hughes</surname><given-names>J.F.</given-names></name></person-group><article-title>As-rigid-as-possible shape manipulation</article-title><source>ACM Trans. Graphics</source><volume>24</volume><year>2005</year><fpage>134</fpage><lpage>1141</lpage></element-citation></ref><ref id="bib60"><element-citation publication-type="journal" id="sref60"><person-group person-group-type="author"><name><surname>Jakob</surname><given-names>E.M.</given-names></name><name><surname>Marshall</surname><given-names>S.D.</given-names></name><name><surname>Uetz</surname><given-names>G.W.</given-names></name></person-group><article-title>Estimating fitness: a comparison of body condition indices</article-title><source>Oikos</source><volume>77</volume><year>1996</year><fpage>61</fpage><lpage>67</lpage></element-citation></ref><ref id="bib61"><element-citation publication-type="journal" id="sref61"><person-group person-group-type="author"><name><surname>Jang</surname><given-names>S.</given-names></name><name><surname>Vitale</surname><given-names>J.M.</given-names></name><name><surname>Jyung</surname><given-names>R.W.</given-names></name><name><surname>Black</surname><given-names>J.B.</given-names></name></person-group><article-title>Direct manipulation is better than passive viewing for learning anatomy in a three-dimensional virtual reality environment</article-title><source>Comp. Educ. Next</source><volume>106</volume><year>2017</year><fpage>150</fpage><lpage>165</lpage></element-citation></ref><ref id="bib62"><element-citation publication-type="book" id="sref62"><person-group person-group-type="author"><name><surname>Kalogerakis</surname><given-names>E.</given-names></name><name><surname>Averkiou</surname><given-names>M.</given-names></name><name><surname>Maji</surname><given-names>S.</given-names></name><name><surname>Chaudhuri</surname><given-names>S.</given-names></name></person-group><part-title>3d shape segmentation with projective convolutional networks</part-title><source>Proc. IEEE Conf. Comp. Vision And Pattern Recognition</source><year>2017</year><fpage>3779</fpage><lpage>3788</lpage></element-citation></ref><ref id="bib63"><element-citation publication-type="journal" id="sref63"><person-group person-group-type="author"><name><surname>Ke</surname><given-names>F.</given-names></name><name><surname>Im</surname><given-names>T.</given-names></name></person-group><article-title>Virtual-Reality-Based Social Interaction Training For Children With High-Functioning Autism</article-title><source>J.&#x000a0;Edn. Res.</source><volume>106</volume><year>2013</year><fpage>441</fpage><lpage>461</lpage></element-citation></ref><ref id="bib64"><element-citation publication-type="book" id="sref64"><person-group person-group-type="author"><name><surname>Kleiber</surname><given-names>M.</given-names></name></person-group><part-title>The Fire of Life: An introduction to animal energetics</part-title><year>1961</year><comment>Krieger pub Co. 478 P</comment></element-citation></ref><ref id="bib65"><element-citation publication-type="journal" id="sref65"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S.W.</given-names></name><name><surname>Kim</surname><given-names>G.H.</given-names></name></person-group><article-title>Thickness-profile measurement of transparent thin-film layers by white-light scanning interferometry</article-title><source>Appl. Optics</source><volume>38</volume><year>1999</year><fpage>5968</fpage><lpage>5973</lpage></element-citation></ref><ref id="bib66"><element-citation publication-type="journal" id="sref66"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S.</given-names></name><name><surname>Laschi</surname><given-names>C.</given-names></name><name><surname>Trimmer</surname><given-names>B.</given-names></name></person-group><article-title>Soft robotics: a bioinspired evolution in robotics</article-title><source>Trends Biotech.</source><volume>31</volume><year>2013</year><fpage>287</fpage><lpage>294</lpage></element-citation></ref><ref id="bib67"><element-citation publication-type="journal" id="sref67"><person-group person-group-type="author"><name><surname>Krishtalka</surname><given-names>L.</given-names></name><name><surname>Humphrey</surname><given-names>P.S.</given-names></name></person-group><article-title>Can natural history museums capture the future?</article-title><source>Bioscience</source><volume>50</volume><year>2000</year><fpage>611</fpage><lpage>617</lpage></element-citation></ref><ref id="bib68"><element-citation publication-type="journal" id="sref68"><person-group person-group-type="author"><name><surname>Labocha</surname><given-names>M.K.</given-names></name><name><surname>Hayes</surname><given-names>J.P.</given-names></name></person-group><article-title>Morphometric indices of body condition in birds: a review</article-title><source>J.&#x000a0;Ornithol.</source><volume>153</volume><year>2012</year><fpage>1</fpage><lpage>22</lpage></element-citation></ref><ref id="bib69"><element-citation publication-type="book" id="sref69"><person-group person-group-type="author"><name><surname>Laforsch</surname><given-names>C.H.</given-names></name><name><surname>Imhof</surname><given-names>S.</given-names></name><name><surname>Settles</surname><given-names>R.M.</given-names></name><name><surname>Heb</surname><given-names>M.W.A.</given-names></name></person-group><part-title>Applications of computational 3d&#x02013;modeling in organismal biology</part-title><person-group person-group-type="editor"><name><surname>Murray-Smith</surname><given-names>D.J.</given-names></name></person-group><source>Modeling And Simulation In Engineering Sciences</source><year>2012</year><publisher-name>Woodhead Publishing</publisher-name><publisher-loc>Uk</publisher-loc><fpage>117</fpage><lpage>142</lpage></element-citation></ref><ref id="bib70"><element-citation publication-type="journal" id="sref70"><person-group person-group-type="author"><name><surname>Lauder</surname><given-names>G.</given-names></name></person-group><article-title>Functional morphology and systematics: studying functional patterns in a historical context</article-title><source>Ann. Rev. Ecol.. Syst.</source><volume>2</volume><year>1990</year><fpage>317</fpage><lpage>340</lpage></element-citation></ref><ref id="bib71"><element-citation publication-type="journal" id="sref71"><person-group person-group-type="author"><name><surname>Lauder</surname><given-names>G.V.</given-names></name></person-group><article-title>Fish locomotion: recent advances and new directions</article-title><source>Ann. Rev. Mar. Sci.</source><volume>7</volume><year>2015</year><fpage>521</fpage><lpage>545</lpage></element-citation></ref><ref id="bib72"><element-citation publication-type="journal" id="sref72"><person-group person-group-type="author"><name><surname>Lauder</surname><given-names>G.V.</given-names></name><name><surname>Madden</surname><given-names>P.A.G.</given-names></name><name><surname>Tangorra</surname><given-names>J.L.</given-names></name><name><surname>Anderson</surname><given-names>E.</given-names></name><name><surname>Baker</surname><given-names>T.V.</given-names></name></person-group><article-title>Bioinspiration from fish for smart material design and function</article-title><source>Smart Mat. Struc.</source><volume>20</volume><year>2011</year><fpage>094014</fpage></element-citation></ref><ref id="bib73"><element-citation publication-type="book" id="sref73"><person-group person-group-type="author"><name><surname>Lawrance</surname><given-names>J.</given-names></name></person-group><part-title>Introduction to neural networks. Design theory and applications</part-title><year>1994</year><publisher-name>California Scientific Software</publisher-name></element-citation></ref><ref id="bib74"><element-citation publication-type="journal" id="sref74"><person-group person-group-type="author"><name><surname>Laha</surname><given-names>B.</given-names></name><name><surname>Da</surname><given-names>B.</given-names></name><name><surname>Jj</surname><given-names>S.</given-names></name></person-group><article-title>Effects of Vr system fidelity on analyzing isosurface visualization of volume datasets</article-title><source>IEEE Trans. Vis. Comp. Graphics</source><volume>20</volume><year>2014</year><fpage>513</fpage><lpage>522</lpage></element-citation></ref><ref id="bib75"><element-citation publication-type="journal" id="sref75"><person-group person-group-type="author"><name><surname>Lecun</surname><given-names>Y.</given-names></name><name><surname>Bottou</surname><given-names>L.</given-names></name><name><surname>Bengio</surname><given-names>Y.</given-names></name><name><surname>Haffner</surname><given-names>P.</given-names></name></person-group><article-title>Gradient-based learning applied to document recognition</article-title><source>Proc. IEEE</source><volume>11</volume><year>1998</year><fpage>2278</fpage><lpage>2324</lpage></element-citation></ref><ref id="bib76"><element-citation publication-type="journal" id="sref76"><person-group person-group-type="author"><name><surname>Li</surname><given-names>P.</given-names></name><name><surname>Aberman</surname><given-names>K.</given-names></name><name><surname>Hanocka</surname><given-names>R.</given-names></name><name><surname>Liu</surname><given-names>L.</given-names></name><name><surname>Sorkine-Hornung</surname><given-names>O.</given-names></name><name><surname>Chen</surname><given-names>B.</given-names></name></person-group><article-title>Learning skeletal articulations with neural blend shapes</article-title><source>ACM Trans. Graphics</source><volume>40</volume><year>2021</year><fpage>4</fpage></element-citation></ref><ref id="bib77"><element-citation publication-type="book" id="sref77"><person-group person-group-type="author"><name><surname>Linder</surname><given-names>W.</given-names></name></person-group><part-title>Digital photogrammetry: a practical course</part-title><year>2009</year><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib78"><element-citation publication-type="book" id="sref78"><person-group person-group-type="author"><name><surname>Lischer-Katz</surname><given-names>Z.</given-names></name><name><surname>Cook</surname><given-names>M.</given-names></name></person-group><part-title>Virtual reality</part-title><source>T.&#x000a0;Trenches: Addressing The Preservation Challenges <bold>o</bold>f Virtual Reality For Scholarship. Presentation At Coalition For Networked Information Conference, Albuquerque, New Mexico</source><year>2017</year></element-citation></ref><ref id="bib79"><element-citation publication-type="journal" id="sref79"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H.</given-names></name></person-group><article-title>Computational biological fluid dynamics: digitizing and visualizing animal swimming and flying</article-title><source>Int. Comp. Biol.</source><volume>42</volume><year>2002</year><fpage>1050</fpage><lpage>1059</lpage></element-citation></ref><ref id="bib80"><element-citation publication-type="journal" id="sref80"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>G.</given-names></name><name><surname>Ren</surname><given-names>Y.</given-names></name><name><surname>Dong</surname><given-names>H.</given-names></name><name><surname>Akanyeti</surname><given-names>O.</given-names></name><name><surname>Liao</surname><given-names>J.</given-names></name><name><surname>Lauder</surname><given-names>G.V.</given-names></name></person-group><article-title>Computational analysis of vortex dynamics and performance enhancement due to body-fin and fin-fin interactions in shark-like locomotion</article-title><source>J.&#x000a0;Fluid Mech</source><volume>829</volume><year>2017</year><fpage>65</fpage><lpage>88</lpage></element-citation></ref><ref id="bib81"><element-citation publication-type="journal" id="sref81"><person-group person-group-type="author"><name><surname>Magnenat-Thalmann</surname><given-names>N.</given-names></name><name><surname>Laperrire</surname><given-names>R.</given-names></name><name><surname>Thalmann</surname><given-names>D.</given-names></name></person-group><article-title>Joint-dependent local deformations for hand animation and object grasping</article-title><source>Proc. Graphics Int.</source><volume>88</volume><year>1988</year><fpage>26</fpage><lpage>33</lpage></element-citation></ref><ref id="bib82"><element-citation publication-type="journal" id="sref82"><person-group person-group-type="author"><name><surname>Medina</surname><given-names>J.J.</given-names></name><name><surname>Maley</surname><given-names>J.M.</given-names></name><name><surname>Sannapareddy</surname><given-names>S.</given-names></name><name><surname>Medina</surname><given-names>N.N.</given-names></name><name><surname>Gilman</surname><given-names>C.M.</given-names></name><name><surname>Mccormick</surname><given-names>J.</given-names></name></person-group><article-title>A rapid and cost-effective pipeline for digitization of museum specimens with 3d photogrammetry</article-title><source>PLoS One</source><volume>15</volume><year>2020</year><fpage>e0236417</fpage><pub-id pub-id-type="pmid">32790700</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal" id="sref83"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>L.A.</given-names></name><name><surname>Goldman</surname><given-names>D.I.</given-names></name><name><surname>Hedrick</surname><given-names>T.L.</given-names></name><name><surname>Tytell</surname><given-names>E.D.</given-names></name><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Yen</surname><given-names>J.</given-names></name><name><surname>Alben</surname><given-names>S.</given-names></name></person-group><article-title>Using computational models to study animal locomotion</article-title><source>Int. Comp. Biol.</source><volume>52</volume><year>2012</year><fpage>553</fpage><lpage>575</lpage></element-citation></ref><ref id="bib84"><element-citation publication-type="journal" id="sref84"><person-group person-group-type="author"><name><surname>M&#x000fc;ller</surname><given-names>G.</given-names></name></person-group><article-title>Evo&#x02013;devo: extending the evolutionary synthesis</article-title><source>Nat. Rev. Genet.</source><volume>8</volume><year>2007</year><fpage>943</fpage><lpage>949</lpage><pub-id pub-id-type="pmid">17984972</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal" id="sref85"><person-group person-group-type="author"><name><surname>Nakata</surname><given-names>T.</given-names></name><name><surname>Liu</surname><given-names>H.</given-names></name><name><surname>Tanaka</surname><given-names>Y.</given-names></name><name><surname>Nishihashi</surname><given-names>N.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Sato</surname><given-names>A.</given-names></name></person-group><article-title>Aerodynamics of a bio-inspired flexible flapping-wing micro air vehicle</article-title><source>Bioinspr. Biomim</source><year>2011</year><fpage>6045002</fpage></element-citation></ref><ref id="bib86"><element-citation publication-type="book" id="sref86"><person-group person-group-type="author"><name><surname>Pantelidis</surname><given-names>V.S.</given-names></name></person-group><part-title>Reasons to use virtual reality in education and training courses and a model to determine when to use virtual reality</part-title><source>Themes in science and technology education special issue, Klidarithmos computer Books special issue</source><year>2010</year><fpage>59</fpage><lpage>70</lpage></element-citation></ref><ref id="bib87"><element-citation publication-type="book" id="sref87"><person-group person-group-type="author"><name><surname>Pianka</surname><given-names>E.R.</given-names></name></person-group><part-title>Evolutionary ecology</part-title><year>2001</year><publisher-name>Book News, Inc</publisher-name></element-citation></ref><ref id="bib88"><element-citation publication-type="journal" id="sref88"><person-group person-group-type="author"><name><surname>Pimm</surname><given-names>S.L.</given-names></name><name><surname>Russell</surname><given-names>G.</given-names></name><name><surname>Gittleman</surname><given-names>J.L.</given-names></name><name><surname>Brooks</surname><given-names>T.M.</given-names></name></person-group><article-title>The future of biodiversity</article-title><source>Science</source><volume>269</volume><year>1995</year><fpage>347</fpage><lpage>350</lpage><pub-id pub-id-type="pmid">17841251</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal" id="sref89"><person-group person-group-type="author"><name><surname>Pleijel</surname><given-names>F.</given-names></name><name><surname>Jondelius</surname><given-names>U.</given-names></name><name><surname>Norlinder</surname><given-names>E.</given-names></name><name><surname>Nygren</surname><given-names>A.</given-names></name><name><surname>Oxelman</surname><given-names>B.</given-names></name><name><surname>Schander</surname><given-names>C.</given-names></name><name><surname>Sundberg</surname><given-names>P.</given-names></name><name><surname>Thollesson</surname><given-names>M.</given-names></name></person-group><article-title>Phylogenies without roots? a plea for the use of vouchers in molecular phylogenetic studies</article-title><source>Mol. Phyl. Evol.</source><volume>48</volume><year>2008</year><fpage>369</fpage><lpage>371</lpage></element-citation></ref><ref id="bib90"><element-citation publication-type="journal" id="sref90"><person-group person-group-type="author"><name><surname>Plum</surname><given-names>F.</given-names></name><name><surname>Labonte</surname><given-names>D.</given-names></name></person-group><article-title>Scant &#x02013; an open-source platform for the creation of 3d models of arthropods (and other small objects)</article-title><source>PeerJ</source><volume>9</volume><year>2021</year><fpage>E11155</fpage><pub-id pub-id-type="pmid">33954036</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="book" id="sref91"><person-group person-group-type="author"><name><surname>Pober</surname><given-names>E.</given-names></name><name><surname>Cook</surname><given-names>M.</given-names></name></person-group><part-title>The design and development of an immersive learning system for spatial analysis and visual cognition</part-title><source>Presentation at Conference of the Design Communication Association, Bozeman, MT</source><year>2016</year></element-citation></ref><ref id="bib92"><element-citation publication-type="journal" id="sref92"><person-group person-group-type="author"><name><surname>Postma</surname><given-names>M.</given-names></name><name><surname>Tordiffe</surname><given-names>A.S.W.</given-names></name><name><surname>Hofmeyr</surname><given-names>M.S.</given-names></name><name><surname>Reisinger</surname><given-names>R.R.</given-names></name><name><surname>Bester</surname><given-names>L.C.</given-names></name><name><surname>Buss</surname><given-names>P.E.</given-names></name><name><surname>De Bruyn</surname><given-names>P.J.N.</given-names></name></person-group><article-title>Terrestrial mammal three-dimensional photogrammetry: multispecies mass estimation</article-title><source>Ecosphere</source><volume>6</volume><year>2015</year><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="bib93"><element-citation publication-type="journal" id="sref93"><person-group person-group-type="author"><name><surname>Reiss</surname><given-names>G.</given-names></name><name><surname>Eulitz</surname><given-names>M.</given-names></name></person-group><article-title>3d reconstruction of sem images by use of optical photogrammetry software</article-title><source>J.&#x000a0;Struct. Bio.</source><volume>191</volume><year>2015</year><fpage>190</fpage><lpage>196</lpage><pub-id pub-id-type="pmid">26073969</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal" id="sref94"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>C.R.</given-names></name><name><surname>Yi</surname><given-names>L.</given-names></name><name><surname>Su</surname><given-names>H.</given-names></name><name><surname>Guibas</surname><given-names>L.J.</given-names></name></person-group><article-title>Pointnet++: deep hierarchical feature learning on point sets in a metric space</article-title><source>Adv. Neural Info. Proc. Syst</source><year>2017</year><fpage>5099</fpage><lpage>5108</lpage></element-citation></ref><ref id="bib95"><element-citation publication-type="journal" id="sref95"><person-group person-group-type="author"><name><surname>Raff</surname><given-names>R.</given-names></name></person-group><article-title>Evo-devo: the evolution of a new discipline</article-title><source>Nat. Rev. Genet.</source><volume>1</volume><year>2000</year><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="pmid">11262880</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal" id="sref96"><person-group person-group-type="author"><name><surname>Ravi</surname><given-names>S.</given-names></name><name><surname>Ryusuke</surname><given-names>N.</given-names></name><name><surname>Gagliardi</surname><given-names>S.</given-names></name><name><surname>Kolomenskiy</surname><given-names>D.</given-names></name><name><surname>Combes</surname><given-names>S.</given-names></name><name><surname>Hao</surname><given-names>L.</given-names></name><name><surname>Biewener</surname><given-names>A.A.</given-names></name><name><surname>Konow</surname><given-names>N.</given-names></name></person-group><article-title>Modulation of flight muscle recruitment and wing rotation enables hummingbirds to mitigate aerial roll perturbations</article-title><source>Curr. Biol.</source><volume>30</volume><year>2020</year><fpage>187</fpage><lpage>195</lpage><pub-id pub-id-type="pmid">31902723</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="book" id="sref97"><person-group person-group-type="author"><name><surname>Riegler</surname><given-names>G.</given-names></name><name><surname>Osman</surname><given-names>U.</given-names></name><name><surname>Geiger</surname><given-names>A.</given-names></name></person-group><part-title>Octnet: Learning deep 3d representations at high resolutions</part-title><source>Proc IEEE Conf. Computer Vision Pattern Recognition</source><year>2017</year><fpage>3577</fpage><lpage>3586</lpage></element-citation></ref><ref id="bib98"><element-citation publication-type="journal" id="sref98"><person-group person-group-type="author"><name><surname>Schindel</surname><given-names>D.E.</given-names></name><name><surname>Cook</surname><given-names>J.A.</given-names></name></person-group><article-title>The next generation of natural history collections</article-title><source>PLoS Biol.</source><volume>16</volume><year>2018</year><fpage>E2006125</fpage><pub-id pub-id-type="pmid">30011273</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal" id="sref99"><person-group person-group-type="author"><name><surname>Seth</surname><given-names>A.</given-names></name><name><surname>Vance</surname><given-names>J.M.</given-names></name><name><surname>Oliver</surname><given-names>J.H.</given-names></name></person-group><article-title>Virtual reality for assembly methods prototyping: a review</article-title><source>Virtual Real.</source><volume>15</volume><year>2011</year><fpage>5</fpage><lpage>20</lpage></element-citation></ref><ref id="bib100"><element-citation publication-type="journal" id="sref100"><person-group person-group-type="author"><name><surname>Shaffer</surname><given-names>H.B.</given-names></name><name><surname>Fisher</surname><given-names>R.</given-names></name><name><surname>Davidson</surname><given-names>C.</given-names></name></person-group><article-title>The role of natural history collections in documenting species declines</article-title><source>Trends Ecol. Evol.</source><volume>13</volume><year>1998</year><fpage>27</fpage><lpage>30</lpage><pub-id pub-id-type="pmid">21238186</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal" id="sref101"><person-group person-group-type="author"><name><surname>Shapiro</surname><given-names>K.</given-names></name></person-group><article-title>Whither Zoos: an inescapable question</article-title><source>J.&#x000a0;Animal Welfare Sci.</source><volume>21</volume><year>2018</year><fpage>1</fpage><lpage>3</lpage></element-citation></ref><ref id="bib102"><element-citation publication-type="journal" id="sref102"><person-group person-group-type="author"><name><surname>Shorter</surname><given-names>A.K.</given-names></name><name><surname>Murray</surname><given-names>M.</given-names></name><name><surname>Johnson</surname><given-names>M.</given-names></name><name><surname>Moore</surname><given-names>M.</given-names></name><name><surname>Howle</surname><given-names>L.E.</given-names></name></person-group><article-title>Drag of suction cup tags on swimming animals: modeling and measurement</article-title><source>Mar. Mammal Sci.</source><volume>30</volume><year>2014</year><fpage>726</fpage><lpage>746</lpage></element-citation></ref><ref id="bib103"><element-citation publication-type="journal" id="sref103"><person-group person-group-type="author"><name><surname>Silverstein</surname><given-names>J.C.</given-names></name><name><surname>Ehrenfeld</surname><given-names>J.M.</given-names></name><name><surname>Croft</surname><given-names>D.A.</given-names></name><name><surname>Dech</surname><given-names>F.W.</given-names></name><name><surname>Small</surname><given-names>S.</given-names></name><name><surname>Cook</surname><given-names>S.</given-names></name></person-group><article-title>Tele-immersion: preferred infrastructure for anatomy instruction</article-title><source>J.&#x000a0;Comp. High. Educ.</source><volume>18</volume><year>2006</year><fpage>80</fpage><lpage>93</lpage></element-citation></ref><ref id="bib104"><element-citation publication-type="journal" id="sref104"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Vedaldi</surname><given-names>A.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Deep inside convolutional networks: visualising image classification models and saliency maps</article-title><comment>Preprint at</comment><source>Arxiv</source><year>2013</year><pub-id pub-id-type="doi">10.48550/arXiv.1312.6034</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="book" id="sref105"><person-group person-group-type="author"><name><surname>Sorkine</surname><given-names>O.</given-names></name><name><surname>Alexa</surname><given-names>M.</given-names></name></person-group><part-title>As-rigid-as-possible surface modeling</part-title><source>Proc. Fifth eurographics symp.. Geometry Processing (Sgp '07)</source><year>2007</year><fpage>109</fpage><lpage>116</lpage></element-citation></ref><ref id="bib106"><element-citation publication-type="book" id="sref106"><person-group person-group-type="author"><name><surname>Su</surname><given-names>H.</given-names></name><name><surname>Kalogerakis</surname><given-names>E.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>Su</surname><given-names>H.</given-names></name><name><surname>Qim</surname><given-names>C.</given-names></name><name><surname>Guibas</surname><given-names>L.J.</given-names></name><name><surname>Bronstein</surname><given-names>M.</given-names></name></person-group><part-title>A tutorial on 3d deep learning. Cvpr 2017 tutorial</part-title><year>2017</year></element-citation></ref><ref id="bib107"><element-citation publication-type="journal" id="sref107"><person-group person-group-type="author"><name><surname>Suarez</surname><given-names>A.V.</given-names></name><name><surname>Tsutsui</surname><given-names>N.D.</given-names></name></person-group><article-title>The value of museum collections for research and society</article-title><source>Bioscience</source><volume>54</volume><year>2004</year><fpage>66</fpage><lpage>74</lpage></element-citation></ref><ref id="bib108"><element-citation publication-type="journal" id="sref108"><person-group person-group-type="author"><name><surname>Szaflik</surname><given-names>J.P.</given-names></name></person-group><article-title>Comparison of in&#x000a0;vivo confocal microscopy of human cornea by white light scanning slit and laser scanning systems</article-title><source>Cornea</source><volume>26</volume><year>2007</year><fpage>438</fpage><lpage>445</lpage><pub-id pub-id-type="pmid">17457193</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal" id="sref109"><person-group person-group-type="author"><name><surname>Tompson</surname><given-names>J.J.</given-names></name><name><surname>Jain</surname><given-names>A.</given-names></name><name><surname>Lecun</surname><given-names>Y.</given-names></name><name><surname>Bregler</surname><given-names>C.</given-names></name></person-group><article-title>Joint training of a convolutional network and A graphical model for human pose estimation</article-title><source>Adv. Neural Inform. Proc. Syst.</source><year>2014</year><fpage>1799</fpage><lpage>1807</lpage></element-citation></ref><ref id="bib110"><element-citation publication-type="book" id="sref110"><person-group person-group-type="author"><name><surname>Tome</surname><given-names>D.</given-names></name><name><surname>Russell</surname><given-names>C.</given-names></name><name><surname>Agapito</surname><given-names>L.</given-names></name></person-group><part-title>Lifting from the deep: convolutional 3d pose estimation from a single image</part-title><source>Proc. IEEE Conf. Comp. Vision Pattern Recognition</source><year>2017</year><fpage>2500</fpage><lpage>2509</lpage></element-citation></ref><ref id="bib111"><element-citation publication-type="book" id="sref111"><person-group person-group-type="author"><name><surname>Toshev</surname><given-names>A.</given-names></name><name><surname>Szegedy</surname><given-names>C.</given-names></name></person-group><part-title>Deeppose: Human pose estimation via deep neural networks</part-title><source>Proc. IEEE Conf. Comp. Vision Pattern Recognition</source><year>2014</year><fpage>653</fpage><lpage>1660</lpage></element-citation></ref><ref id="bib112"><element-citation publication-type="journal" id="sref112"><person-group person-group-type="author"><name><surname>Turvey</surname><given-names>S.T.</given-names></name><name><surname>Crees</surname><given-names>J.J.</given-names></name></person-group><article-title>Extinction in the Anthropocene</article-title><source>Curr. Biol.</source><volume>29</volume><year>2019</year><fpage>R942</fpage><lpage>R995</lpage><pub-id pub-id-type="pmid">31593673</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="book" id="sref113"><person-group person-group-type="author"><name><surname>Van Gerven</surname><given-names>M.</given-names></name><name><surname>Bohte</surname><given-names>S.</given-names></name></person-group><part-title>Artificial neural networks as models of neural information processing</part-title><year>2018</year><publisher-name>Frontiers Media Sa</publisher-name></element-citation></ref><ref id="bib114"><element-citation publication-type="journal" id="sref114"><person-group person-group-type="author"><name><surname>Vandenabeele</surname><given-names>S.P.</given-names></name><name><surname>Grundy</surname><given-names>E.</given-names></name><name><surname>Friswell</surname><given-names>M.I.</given-names></name><name><surname>Grogan</surname><given-names>A.</given-names></name><name><surname>Votier</surname><given-names>S.C.</given-names></name><name><surname>Wilson</surname><given-names>R.P.</given-names></name></person-group><article-title>Excess baggage for birds: inappropriate placement of tags on gannets changes flight patterns</article-title><source>PLoS One</source><year>2014</year><pub-id pub-id-type="doi">10.1371/journal.pone.0092657</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal" id="sref115"><person-group person-group-type="author"><name><surname>Vargas</surname><given-names>A.</given-names></name><name><surname>Mittal</surname><given-names>R.</given-names></name><name><surname>Dong</surname><given-names>H.</given-names></name></person-group><article-title>A computational study of the aerodynamic performance of a dragonfly wing section</article-title><source>Bioinspir. Biomim.</source><volume>3</volume><year>2008</year><fpage>026004</fpage><pub-id pub-id-type="pmid">18503106</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="book" id="sref116"><person-group person-group-type="author"><name><surname>Wainwright</surname><given-names>P.C.</given-names></name><name><surname>Reilly</surname><given-names>S.M.</given-names></name></person-group><part-title>Ecological morphology: integrative organismal biology</part-title><year>1994</year><publisher-name>University of Chicago Press</publisher-name></element-citation></ref><ref id="bib117"><element-citation publication-type="journal" id="sref117"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Wainwright</surname><given-names>D.K.</given-names></name><name><surname>Lindengren</surname><given-names>R.E.</given-names></name><name><surname>Lauder</surname><given-names>G.V.</given-names></name><name><surname>Dong</surname><given-names>H.</given-names></name></person-group><article-title>Tuna locomotion: a computational hydrodynamic analysis of finlet function</article-title><source>J.&#x000a0;R. Soc. Interface</source><volume>17</volume><year>2020</year><fpage>20190590</fpage><pub-id pub-id-type="pmid">32264740</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal" id="sref118"><person-group person-group-type="author"><name><surname>Watters</surname><given-names>J.L.</given-names></name><name><surname>Cumings</surname><given-names>S.T.</given-names></name><name><surname>Flanagan</surname><given-names>R.L.</given-names></name><name><surname>Siler</surname><given-names>C.D.</given-names></name></person-group><article-title>Review of morphometric measurements used in Anuran species descriptions and recommendations for a standardized approach</article-title><source>Zootaxa</source><volume>4072</volume><year>2016</year><fpage>477</fpage><lpage>495</lpage><pub-id pub-id-type="pmid">27395941</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal" id="sref119"><person-group person-group-type="author"><name><surname>Weinberg</surname><given-names>S.M.</given-names></name><name><surname>Scott</surname><given-names>N.M.</given-names></name><name><surname>Neiswanger</surname><given-names>K.</given-names></name><name><surname>Brandon</surname><given-names>C.A.</given-names></name><name><surname>Marazita</surname><given-names>M.L.</given-names></name></person-group><article-title>Digital three-dimensional photogrammetry: evaluation of anthropometric precision and accuracy using a genex 3d camera system</article-title><source>Cleft Palate-Craniofacial J.</source><volume>41</volume><year>2004</year><fpage>507</fpage><lpage>518</lpage></element-citation></ref><ref id="bib120"><element-citation publication-type="book" id="sref120"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z.</given-names></name><name><surname>Song</surname><given-names>S.</given-names></name><name><surname>Khosla</surname><given-names>A.</given-names></name><name><surname>Yu</surname><given-names>F.</given-names></name><name><surname>Zhang</surname><given-names>L.</given-names></name><name><surname>Tang</surname><given-names>X.</given-names></name><name><surname>Xiao</surname><given-names>J.</given-names></name></person-group><part-title>3d Shapenets: a deep representation for volumetric shapes</part-title><source>Proc. IEEE Conf. Comp. Vision Pattern Recognition</source><year>2015</year><fpage>1912</fpage><lpage>1920</lpage></element-citation></ref><ref id="bib121"><element-citation publication-type="journal" id="sref121"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Z.</given-names></name><name><surname>Zhou</surname><given-names>Y.</given-names></name><name><surname>Kalogerakis</surname><given-names>E.</given-names></name><name><surname>Landreth</surname><given-names>C.</given-names></name><name><surname>Singh</surname><given-names>K.</given-names></name></person-group><article-title>Rignet: neural rigging for articulated characters</article-title><source>ACM Trans. Graphics</source><volume>39</volume><year>2020</year><fpage>4</fpage></element-citation></ref><ref id="bib122"><element-citation publication-type="book" id="sref122"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>X.</given-names></name><name><surname>Wu</surname><given-names>Z.</given-names></name><name><surname>Yu</surname><given-names>J.</given-names></name></person-group><part-title>Design and implementation of a robotic shark with a novel embedded vision system</part-title><source>2016 IEEE International Conference on Robotics And Biomimetics</source><year>2016</year><fpage>841</fpage><lpage>846</lpage></element-citation></ref><ref id="bib123"><element-citation publication-type="book" id="sref123"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>M.D.</given-names></name><name><surname>Fergus</surname><given-names>R.</given-names></name></person-group><part-title>Visualizing and understanding convolutional networks</part-title><source>Proc. European Conf. Computer Vision</source><year>2014</year><fpage>818</fpage><lpage>833</lpage></element-citation></ref><ref id="bib124"><element-citation publication-type="book" id="sref124"><person-group person-group-type="author"><name><surname>Zelditch</surname><given-names>M.</given-names></name><name><surname>Swiderski</surname><given-names>D.L.</given-names></name><name><surname>Sheets</surname><given-names>H.H.</given-names></name></person-group><source>Geometric morphometrics for biologists: a primer</source><year>2012</year><publisher-name>Academic Press</publisher-name></element-citation></ref></ref-list><sec id="appsec3"><title>Supporting citations</title><p id="p0145">The following references appear in the Supplemental Information: <xref rid="bib21" ref-type="bibr">Bronstein et&#x000a0;al., 2017</xref>; <xref rid="bib18" ref-type="bibr">Boykov et&#x000a0;al., 1999</xref>; <xref rid="bib48" ref-type="bibr">He et&#x000a0;al., 2016</xref>; <xref rid="bib49" ref-type="bibr">Huang et&#x000a0;al., 2013</xref>; <xref rid="bib59" ref-type="bibr">Igarashi et&#x000a0;al., 2005</xref>; <xref rid="bib62" ref-type="bibr">Kalogerakis et&#x000a0;al., 2017</xref>; <xref rid="bib75" ref-type="bibr">Lecun et&#x000a0;al., 1998</xref>; <xref rid="bib76" ref-type="bibr">Li et&#x000a0;al., 2021</xref>; <xref rid="bib104" ref-type="bibr">Simonyan et&#x000a0;al., 2013</xref>; <xref rid="bib106" ref-type="bibr">Su et&#x000a0;al., 2017</xref>; <xref rid="bib109" ref-type="bibr">Tompson et&#x000a0;al., 2014</xref>; <xref rid="bib113" ref-type="bibr">Van Gerven and Bohte, 2018</xref>; <xref rid="bib120" ref-type="bibr">Wu et&#x000a0;al., 2015</xref>; <xref rid="bib111" ref-type="bibr">Toshev and Szegedy, 2014</xref>; <xref rid="bib123" ref-type="bibr">Zeiler and Fergus, 2014</xref>; <xref rid="bib121" ref-type="bibr">Xu et&#x000a0;al., 2020</xref>; <xref rid="bib10" ref-type="bibr">Baran and Popovi&#x00107;, 2007</xref>; <xref rid="bib67" ref-type="bibr">Krishtalka and Humphrey, 2000</xref>; <xref rid="bib97" ref-type="bibr">Riegler et&#x000a0;al., 2017</xref>; <xref rid="bib105" ref-type="bibr">Sorkine and Alexa, 2007</xref>; <xref rid="bib110" ref-type="bibr">Tome et&#x000a0;al., 2017</xref>.</p></sec><sec id="appsec2" sec-type="supplementary-material"><title>Supplemental information</title><p id="p0170">
<supplementary-material content-type="local-data" id="mmc1"><caption><title>Figure&#x000a0;S1. Obtaining local surface neighborhoods from the meshes and point clouds</title><p>(A) Finding local neighbor points based on original mesh connectivity</p><p>Topology of local region varies much. (B) Finding local neighbor points based on uniformly sampled points on the surface. The number of neighboring points, as well as their spatial span, are more consistent.</p></caption><media xlink:href="mmc1.pdf"/></supplementary-material>
<supplementary-material content-type="local-data" id="mmc2"><caption><title>Figure&#x000a0;S2. Rigging neural network architecture used in ANATAR</title><p>The symbol <inline-formula><mml:math id="M1" altimg="si1.gif"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes number of the input feature channels, and <inline-formula><mml:math id="M2" altimg="si2.gif"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> denotes number of the output feature channels. <inline-formula><mml:math id="M3" altimg="si3.gif"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> means the number of points. STN represents a spatial transformer network, and learned transformation is in the form of <inline-formula><mml:math id="M4" altimg="si4.gif"><mml:mrow><mml:mn>3</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x000d7;</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> matrix (<xref rid="bib94" ref-type="bibr">Qi et&#x000a0;al., 2017</xref>). Activation function between FC layers are omitted in this figure.</p></caption><media xlink:href="mmc2.pdf"/></supplementary-material>
<supplementary-material content-type="local-data" id="mmc3"><caption><title>Data S1. Presence/absence of morphological data on various 3D digital frog specimens</title></caption><media xlink:href="mmc3.docx"/></supplementary-material>
<supplementary-material content-type="local-data" id="mmc4"><caption><title>Data S2. Computational Fluids Dynamics research on a 3D sea turtle specimen</title></caption><media xlink:href="mmc4.docx"/></supplementary-material>
<supplementary-material content-type="local-data" id="mmc5"><caption><title>Data S3. Machine-learning algorithm for animal motion reconstruction</title></caption><media xlink:href="mmc5.docx"/></supplementary-material>
</p></sec><ack id="ack0010"><title>Acknowledgments</title><p id="p0150">We thank various members of the Irschick lab for assisting with model creation and the technical workflow, including Corey Zeng, Haley Huang, Jennifer Velasquez, and Madison Palmer. The blacktip shark research was covered by IACUC Protocol 15-238 to N. Hammerschlag.</p><sec id="sec11"><title>Author contributions</title><p id="p0155">D. I. did the bulk of the writing. J. Martin contributed data in the form of working on 3D models. J. Medina contributed with some writing and figure creation. E. K, Z. X., J. D., and B. P. all contributed data. N. H., P. M., J. W., A. B., A. G., S. F., T. G., F. F., U. S., M. M. S. Z., and G. L. all contributed ideas to the article, A. M. and J. P. assisted with some data collection and appendix preparation.</p></sec><sec sec-type="COI-statement" id="sec12"><title>Declaration of interests</title><p id="p0160">The authors declare no conflict of interest.</p></sec></ack><fn-group><fn id="appsec1" fn-type="supplementary-material"><p id="p0165">Supplemental information can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.isci.2022.104867" id="intref0070">https://doi.org/10.1016/j.isci.2022.104867</ext-link>.</p></fn></fn-group></back></article>
