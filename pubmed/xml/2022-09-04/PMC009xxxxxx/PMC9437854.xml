<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" article-type="research-article" dtd-version="1.3"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">iScience</journal-id><journal-id journal-id-type="iso-abbrev">iScience</journal-id><journal-title-group><journal-title>iScience</journal-title></journal-title-group><issn pub-type="epub">2589-0042</issn><publisher><publisher-name>Elsevier</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmc">PMC9437854</article-id><article-id pub-id-type="pii">S2589-0042(22)01196-8</article-id><article-id pub-id-type="doi">10.1016/j.isci.2022.104924</article-id><article-id pub-id-type="publisher-id">104924</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Artificial intelligence versus natural selection: Using computer vision techniques to classify bees and bee mimics</article-title></title-group><contrib-group><contrib contrib-type="author" id="au1"><name><surname>Bhuiyan</surname><given-names>Tanvir</given-names></name><email>bhuiyan@usf.edu</email><xref rid="aff1" ref-type="aff">1</xref><xref rid="fn1" ref-type="fn">3</xref><xref rid="cor1" ref-type="corresp">&#x02217;</xref></contrib><contrib contrib-type="author" id="au2"><name><surname>Carney</surname><given-names>Ryan M.</given-names></name><email>ryancarney@usf.edu</email><xref rid="aff2" ref-type="aff">2</xref><xref rid="cor2" ref-type="corresp">&#x02217;&#x02217;</xref></contrib><contrib contrib-type="author" id="au3"><name><surname>Chellappan</surname><given-names>Sriram</given-names></name><email>sriramc@usf.edu</email><xref rid="aff1" ref-type="aff">1</xref><xref rid="cor3" ref-type="corresp">&#x02217;&#x02217;&#x02217;</xref></contrib><aff id="aff1"><label>1</label>Computer Science and Engineering, University of South Florida, Tampa, FL 33620, USA</aff><aff id="aff2"><label>2</label>Integrative Biology, University of South Florida, Tampa, FL 33620, USA</aff></contrib-group><author-notes><corresp id="cor1"><label>&#x02217;</label>Corresponding author <email>bhuiyan@usf.edu</email></corresp><corresp id="cor2"><label>&#x02217;&#x02217;</label>Corresponding author <email>ryancarney@usf.edu</email></corresp><corresp id="cor3"><label>&#x02217;&#x02217;&#x02217;</label>Corresponding author <email>sriramc@usf.edu</email></corresp><fn id="fn1"><label>3</label><p id="ntpara0010">Lead contact</p></fn></author-notes><pub-date pub-type="pmc-release"><day>13</day><month>8</month><year>2022</year></pub-date><!-- PMC Release delay is 0 months and 0 days and was based on <pub-date
						pub-type="epub">.--><pub-date pub-type="collection"><day>16</day><month>9</month><year>2022</year></pub-date><pub-date pub-type="epub"><day>13</day><month>8</month><year>2022</year></pub-date><volume>25</volume><issue>9</issue><elocation-id>104924</elocation-id><history><date date-type="received"><day>10</day><month>2</month><year>2022</year></date><date date-type="rev-recd"><day>19</day><month>6</month><year>2022</year></date><date date-type="accepted"><day>9</day><month>8</month><year>2022</year></date></history><permissions><copyright-statement>&#x000a9; 2022 The Authors</copyright-statement><copyright-year>2022</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).</license-p></license></permissions><abstract id="abs0010"><title>Summary</title><p>Many groups of stingless insects have independently evolved mimicry of bees to fool would-be predators. To investigate this mimicry, we trained artificial intelligence (AI) algorithms&#x02014;specifically, computer vision&#x02014;to classify citizen scientist images of bees, bumble bees, and diverse bee mimics. For detecting bees and bumble bees, our models achieved accuracies of <inline-formula><mml:math id="M1" altimg="si1.gif"><mml:mrow><mml:mn>91.71</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M2" altimg="si2.gif"><mml:mrow><mml:mn>88.86</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, respectively. As a proxy for a natural predator, our models were poorest in detecting bee mimics that exhibit both aggressive and defensive mimicry. Using the explainable AI method of class activation maps, we validated that our models learn from appropriate components within the image, which in turn provided anatomical insights. Our t-SNE plot yielded perfect within-group clustering, as well as between-group clustering that grossly replicated the phylogeny. Ultimately, the transdisciplinary approaches herein can enhance global citizen science efforts as well as investigations of mimicry and morphology of bees and other insects.</p></abstract><abstract abstract-type="graphical" id="abs0015"><title>Graphical abstract</title><fig id="undfig1" position="anchor"><graphic xlink:href="fx1"/></fig></abstract><abstract abstract-type="author-highlights" id="abs0020"><title>Highlights</title><p><list list-type="simple" id="ulist0010"><list-item id="u0010"><label>&#x02022;</label><p id="p0010">AI models for classifying bees and bumble bees achieved 92% and 89% accuracy</p></list-item><list-item id="u0015"><label>&#x02022;</label><p id="p0015">AI models were fooled most by bee mimics exhibiting both aggressive and defensive mimicry</p></list-item><list-item id="u0020"><label>&#x02022;</label><p id="p0020">Class activation maps explained the anatomical reasoning of AI model classifications</p></list-item><list-item id="u0025"><label>&#x02022;</label><p id="p0025">t-SNE plot exhibited perfect phylogenetic clustering within and between groups</p></list-item></list></p></abstract><abstract abstract-type="teaser" id="abs0025"><p>Artificial intelligence; Bioinformatics; Computing methodology; Entomology; Zoology</p></abstract><kwd-group id="kwrds0010"><title>Subject areas</title><kwd>Artificial intelligence</kwd><kwd>Bioinformatics</kwd><kwd>Computing methodology</kwd><kwd>Entomology</kwd><kwd>Zoology</kwd></kwd-group></article-meta><notes><p id="misc0010">Published: September 16, 2022</p></notes></front><body><sec id="sec1"><title>Introduction</title><p id="p0050">Bees have been important pollinators ever since the Cretaceous (<xref rid="bib17" ref-type="bibr">Genise et&#x000a0;al., 2020</xref>). As a source of honey, widespread exploitation of honey bees dates back to at least the early Neolithic farmers (<inline-formula><mml:math id="M3" altimg="si3.gif"><mml:mrow><mml:mo>&#x0223c;</mml:mo></mml:mrow></mml:math></inline-formula> 9 kya, (<xref rid="bib39" ref-type="bibr">Roffet-Salque et&#x000a0;al., 2015</xref>)), and the earliest known apiculture was practiced by the Ancient Egyptians (<xref rid="bib9" ref-type="bibr">Crane 1999</xref>). Darwin&#x02019;s experiments with &#x0201c;humble bees,&#x0201d; as bumble bees were once called, exemplify how these insects have furthered our understanding of natural selection with respect to co-evolution, ecosystem webs, and social behavior (<xref rid="bib11" ref-type="bibr">Darwin 2003</xref>). Such a rich history makes it all the more tragic that today, despite their ecological and economic importance, bees are facing an unprecedented anthropogenic decline in diversity and abundance (<xref rid="bib38" ref-type="bibr">Potts et&#x000a0;al., 2010</xref>)&#x02014;making their identification and conservation urgent concerns.</p><p id="p0055">The mimicry of bees also has a storied history, albeit inadvertently so. As recounted by poets from Virgil to Shakespeare, the ancient superstitious ritual of bugonia held that bees spontaneously generated from the decaying carcasses of animals such as oxen (<xref rid="bib31" ref-type="bibr">Osten-Sacken, 1894</xref>). The primary culprit of this deception was likely the honey bee-mimicking <italic>Eristalis tenax</italic> (Syrphidae), a cosmopolitan hoverfly that occasionally lays its eggs on carcasses (<xref rid="bib31" ref-type="bibr">Osten-Sacken, 1894</xref>) (<xref rid="bib40" ref-type="bibr">Scholl et&#x000a0;al., 2019</xref>). Its common name, the common drone fly, stems from the resemblance of members of this genus to the drones of honey bees (<xref rid="fig1" ref-type="fig">Figure&#x000a0;1</xref>).<fig id="fig1"><label>Figure&#x000a0;1</label><caption><p>Citizen scientist photos of bees and bee mimic flies</p><p>(A) Bumble bee mimic (Asilidae: <italic>Laphria thoracica</italic>) preying on bumble bee (Hymenoptera: <italic>Bombus</italic> spp.), a putative example of aggressive mimicry. Honey bee mimic (B, Syrphidae: <italic>Eristalis tenax</italic>) and honey bee (C, Hymenoptera: <italic>Apis mellifera</italic>), an example of defensive mimicry. Red areas in the class activation maps (D and E) denote the importance of the wings and abdominal markings in these two correct classifications (mimic as a non-bee, honey bee as a bee) by our AI algorithms, elaborated in this article. Original images from iNaturalist.</p></caption><graphic xlink:href="gr1"/></fig></p><p id="p0060">Indeed, Hymenoptera&#x02014;consisting of bees, wasps, and ants&#x02014;is the most mimicked insect order (<xref rid="bib36" ref-type="bibr">Poulton 1890</xref>), which is not surprising given these insects&#x02019; formidable sting. A resemblance to such well-equipped insect models protects harmless mimics by fooling would-be predators with a false harmful signal. This type of defensive mimicry is known as Batesian mimicry (<xref rid="bib33" ref-type="bibr">Pasteur 1982</xref>).</p><p id="p0065">A less common and somewhat opposite&#x02014;although not mutually exclusive&#x02014;phenomenon is aggressive mimicry. This &#x0201c;wolf in sheep&#x02019;s clothing&#x0201d; strategy evolved to fool prey into thinking that the mimic is harmless or even beneficial. A classic example of this is the anglerfish and its lure. However, the term aggressive mimicry was actually first ascribed to bumble bee mimics, also from the hover fly family Syrphidae (genus <italic>Volucella</italic>), by Poulton in 1890 (<xref rid="bib36" ref-type="bibr">Poulton 1890</xref>) following similar observations of this genus by Kirby and Spence (1817) (<xref rid="bib24" ref-type="bibr">Kirby 1857</xref>) and <xref rid="bib46" ref-type="bibr">Wallace (1871)</xref> (<xref rid="bib46" ref-type="bibr">Wallace 1871</xref>). Poulton noted: &#x0201c;<italic>In some cases the Mimicry enables the aggressive form to lay eggs in the nest of that which it resembles</italic>, <italic>so that its larv&#x000e6; live upon the food stored up by the latter or even upon the larv&#x000e6; themselves</italic>. <italic>The boldness of these enemies sometimes depends upon the perfection of their disguise</italic>.&#x0201d; (<xref rid="bib36" ref-type="bibr">Poulton 1890</xref>; see also discussion in <xref rid="bib5" ref-type="bibr">Brower et&#x000a0;al., 1960</xref>).</p><p id="p0070">Poulton later published observations of three other researchers, who demonstrated that larvae of the robber fly family Asilidae (<italic>Hyperechia</italic> spp.) also prey upon larvae of the carpenter bees (<italic>Xylocopa</italic> spp.) that they mimic as adults (<xref rid="bib5" ref-type="bibr">Brower et&#x000a0;al., 1960</xref>). These findings were later corroborated by <xref rid="bib43" ref-type="bibr">Tsacas et&#x000a0;al., (1970)</xref>, who similarly hypothesized that this mimicry enables egg-laying in or near the opening of the carpenter bee nests (<xref rid="bib43" ref-type="bibr">Tsacas et&#x000a0;al., 1970</xref>). Strikingly, <italic>Hyperechia</italic> and other asilids also prey upon the adult forms of their model species (and related aculeate hymenoptera) (<xref rid="fig1" ref-type="fig">Figure&#x000a0;1</xref>) (<xref rid="bib4" ref-type="bibr">Bromley 1930</xref>). Thus, these robber flies exhibit not only defensive (Batesian) mimicry, but possibly one or two types of aggressive mimicry: Kirbyan mimicry to prey on bee larvae and Batesian-Wallacian mimicry to prey on bee adults (<xref rid="bib33" ref-type="bibr">Pasteur 1982</xref>).</p><p id="p0075">Our motivation for this study is 2-fold: first, to design artificial intelligence techniques to classify bees, especially bumble bees. Second, to identify a variety of convergently evolved bee mimics&#x02014;in other words, to assess whether the visual resemblance to bees gained through natural selection can fool artificial intelligence algorithms that were trained on bees. For both problems, we use images uploaded to the iNaturalist platform (<xref rid="bib28" ref-type="bibr">iNaturalist, n.d.</xref>), which is a crowning example of a global citizen science effort toward conservation. iNaturalist was launched in 2008, and as of July 2022, users have contributed more than 123 million observations of animals, plants, and other organisms worldwide. At the time of writing, the platform has <inline-formula><mml:math id="M4" altimg="si4.gif"><mml:mrow><mml:mn>1,571,407</mml:mn></mml:mrow></mml:math></inline-formula> observations of bees from <inline-formula><mml:math id="M5" altimg="si5.gif"><mml:mrow><mml:mn>242,667</mml:mn></mml:mrow></mml:math></inline-formula> observers. For each observation contributed, multiple identifications can be made by community members. Among these observations, the most reliable is &#x0201c;Research Grade,&#x0201d; for which two conditions must be met. The observation must contain a valid date, location, photo, or sound, and not be of a captive/cultivated organism; and at least two members must agree on the identification (<xref rid="bib29" ref-type="bibr">iNaturalist, 2020</xref>). For bees, we found that <inline-formula><mml:math id="M6" altimg="si6.gif"><mml:mrow><mml:mn>812,847</mml:mn></mml:mrow></mml:math></inline-formula>
<inline-formula><mml:math id="M7" altimg="si7.gif"><mml:mrow><mml:mo>(</mml:mo><mml:mn>52</mml:mn><mml:mo>%</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> of the observations are Research Grade, which means that close to half of the uploaded observations are not reliably identified. Leveraging Research Grade images, our specific contributions in this article are:</p><sec id="sec1.1"><title>An artificial intelligence model for classifying bees from other insects</title><p id="p0080">We designed a model based on VGG16 (<xref rid="bib41" ref-type="bibr">Simonyan and Zisserman 2015</xref>) to classify bees from non-bees. This AI model was trained, validated, and tested on <inline-formula><mml:math id="M8" altimg="si8.gif"><mml:mrow><mml:mn>3,029</mml:mn></mml:mrow></mml:math></inline-formula> bee and <inline-formula><mml:math id="M9" altimg="si9.gif"><mml:mrow><mml:mn>2,943</mml:mn></mml:mrow></mml:math></inline-formula> non-bee insect images. Note that throughout the text, the term &#x0201c;model&#x0201d; refers to one of two disparate entities: an AI algorithm, or an insect species upon which mimicry is based.</p></sec><sec id="sec1.2"><title>An artificial intelligence model for classifying bumble bees from other bees</title><p id="p0085">We designed a model based on ResNet-101 (<xref rid="bib19" ref-type="bibr">He et&#x000a0;al., 2016</xref>) to classify bumble bees from other bees (which we refer to as non-bumble bee bees, or simply non-bumble bees when the context is clear). This model was trained, validated, and tested on <inline-formula><mml:math id="M10" altimg="si10.gif"><mml:mrow><mml:mn>1,554</mml:mn></mml:mrow></mml:math></inline-formula> bumble bee and <inline-formula><mml:math id="M11" altimg="si11.gif"><mml:mrow><mml:mn>1,475</mml:mn></mml:mrow></mml:math></inline-formula> non-bumble bee images.</p></sec><sec id="sec1.3"><title>Evaluating both artificial intelligence models against independently evolved bee mimics</title><p id="p0090">Our taxa comprise 19 bee mimic species across six diverse insect families (Asilidae, Bombyliidae, Scarabaeidae, Sphingidae, Syrphidae, Tachinidae) in three orders&#x02014;Coleoptera (beetles), Diptera (flies), and Lepidoptera (moths)&#x02014;as well as related outgroups of nine species of wasp mimics and 13 species of non-mimics to serve as controls (see Supplemental Information). We hypothesize that 1) within each clade, bee mimics will exhibit lower model classification accuracy compared to their non-mimic counterparts, and 2) wasp mimics will exhibit intermediate accuracy; furthermore, 3) bee mimics that are believed to exploit aggressive mimicry toward bees will exhibit better mimicry&#x02014;as defined by lower classification accuracy&#x02014;compared to bee mimics that employ only defensive mimicry. Finally, we visualize such classifications of artificial intelligence vis-a-vis natural selection through an integration of the t-Distributed Stochastic Neighbor Embedding (t-SNE) technique (phenotype) and evolutionary relationships (phylogeny). t-SNE is an AI technique for dimensionality reduction, particularly well-suited for the visualization of high-dimensional datasets (Hinton and Roweis 2002)).</p></sec><sec id="sec1.4"><title>Artificial intelligence-driven insights on the fidelity of our techniques</title><p id="p0095">To evaluate the fidelity and explainability of our AI models, we adopt the technique of class activation maps (CAMs) (<xref rid="bib55" ref-type="bibr">Zhou et&#x000a0;al., 2016</xref>) to pinpoint which pixels in an image are most used in a classification (<xref rid="fig1" ref-type="fig">Figures&#x000a0;1</xref>D and 1E). We also convert images to grayscale in order to compare the performance of each dataset and to make observations on the respective roles of color vs. pattern in the classifications (especially relevant in the context of aposematic mimicry).</p></sec><sec id="sec1.5"><title>Related work</title><sec id="sec1.5.1"><title>Artificial intelligence techniques to classify bees</title><p id="p0100">In a 2021 study (<xref rid="bib42" ref-type="bibr">Spiesman et&#x000a0;al., 2021</xref>), multiple deep learning techniques were designed to classify species of bumble bees using images from citizen science platforms such as Bumble Bee Watch, BugGuide, and iNaturalist. The authors collected <inline-formula><mml:math id="M12" altimg="si12.gif"><mml:mrow><mml:mn>89,000</mml:mn></mml:mrow></mml:math></inline-formula> images from 36 species of bumble bees, including (as in our article) <italic>Bombus (B.) vosnesenskii</italic>, <italic>B. terricola</italic>, <italic>B. pensylvanicus</italic>, <italic>B. griseocollis</italic>, and <italic>B. affinis</italic>. The deep learning architectures used were Wide-ResNet-101, InceptionV3, ResNet-101, and MnasNet101, which achieved accuracies of <inline-formula><mml:math id="M13" altimg="si1.gif"><mml:mrow><mml:mn>91.71</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M14" altimg="si13.gif"><mml:mrow><mml:mn>91.62</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="M15" altimg="si14.gif"><mml:mrow><mml:mn>91.33</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M16" altimg="si15.gif"><mml:mrow><mml:mn>85.79</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>, respectively, in classifying the species of bumble bees. In a 2022 study (<xref rid="bib13" ref-type="bibr">De Nart et&#x000a0;al., 2022</xref>), the authors classified species of honey bees. What is unique about this work is that the images used were exclusively wings. <inline-formula><mml:math id="M17" altimg="si16.gif"><mml:mrow><mml:mn>9,887</mml:mn></mml:mrow></mml:math></inline-formula> wing images spread across seven species were extracted from a large dataset of honey bee images archived at CREA-Research Center for Agriculture and Environment (CREA-AA, Italy). To extract the wings alone from the archived images, a RetinaNet model with a ResNet-50 backbone was designed (<xref rid="bib13" ref-type="bibr">De Nart et&#x000a0;al., 2022</xref>). Multiple deep neural network models were then developed, including ResNet-50, MobileNetV2, InceptionNetV3, and InceptionResNetV2, with the highest accuracy of <inline-formula><mml:math id="M18" altimg="si17.gif"><mml:mrow><mml:mn>99.15</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> achieved using the InceptionNetV3 model. Both works referred to above focus exclusively on identifying species of bumble bees and honey bees, and as such are complementary to the work in our article. Other related work on insect identification using image data and AI techniques include recent studies detecting invasive mosquito vectors (<xref rid="bib7" ref-type="bibr">Carney et&#x000a0;al., 2022</xref>; <xref rid="bib27" ref-type="bibr">Minakshi et&#x000a0;al., 2020</xref>), crop pests (<xref rid="bib23" ref-type="bibr">Kasinathan et&#x000a0;al., 2021</xref>), beetles (<xref rid="bib45" ref-type="bibr">Venegas et&#x000a0;al., 2021</xref>), and hornets (<xref rid="bib21" ref-type="bibr">Jeong et&#x000a0;al., 2020</xref>), as well as ants and their movements (<xref rid="bib51" ref-type="bibr">Wu et&#x000a0;al., 2020</xref>).</p></sec><sec id="sec1.5.2"><title>Artificial intelligence techniques to study bee mimicry</title><p id="p0105">In the realm of investigating bee mimicry using deep neural networks, we are aware of one recent work in 2019, which looks at M<inline-formula><mml:math id="M19" altimg="si18.gif"><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>&#x000a8;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>llerian mimicry among bumble bees across spatial scales (<xref rid="bib16" ref-type="bibr">Ezray et&#x000a0;al., 2019</xref>). M<inline-formula><mml:math id="M20" altimg="si18.gif"><mml:mrow><mml:mover accent="true"><mml:mi>u</mml:mi><mml:mo>&#x000a8;</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>llerian mimicry is a form of biological resemblance in which two or more unrelated harmful (e.g., stinging) organisms exhibit closely similar warning systems, such as the same pattern of bright colors. The authors generated their own dataset to represent the color patterns of 35 bumble bee species using a standardized template that removed the effects of body size while still maintaining the morphologically monotonous shape of bumble bees. This method focuses on only color pattern differences and avoids variation from other sources. Color diagrams for the 35 species were generated in this manner by applying particular colors to each segmental domain of their template to match the color patterns displayed in a guide (<xref rid="bib50" ref-type="bibr">Williams et&#x000a0;al., 2014</xref>). With the resulting dataset, the authors first calculated the perceptual distance between every pairwise set of bumble bee color pattern diagrams, using a method proposed in (<xref rid="bib48" ref-type="bibr">Wham et&#x000a0;al., 2019</xref>). Then, they employed the t-SNE method (<xref rid="bib44" ref-type="bibr">Van der Maaten and Hinton 2008</xref>) to visualize high-dimensionality distances in a two-dimensional (2D) plot. To derive data on the geographic locations of various bumble bee species in the contiguous United States, the authors extracted location information from <inline-formula><mml:math id="M21" altimg="si19.gif"><mml:mrow><mml:mn>160,213</mml:mn></mml:mrow></mml:math></inline-formula> bumble bee records stored at the Global Biodiversity Information Facility (GBIF). Using both the t-SNE plots and the geographic data, the authors found that bumble bees exhibit color mimicry patterns that are geographically clustered, but sometimes imperfect. The authors also concluded that mimicry patterns gradually transition spatially instead of exhibiting discrete boundaries. Furthermore, the authors identified that transition zones of three co-mimicking, polymorphic species (<italic>B. flavifrons</italic>, <italic>B. melanopygus</italic>, and <italic>B. bifarius</italic>), where active selection is driving phenotype frequencies, differ within a broad region of poor mimicry.</p><p id="p0110">Our study is related to the above-referenced work (<xref rid="bib16" ref-type="bibr">Ezray et&#x000a0;al., 2019</xref>) in that we also employ the t-SNE technique (<xref rid="bib44" ref-type="bibr">Van der Maaten and Hinton 2008</xref>). However, we do not investigate mimicry between various bumble bee species, but rather examine similarities and differences between various species of bee mimics, wasp mimics, and non-mimic insects. Furthermore, our work in this article classifies bee mimics vs. bees, which is not the case with the work in (<xref rid="bib16" ref-type="bibr">Ezray et&#x000a0;al., 2019</xref>).</p></sec></sec></sec><sec id="sec2"><title>Results</title><p id="p0115">We used <inline-formula><mml:math id="M22" altimg="si20.gif"><mml:mrow><mml:mn>6,332</mml:mn></mml:mrow></mml:math></inline-formula> color images&#x02014;which were also converted to grayscale&#x02014;to train, validate, and test our AI models. Image counts are presented in <xref rid="tbl1" ref-type="table">Tables&#x000a0;1</xref>, <xref rid="tbl2" ref-type="table">2</xref>, <xref rid="tbl3" ref-type="table">3</xref>, and <xref rid="tbl4" ref-type="table">4</xref>. The metric we used to evaluate our models is classification accuracy, defined as the percentage of correct predictions out of the total number of predictions:<disp-formula id="fd1"><label>(Equation&#x000a0;1)</label><mml:math id="M23" altimg="si21.gif"><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mo linebreak="badbreak">=</mml:mo><mml:mfrac><mml:mrow><mml:mn>100</mml:mn><mml:mo linebreak="badbreak">&#x000d7;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mo linebreak="badbreak">+</mml:mo><mml:mi>F</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.25em"/><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula><table-wrap position="float" id="tbl1"><label>Table&#x000a0;1</label><caption><p>Insect orders and non-bee image counts</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Order</th><th>Count</th></tr></thead><tbody><tr><td>Hymenoptera</td><td>330</td></tr><tr><td>Blattodea</td><td>329</td></tr><tr><td>Coleoptera</td><td>337</td></tr><tr><td>Diptera</td><td>328</td></tr><tr><td>Lepidoptera</td><td>630</td></tr><tr><td>Odonata</td><td>329</td></tr><tr><td>Orthoptera</td><td>660</td></tr></tbody></table><table-wrap-foot><fn><p>See <xref rid="mmc2" ref-type="supplementary-material">Table&#x000a0;S2</xref> for details.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="tbl2"><label>Table&#x000a0;2</label><caption><p>Bumble bee species and image counts</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Species</th><th>Count</th></tr></thead><tbody><tr><td><italic>affinis</italic></td><td>247</td></tr><tr><td><italic>griseocollis</italic></td><td>250</td></tr><tr><td><italic>impatiens</italic></td><td>250</td></tr><tr><td><italic>melanopygus</italic></td><td>150</td></tr><tr><td><italic>pascuorum</italic></td><td>157</td></tr><tr><td><italic>pensylvanicus</italic></td><td>200</td></tr><tr><td><italic>terrestris</italic></td><td>250</td></tr><tr><td><italic>bimaculatus</italic></td><td>10</td></tr><tr><td><italic>flavifrons</italic></td><td>10</td></tr><tr><td><italic>lucorum</italic></td><td>10</td></tr><tr><td><italic>terricola</italic></td><td>10</td></tr><tr><td><italic>vosnesenskii</italic></td><td>10</td></tr></tbody></table><table-wrap-foot><fn><p>Images in the last five rows are not used for training and validation but only used for testing. See <xref rid="mmc1" ref-type="supplementary-material">Table&#x000a0;S1</xref> for details.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="tbl3"><label>Table&#x000a0;3</label><caption><p>Non-bumble bee genera and image counts</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Genus</th><th>Count</th></tr></thead><tbody><tr><td><italic>Andrena</italic></td><td>179</td></tr><tr><td><italic>Anthidium</italic></td><td>180</td></tr><tr><td><italic>Apis</italic></td><td>518</td></tr><tr><td><italic>Centris</italic></td><td>239</td></tr><tr><td><italic>Megachile</italic></td><td>184</td></tr><tr><td><italic>Melissodes</italic></td><td>174</td></tr><tr><td><italic>Osmia</italic></td><td>1</td></tr></tbody></table><table-wrap-foot><fn><p>See <xref rid="mmc1" ref-type="supplementary-material">Table&#x000a0;S1</xref> for details.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="tbl4"><label>Table&#x000a0;4</label><caption><p>Mimics and outgroups with image counts, categorized into three orders (from top to bottom: Coleoptera, Diptera, and Lepidoptera)</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Family</th><th>Count</th></tr></thead><tbody><tr><td>Scarabaeidae (bee mimics)</td><td>30</td></tr><tr><td><italic>Scarabaeidae (non-mimic</italic><italic>s</italic><italic>)</italic></td><td>30</td></tr><tr><td>Asilidae (bee mimics)</td><td>30</td></tr><tr><td>Bombyliidae (bee mimics)</td><td>30</td></tr><tr><td>Syrphidae (bee mimics)</td><td>30</td></tr><tr><td><italic>Syrphidae</italic> (<italic>wasp mimic</italic><italic>s</italic>)</td><td>30</td></tr><tr><td>Tachinidae (bee mimics)</td><td>30</td></tr><tr><td><italic>Tachinidae</italic> (<italic>wasp mimic</italic><italic>s</italic>)</td><td>30</td></tr><tr><td><italic>Tachinidae</italic> (<italic>non-mimic</italic><italic>s</italic>)</td><td>30</td></tr><tr><td>Sphingidae (bee mimics)</td><td>30</td></tr><tr><td><italic>Sesiidae</italic> (<italic>wasp mimic</italic><italic>s</italic>)</td><td>30</td></tr><tr><td><italic>various</italic> (<italic>non-mimic</italic><italic>s</italic>)</td><td>30</td></tr><tr><td>Total</td><td>360</td></tr></tbody></table><table-wrap-foot><fn><p>Italics denote the non-bee mimics (i.e., wasp mimics and non-mimics). See <xref rid="mmc3" ref-type="supplementary-material">Table&#x000a0;S3</xref> for species-level designations.</p></fn></table-wrap-foot></table-wrap></p><sec id="sec2.1"><title>Classifying bees from other insects</title><p id="p0120">For this problem, we chose images presented in <xref rid="tbl1" ref-type="table">Tables&#x000a0;1</xref>, <xref rid="tbl2" ref-type="table">2</xref>, and <xref rid="tbl3" ref-type="table">3</xref> for training and validation. Broadly, <inline-formula><mml:math id="M24" altimg="si22.gif"><mml:mrow><mml:mn>80</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> of images in each row in the tables was used for training and validation, and the other <inline-formula><mml:math id="M25" altimg="si23.gif"><mml:mrow><mml:mn>20</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> was unseen and purely used for testing. Note that images of bumble bee species in the last five rows in <xref rid="tbl2" ref-type="table">Table&#x000a0;2</xref> were used only for testing, and not for training or validation. We have included all the species and image counts that were used in our bee and non-bee datasets in <xref rid="mmc2" ref-type="supplementary-material">Supplemental Tables 2</xref> and <xref rid="mmc3" ref-type="supplementary-material">3</xref>. All the bee species were from the families &#x0201c;Andrenidae,&#x0201d; &#x0201c;Apidae,&#x0201d; and &#x0201c;Megachilidae.&#x0201d;</p><p id="p0125">Results from our VGG-based AI algorithm (<xref rid="bib41" ref-type="bibr">Simonyan and Zisserman 2015</xref>; details are elaborated in <xref rid="sec5" ref-type="sec">STAR Methods</xref> section) for the classification of bees from other insects are presented in <xref rid="tbl5" ref-type="table">Table&#x000a0;5</xref>. We considered a classification as correct if the AI model identified a mimic as a non-bee. All testing results reported are for unseen images only. We trained three AI models separately on color images only, grayscale images only, and an equal combination of color and grayscale images. Each model was tested on an equal number of color and grayscale images, and the results are presented in the appropriate column in <xref rid="tbl5" ref-type="table">Table&#x000a0;5</xref>. Boldface percentages represent the top classification accuracy for each of the color and grayscale testing image sets among the three models. We can observe in row 3 that the model trained on grayscale images yielded the best accuracies in classifying bees from non-bees. From the perspective of classifying mimics, the accuracy of the VGG model is not very good. However, we observe that the model correctly detected non-mimics with better accuracy than mimics across color and grayscale images (as expected, owing to the morphological similarities between bee mimics and bees). For clarity, we wish to emphasize that no mimic images were used for training and validation in any of our AI models. As such, the mimic species were purely unseen by the AI. Details of this model architecture are shown in <xref rid="tbl6" ref-type="table">Table 6</xref> and hyperparameters are shown in <xref rid="tbl7" ref-type="table">Table 7</xref>.<table-wrap position="float" id="tbl5"><label>Table&#x000a0;5</label><caption><p>Comparison of accuracy (in %) between three VGG16-based models that classify bees from non-bees, evaluated against testing images of bee mimics and non-bee mimics (italicized)</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Training image type</th><th colspan="2">Color</th><th colspan="2">Gray</th><th colspan="2">Color&#x000a0;+ Gray</th><th>Average</th></tr></thead><tbody><tr><td>Testing image type</td><td>Color</td><td>Gray</td><td>Color</td><td>Gray</td><td>Color</td><td>Gray</td><td/></tr><tr><td>Bees vs. non-bees</td><td>84.63</td><td>79.79</td><td><bold>91.36</bold></td><td><bold>91.71</bold></td><td>90.85</td><td>91.62</td><td>88.33</td></tr><tr><td colspan="8"><hr/></td></tr><tr><td colspan="8"><bold>Testing accuracy</bold></td></tr><tr><td colspan="8"><hr/></td></tr><tr><td>Scarabaeidae (bee mimics)</td><td>23.33</td><td>43.33</td><td><bold>60.00</bold></td><td><bold>53.33</bold></td><td>43.33</td><td>43.33</td><td>44.44</td></tr><tr><td><italic>Scarabaeidae (non-mimic</italic><italic>s</italic><italic>)</italic></td><td>76.67</td><td>80.00</td><td>83.33</td><td>83.33</td><td><bold>86.67</bold></td><td><bold>86.67</bold></td><td>82.78</td></tr><tr><td>Asilidae (bee mimics)</td><td>10.00</td><td><bold>36.67</bold></td><td>16.67</td><td>13.33</td><td><bold>20.00</bold></td><td>13.33</td><td>18.33</td></tr><tr><td>Bombyliidae (bee mimics)</td><td><bold>46.67</bold></td><td><bold>53.33</bold></td><td>20.00</td><td>16.67</td><td>33.33</td><td>26.67</td><td>32.78</td></tr><tr><td>Syrphidae (bee mimics)</td><td><bold>23.33</bold></td><td><bold>36.67</bold></td><td><bold>23.33</bold></td><td>16.67</td><td>13.33</td><td>13.33</td><td>21.11</td></tr><tr><td><italic>Syrphidae (wasp mimic</italic><italic>s</italic><italic>)</italic></td><td>36.67</td><td>30.00</td><td><bold>46.67</bold></td><td>36.67</td><td><bold>50.00</bold></td><td>33.33</td><td>38.89</td></tr><tr><td>Tachinidae (bee mimics)</td><td>23.33</td><td><bold>30.00</bold></td><td><bold>30.00</bold></td><td>20.00</td><td>16.67</td><td>13.33</td><td>22.22</td></tr><tr><td><italic>Tachinidae (wasp mimic</italic><italic>s</italic><italic>)</italic></td><td>60.00</td><td>60.00</td><td><bold>83.33</bold></td><td><bold>86.67</bold></td><td>63.33</td><td>70.00</td><td>71.67</td></tr><tr><td><italic>Tachinidae (non-mimic</italic><italic>s</italic><italic>)</italic></td><td>66.67</td><td>60.00</td><td><bold>80.00</bold></td><td><bold>83.33</bold></td><td>60.00</td><td>63.33</td><td>68.89</td></tr><tr><td>Lepidoptera (bee mimics)</td><td>30.00</td><td>50.00</td><td>56.67</td><td><bold>60.00</bold></td><td><bold>70.00</bold></td><td><bold>60.00</bold></td><td>54.45</td></tr><tr><td><italic>Lepidoptera (wasp mimic</italic><italic>s</italic><italic>)</italic></td><td>66.67</td><td><bold>83.33</bold></td><td><bold>73.33</bold></td><td>70.00</td><td>70.00</td><td>70.00</td><td>72.22</td></tr><tr><td><italic>Lepidoptera (non-mimic</italic><italic>s</italic><italic>)</italic></td><td>86.67</td><td>90.00</td><td>96.67</td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td>95.56</td></tr><tr><td>Average</td><td>45.83</td><td><bold>54.44</bold></td><td><bold>55.83</bold></td><td>53.33</td><td>52.22</td><td>49.44</td><td>51.95</td></tr></tbody></table><table-wrap-foot><fn><p>A correct classification is when the model accurately identifies one of these insects as non-bee. Boldface percentages represent the top classification accuracy for color and grayscale images among the three models for each row. The third row presents the accuracy for classifying bees from non-bees. For this, 683 bee and 539 non-bee color images were used, which were evenly distributed across all the species. Another 683 bee and 539 non-bee grayscale versions of those same images were used for the evaluation of the models with grayscale images. The following rows denote accuracies for the same AI models in classifying mimics as in <xref rid="tbl4" ref-type="table">Table&#x000a0;4</xref>. For testing mimics, 30 color and 30 corresponding grayscale images for each of the 12 groups were used.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="tbl6"><label>Table&#x000a0;6</label><caption><p>VGG16-based model architecture details</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Layer</th><th>Input size</th><th>Output size</th></tr></thead><tbody><tr><td>VGG16 5 Conv blocks</td><td>224, 224, 3</td><td>7, 7, 512</td></tr><tr><td>Global_average_pooling2d</td><td>7, 7, 512</td><td>512</td></tr><tr><td>dense_1 (Dense)</td><td>512</td><td>256</td></tr><tr><td>dropout_1 (Dropout)</td><td>256</td><td>256</td></tr><tr><td>dense_2 (Dense)</td><td>256</td><td>128</td></tr><tr><td>dropout_2 (Dropout)</td><td>128</td><td>128</td></tr><tr><td>dense_3 (Dense)</td><td>128</td><td>64</td></tr><tr><td>dropout_3 (Dropout)</td><td>64</td><td>64</td></tr><tr><td>dense_4 (Dense)</td><td>64</td><td>2</td></tr></tbody></table></table-wrap><table-wrap position="float" id="tbl7"><label>Table&#x000a0;7</label><caption><p>VGG16-based model hyperparameters</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Hyperparameter</th><th>Value</th></tr></thead><tbody><tr><td>Loss</td><td>Binary Cross entropy</td></tr><tr><td>Optimizer</td><td>Adam Optimizer</td></tr><tr><td>Momentum</td><td>0.9</td></tr><tr><td>Early training epochs</td><td>50</td></tr><tr><td>Early epochs learning rate</td><td>0.005</td></tr><tr><td>Whole model training epochs</td><td>200</td></tr><tr><td>Learning rate for all epochs</td><td>0.0005</td></tr></tbody></table></table-wrap></p></sec><sec id="sec2.2"><title>Classifying bumble bees from non-bumble bees</title><p id="p0130">For this problem, we employ a similar approach as above. The AI model used was ResNet-101 (<xref rid="bib19" ref-type="bibr">He et&#x000a0;al., 2016</xref>; details are elaborated in <xref rid="sec5" ref-type="sec">STAR Methods</xref> section). Here again, we selected <inline-formula><mml:math id="M26" altimg="si22.gif"><mml:mrow><mml:mn>80</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> of&#x000a0;the images from the first seven species in <xref rid="tbl2" ref-type="table">Table&#x000a0;2</xref> and the first seven genera in <xref rid="tbl3" ref-type="table">Table&#x000a0;3</xref> for model training and validation. The remaining images were used for testing. Here also, we trained three AI models separately on color images only, grayscale images only, and an equal combination of color and grayscale images. Each model was tested on an equal number of color and grayscale images, and the results are presented in the appropriate column in <xref rid="tbl8" ref-type="table">Table&#x000a0;8</xref>. We considered a classification as correct if the AI model identified a mimic as a non-bumble bee. All testing results are for unseen images only.<table-wrap position="float" id="tbl8"><label>Table&#x000a0;8</label><caption><p>Comparison of accuracy (in %) between three ResNet-101-based models that classify bumble bees from non-bumble bee bees, evaluated against testing images of bee mimics and non-bee mimics (italicized)</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Training image type</th><th colspan="2">Color</th><th colspan="2">Gray</th><th colspan="2">Color&#x000a0;+ Gray</th><th>Average</th></tr></thead><tbody><tr><td>Testing image type</td><td>Color</td><td>Gray</td><td>Color</td><td>Gray</td><td>Color</td><td>Gray</td><td/></tr><tr><td>Species used in training, validation</td><td>73.48</td><td>77.82</td><td>82.33</td><td>81.99</td><td><bold>85.78</bold></td><td><bold>86.87</bold></td><td>81.38</td></tr><tr><td>Species not used in training, validation</td><td>86.73</td><td>86.73</td><td><bold>94.90</bold></td><td><bold>93.88</bold></td><td>92.86</td><td>92.86</td><td>91.33</td></tr><tr><td>Average for testing</td><td>77.90</td><td>80.79</td><td>86.52</td><td>85.95</td><td><bold>88.14</bold></td><td><bold>88.86</bold></td><td>86.36</td></tr><tr><td colspan="8"><hr/></td></tr><tr><td colspan="8"><bold>Testing accuracy</bold></td></tr><tr><td colspan="8"><hr/></td></tr><tr><td>Scarabaeidae (bee mimics)</td><td><bold>93.33</bold></td><td><bold>93.33</bold></td><td>76.67</td><td>60.00</td><td><bold>93.33</bold></td><td>90.00</td><td>84.44</td></tr><tr><td><italic>Scarabaeidae (non-mimic</italic><italic>s</italic><italic>)</italic></td><td><bold>90.00</bold></td><td><bold>86.67</bold></td><td>80.00</td><td><bold>86.67</bold></td><td>80.00</td><td>83.33</td><td>84.45</td></tr><tr><td>Asilidae (bee mimics)</td><td><bold>66.67</bold></td><td><bold>56.67</bold></td><td>33.33</td><td>26.67</td><td>56.67</td><td>43.33</td><td>47.22</td></tr><tr><td>Bombyliidae (bee mimics)</td><td><bold>96.67</bold></td><td>66.67</td><td>56.67</td><td>63.33</td><td>90.00</td><td><bold>83.33</bold></td><td>76.11</td></tr><tr><td>Syrphidae (bee mimics)</td><td><bold>100.00</bold></td><td>96.67</td><td>96.67</td><td>96.67</td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td>98.34</td></tr><tr><td><italic>Syrphidae (wasp mimic</italic><italic>s</italic><italic>)</italic></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td>100.00</td></tr><tr><td>Tachinidae (bee mimics)</td><td><bold>100.00</bold></td><td>80.00</td><td>70.00</td><td>66.67</td><td>93.33</td><td><bold>86.67</bold></td><td>82.78</td></tr><tr><td><italic>Tachinidae (wasp mimic</italic><italic>s</italic><italic>)</italic></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td>96.67</td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td>99.45</td></tr><tr><td><italic>Tachinidae (non-mimic</italic><italic>s</italic><italic>)</italic></td><td><bold>100.00</bold></td><td>96.67</td><td>93.33</td><td>86.67</td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td>96.11</td></tr><tr><td>Lepidoptera (bee mimics)</td><td><bold>90.00</bold></td><td>70.00</td><td>70.00</td><td>66.67</td><td>86.67</td><td><bold>73.33</bold></td><td>76.11</td></tr><tr><td><italic>Lepidoptera (wasp mimic</italic><italic>s</italic><italic>)</italic></td><td>96.67</td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td><bold>100.00</bold></td><td>99.45</td></tr><tr><td><italic>Lepidoptera (non-mimic</italic><italic>s</italic><italic>)</italic></td><td>66.67</td><td>56.67</td><td><bold>90.00</bold></td><td>66.67</td><td>86.67</td><td><bold>70.00</bold></td><td>72.78</td></tr><tr><td>Average</td><td><bold>91.67</bold></td><td>83.61</td><td>76.67</td><td>76.67</td><td>90.56</td><td><bold>85.83</bold></td><td>84.77</td></tr></tbody></table><table-wrap-foot><fn><p>A correct classification is when the model accurately identifies these insects as non-bumble bee. Boldface percentages represent the top classification accuracy for color and gray images among the three models for each row. The third row presents the accuracy of a dataset comprising unseen images from those species of bumble bees and non-bumble bees used in training and validation. The fourth row presents the accuracy in a dataset comprising unseen images from species of bumble bees and non-bumble bees that were not used in training and validation. In other words, these species were also unseen by the AI model. The number of testing images of seen species used for testing was 585 color and 585 corresponding grayscale images, distributed evenly among all species. The number of testing images of unseen species used for testing was 98 color and 98 corresponding grayscale images, distributed evenly among all species. More details on the species used in testing are included in <xref rid="mmc4" ref-type="supplementary-material">Tables&#x000a0;S4</xref> and <xref rid="mmc5" ref-type="supplementary-material">S5</xref>. Later rows in this table denote accuracies for the same AI models in classifying mimics in <xref rid="tbl4" ref-type="table">Table&#x000a0;4</xref>. For testing mimics, 30 color and 30 corresponding grayscale images s for each of the 12 groups were used.</p></fn></table-wrap-foot></table-wrap></p><p id="p0135">First, we can see that the ResNet-101 architecture achieved good accuracy in classifying bumble bees from non-bumble bees, with the best testing accuracies exhibited by the model trained with color and grayscale images (<inline-formula><mml:math id="M27" altimg="si24.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x0003e;</mml:mo><mml:mn>88</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula>; average for testing, row 5). Second, we observe that the ResNet-101 models classified mimics with much higher accuracy compared to the previous VGG models. Note that the grayscale image models performed better than color image models overall for the problem of classifying bumble bees from non-bumble bees. For the case of bee mimics, color models generally performed a little better in classification. Again, none of the mimic images were used for training and validation, thus the mimic species are purely unseen by the AI. Hyperparameters for this model are shown in <xref rid="tbl9" ref-type="table">Table 9</xref>.<table-wrap position="float" id="tbl9"><label>Table&#x000a0;9</label><caption><p>ResNet-101 based model hyperparameters</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Hyperparameter</th><th>Value</th></tr></thead><tbody><tr><td>Loss</td><td>Binary Cross entropy</td></tr><tr><td>Optimizer</td><td>Adam Optimizer</td></tr><tr><td>Momentum</td><td>0.9</td></tr><tr><td>Epochs</td><td>100,000</td></tr><tr><td>Learning rate (up to 50,000 epochs)</td><td>0.0003</td></tr><tr><td>Learning rate (up to 80,000 epochs)</td><td>0.00003</td></tr><tr><td>Learning rate (up to 100,000 epochs)</td><td>0.000003</td></tr></tbody></table></table-wrap></p></sec><sec id="sec2.3"><title>Evaluating model fidelity</title><p id="p0140">To evaluate the fidelity and explainability of our AI models, we employed the CAM technique (<xref rid="bib55" ref-type="bibr">Zhou et&#x000a0;al., 2016</xref>) to pinpoint which pixels in an image were most used to make a classification by the AI. The warmer (i.e., redder) a pixel is in the CAM, the higher the weight of that pixel used for classification. If the pixels highlighted in the CAM appear on anatomical components of the insect, that means that the model learned to classify correctly, while ignoring the background. From <xref rid="fig2" ref-type="fig">Figures&#x000a0;2</xref>, <xref rid="fig3" ref-type="fig">3</xref>, <xref rid="fig4" ref-type="fig">4</xref>, <xref rid="fig5" ref-type="fig">5</xref>, and <xref rid="fig6" ref-type="fig">6</xref> for all AI models&#x000a0;and image classes, we see that our models focused primarily on the anatomical components of the insect (for both color and grayscale images), and have learned well enough to ignore the background. Results are indeed generalizable to other images in our dataset and as such increase confidence in our AI models.<fig id="fig2"><label>Figure&#x000a0;2</label><caption><p>Non-bee insects</p><p>Citizen science photos (top) and class activation maps (bottom) of non-bee insects, using the bee vs. non-bee classifier (VGG16-based model, trained with color images). Species and classifications, from left: <italic>Ectobius vittiventris</italic> (correct), <italic>Hapithus agitator</italic> (correct), <italic>Uropetala carovei</italic> (correct).</p></caption><graphic xlink:href="gr2"/></fig><fig id="fig3"><label>Figure&#x000a0;3</label><caption><p>Non-bumble bee bees</p><p>Citizen science photos (top) and class activation maps (bottom) of non-bumble bee bees using the bee vs. non-bee classifier (VGG16-based model, trained with color images). Species and classifications, from left: <italic>Andrena cineraria</italic> (correct), <italic>Apis dorsata</italic> (correct), <italic>Megachile mendica</italic> (correct).</p></caption><graphic xlink:href="gr3"/></fig><fig id="fig4"><label>Figure&#x000a0;4</label><caption><p>Bumble bees</p><p>Citizen science photos (top) and class activation maps (bottom) of bumble bees using the bee vs. non-bee classifier (VGG16-based model, trained with color images). Species and classifications, from left: <italic>Bombus impatiens</italic> (correct), <italic>Bombus impatiens</italic> (correct), <italic>Bombus pensylvanicus</italic> (correct).</p></caption><graphic xlink:href="gr4"/></fig><fig id="fig5"><label>Figure&#x000a0;5</label><caption><p>Bee mimics (Asilidae)</p><p>Citizen science photos (top) and class activation maps (bottom) of bee mimics from the family Asilidae using the bee vs. non-bee classifier (VGG16-based model, trained with color images). Species and classifications, from left: <italic>Mallophora leschenaulti</italic> (incorrect), <italic>Mallophora leschenaulti</italic> (incorrect), <italic>Mallophora leschenaulti</italic> (grayscale) (correct).</p></caption><graphic xlink:href="gr5"/></fig><fig id="fig6"><label>Figure&#x000a0;6</label><caption><p>Bee mimics (Scarabaeidae)</p><p>Citizen science photos (top) and class activation maps (bottom) of bee mimics from the family Scarabaeidae using the bee vs. non-bee classifier (VGG16-based model, trained with color images). Species and classifications, from left: <italic>Trichius fasciatus</italic> (grayscale) (incorrect), <italic>Trichius gallicus</italic> (incorrect), <italic>Trichius sexualis</italic> (correct).</p></caption><graphic xlink:href="gr6"/></fig></p></sec><sec id="sec2.4"><title>Visualizing high-dimensional image data in two dimensions using t-distributed Stochastic Neighbor Embedding</title><p id="p0145">We plotted the t-SNE coordinates onto a 2D graph and color-coded them by phylogenetic and mimetic grouping (<xref rid="fig7" ref-type="fig">Figure&#x000a0;7</xref>). We observed that those data points yielded twelve individual clusters, with no outliers. Furthermore, multiple clusters within a given clade (e.g., Lepidoptera) were also grouped together.<fig id="fig7"><label>Figure&#x000a0;7</label><caption><p>t-Distributed Stochastic Neighbor Embedding</p><p>t-SNE plot of bee mimics (n&#x000a0;= 180), <italic>wasp mimics</italic> (n&#x000a0;= 90), and <italic>non-mimics</italic> (n&#x000a0;= 90) from <xref rid="tbl2" ref-type="table">Table&#x000a0;2</xref>, with the phylogenetic tree overlaid. Note the perfect clustering within each group, as well as the clustering between groups that grossly corresponds to the phylogeny. Evolutionary relationships follow (<xref rid="bib49" ref-type="bibr">Wiegmann et&#x000a0;al., 2009</xref>), (<xref rid="bib18" ref-type="bibr">Gunter et&#x000a0;al., 2016</xref>), (<xref rid="bib37" ref-type="bibr">Powell 2009</xref>), (<xref rid="bib34" ref-type="bibr">Penney et&#x000a0;al., 2012</xref>), and (<xref rid="bib3" ref-type="bibr">Blaschke et&#x000a0;al., 2018</xref>).</p></caption><graphic xlink:href="gr7"/></fig></p></sec><sec id="sec2.5"><title>Training, hardware, and inference time</title><p id="p0150">Our training and validation were conducted on a GPU cluster of four Nvidia GeForce GTX TITAN X cards each having <inline-formula><mml:math id="M28" altimg="si25.gif"><mml:mrow><mml:mn>3,072</mml:mn></mml:mrow></mml:math></inline-formula> CUDA cores and 12 GB of memory each (<xref rid="bib30" ref-type="bibr">Nvidia</xref>). It took around 28&#x000a0;h to train and validate the VGG16-based bee vs. non-bee model and took 46&#x000a0;h to train and validate the ResNet-101-based bumble bee vs. non-bumble bee model. Inference time for a single image was less than a second for both models.</p></sec></sec><sec id="sec3"><title>Discussion</title><p id="p0155">The ability to identify bees from other insects&#x02014;especially bee mimics&#x02014;has important applications for conservation, education, and AI in biology. Overall, our accuracies reach <inline-formula><mml:math id="M29" altimg="si26.gif"><mml:mrow><mml:mtext>&#x0223c;</mml:mtext><mml:mspace width="0.25em"/><mml:mn>90</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> for classifying bees from non-bees, as well as bumble bees from non-bumble bees, when grayscale images were included in the training. For the mimics studied, the bee vs. non-bee model (VGG16-based) yielded lower classification accuracies compared to those of the bumble bee vs. non-bumble bee model (ResNet-101-based) (<xref rid="tbl5" ref-type="table">Table&#x000a0;5</xref> and <xref rid="tbl8" ref-type="table">8</xref>). This difference could be owing to a number of reasons, such as the disparate neural network architectures. Note that the ResNet-101 model is much heavier with many more layers, and is better suited for learning finer-grained discriminators compared to the lighter-weight VGG models. Second, the difference could be owing to the phylogenetic specificity of both the bumble bee and non-bumble bee classes. In other words, the non-bumble bee class was trained using non-bumble bee bees, as opposed to the more general non-bee insects used to train the bee model. It should also be noted that the great majority of the bee mimics tested were bumble bee mimics, compared to others such as the several honey bee mimics (i.e., <italic>Eristalis</italic> spp.).</p><p id="p0160">One surprising finding from the bumble bee model was that unseen species had higher accuracies than seen species, for both true negatives and true positives (<xref rid="tbl8" ref-type="table">Table&#x000a0;8</xref>: rows 4 vs. 3; Supplemental Information). We hypothesize that this counterintuitive result may be due in part to selection bias. Specifically, the unseen non-bumble bee species used in testing may be more morphologically derived&#x02014;presumably, exhibiting traits that are further away from the bumble bee gestalt that the AI was trained to recognize. For example, the two unseen <italic>Apis</italic> species&#x02014;which have accuracies of 100% (<xref rid="mmc5" ref-type="supplementary-material">Supplementary Table 5</xref>)&#x02014;include the anatomically distinct black dwarf honey bee, <italic>Apis andreniformis</italic>. Among the true positives, the unseen species of <italic>Bombus</italic> may have more prominent versions of diagnostic bumble bee traits recognized by the AI, such as hair. For example, the unseen species <italic>B.&#x000a0;flavifrons</italic> has unusually long and uneven hair lengths (<xref rid="bib25" ref-type="bibr">Koch 2012</xref>).</p><p id="p0165">Results supported our first hypothesis, as the bee mimics within all four relevant clades&#x02014;Scarabaeidae, Syrphidae, Tachinidae, and Lepidoptera&#x02014;had the lowest classification accuracy compared to their wasp mimic and non-bee mimic counterparts, in both bee and bumble bee models (<xref rid="tbl5" ref-type="table">Tables&#x000a0;5</xref> and <xref rid="tbl8" ref-type="table">8</xref>). The only exception to this was that the bumble bee model yielded a lower accuracy for the Lepidoptera non-mimics compared to bee mimics (72.78% vs. 76.11%; also, the difference was negligible for Scarabaeidae).</p><p id="p0170">Our second hypothesis was not supported, as the accuracy of the wasp mimics was intermediate between bee mimics and non-mimics in only one of the four three-way comparisons across both models. Indeed, for the bumble bee model, this included identical near-perfect classification (99.45%) for the wasp mimics within Tachinidae and Lepidoptera (not to mention 100% classification of the wasp mimics in Syrphidae). Note also that this essentially perfect classification of wasp mimics in all three clades (Syrphidae, Tachinidae, Lepidoptera) was across all three training image types (color, grayscale, color&#x000a0;+ grayscale) and both testing image types (color, grayscale). Thus, while the bee mimics were better at fooling both algorithms, the phenotypic divergence of the wasp mimics may have been more easily detected (i.e., less confused for bees) compared to the non-mimics. Phylogeny may play a non-mutually exclusive role within Tachinidae as well, as the wasp mimics and non-mimics are sister groups (<xref rid="fig7" ref-type="fig">Figure&#x000a0;7</xref>), and have more similar accuracies compared to those of the bee mimics (based on both models; <xref rid="tbl5" ref-type="table">Tables&#x000a0;5</xref> and <xref rid="tbl8" ref-type="table">8</xref>).</p><p id="p0175">All of the bee and wasp mimicry herein is considered to be defensive (Batesian), enabling an aposematic warning to would-be predators by falsely appearing to be a stinging taxon. In terms of fooling the AI model (i.e., convincing the algorithm that a mimic was actually a bee), the poorest performance was exhibited by images of bee hawkmoths (<italic>Hemaris</italic>, Sphingidae, Lepidoptera) in the bee model (54.45% of images were correctly classified as a mimic), and the bee beetles (<italic>Trichius</italic>, Scarabaeidae) in the bumble bee model (84.44% of images were correctly classified as a mimic) (<xref rid="tbl5" ref-type="table">Tables&#x000a0;5</xref> and <xref rid="tbl8" ref-type="table">8</xref>).</p><p id="p0180">Conversely, the best bee mimicry, as defined by the lowest AI classification accuracy, was exhibited by Asilidae in both models (18.33%, 47.22%). This robber fly family is represented herein by three species from the genus <italic>Mallophora</italic>, known as the bee killers (<xref rid="bib5" ref-type="bibr">Brower et&#x000a0;al., 1960</xref>), (<xref rid="bib4" ref-type="bibr">Bromley 1930</xref>). These flies mimic the color, form, and hairy appearance of their bumble bee and carpenter bee prey (<xref rid="fig5" ref-type="fig">Figure&#x000a0;5</xref>), as well as exhibit bee-like flight behavior and buzz (<xref rid="bib26" ref-type="bibr">Linsley 1960</xref>), (<xref rid="bib4" ref-type="bibr">Bromley 1930</xref>). For one of our species, <italic>M.&#x000a0;bomboides</italic>, <xref rid="bib5" ref-type="bibr">Brower et&#x000a0;al., (1960)</xref> experimentally demonstrated that this mimicry is defensive (Batesian). These authors proposed that this mimicry is also aggressive, in a non-mutually exclusive manner, and operates at two possible levels: during the known predation of the adult bees, and during the presumed oviposition in bee nests (the larval feeding habits are unknown). Assuming that such aggressive mimicry is operating on one or both levels in our <italic>Mallophora</italic> (in addition to defensive mimicry), the superior bee mimicry of these bee killers confirms our third hypothesis. Ultimately, such two- or three-way selective pressure may be responsible for driving the evolution of more accurate mimicry within this genus, and perhaps other asilids.</p><p id="p0185">It is also worth noting that in addition to being non-mutually exclusive, the co-existence of defensive and aggressive mimicry could also provide a positive evolutionary feedback loop. Theoretically, asilid predation upon bees would assure sympatry (<xref rid="bib5" ref-type="bibr">Brower et&#x000a0;al., 1960</xref>), which in turn would enable defensive mimicry (i.e., mimics and their models must co-occur), which in turn could be exapted for aggressive mimicry to enhance predation.</p><p id="p0190">We also advocate for a less restrictive definition of aggressive mimicry (per the original <xref rid="bib36" ref-type="bibr">Poulton 1890</xref>: p.268) than what is sometimes followed in the literature. Namely, that mimicry should be considered aggressive if it fools any prey species (not just the model species)&#x02014;just as mimicry would be considered defensive if it fools any predator species. For adult prey, <italic>Mallophora</italic> prefers aculeate Hymenoptera (<xref rid="bib4" ref-type="bibr">Bromley 1930</xref>), and if the former&#x02019;s mimicry fatally dupes a member of the latter, it should be considered no less aggressive if that prey is a honey bee rather than a bumble bee. Such an inclusive definition would apply to the putative mimicry-enabled egg-laying as well. Fundamentally, the phenomenon of aggressive mimicry in bee mimics is understudied and not well-understood, so further investigations would be welcomed.</p><p id="p0195">Findings from the three other groups&#x02014;if they do, indeed, exhibit aggressive mimicry&#x02014;would also be consistent with our third hypothesis. Bombyliidae is the sister group to Asilidae, and represents a large clade known as the bee flies (15 subfamilies, 5K described species; (<xref rid="bib15" ref-type="bibr">Evenhuis and Greathead, 1999</xref>)). Members of Bombyliidae may theoretically include aggressive mimics, but only in the Kirbyan sense: while their adult forms do not feed upon adult bees like those of <italic>Mallophora</italic> do, <italic>Bombylius</italic> (Bombyliidae) larvae are ectoparasitoids in the nests of bees (<xref rid="bib53" ref-type="bibr">Yeates and Greathead 1997</xref>). Larvae of some Syrphidae and the bee mimic genus <italic>Tachina</italic> also parasitize bees (<xref rid="bib32" ref-type="bibr">Packard, 1868</xref>). Interestingly, these three groups exhibit accuracy values intermediate between the bee mimics in Asilidae and in Lepidoptera/Scarabaeidae, in the bee model. This pattern is not the case in the bumble bee model, given the high accuracy of the syrphids and tachinids, which may be owing to their mimicking of honey bees, rather than bumble bees as in the bombylids.</p><p id="p0200">Among the incorrect classifications&#x02014;i.e., when the mimicry successfully fooled the AI model&#x02014;within the Scarabaeidae (represented here by <italic>Trichius</italic>), the CAMs were often located on the bee-like hairy thorax, or the elytra (<xref rid="fig6" ref-type="fig">Figure&#x000a0;6</xref>, left). Elytra are the modified, hardened forewings of the beetle order Coleoptera, which is interesting in that this differentiates the order from both the bee mimics in Diptera and the bees in Hymenoptera. In <italic>Trichius</italic>, the elytra mimic the gold and black banding pattern of bee abdomens (<xref rid="fig6" ref-type="fig">Figure&#x000a0;6</xref>). We can quantify the role of elytra color (4%) vs. pattern (44%) based on the percentage of images in which the CAMs selected the elytra in the color incorrect images compared to the grayscale incorrect images: 48% (10/21) vs. 44% (7/16), respectively. Interestingly, the VGG16-based model trained only on color images performed much better on grayscale than color versions of the testing images (43.33 vs. 23.33%; <xref rid="tbl5" ref-type="table">Table&#x000a0;5</xref>).</p><p id="p0205">When comparing training with color images vs. grayscale images, we notice a trend of grayscale images giving a consistently similar performance. The exceptions to this include the Bombyliidae in <xref rid="tbl5" ref-type="table">Table&#x000a0;5</xref>, and, interestingly, the bee mimics in <xref rid="tbl8" ref-type="table">Table&#x000a0;8</xref>. The latter demonstrates that the algorithm was able to better detect these mimics as non-bumble bees on the basis of color. Conversely, for our entire testing set, the grayscale training yielded higher classification accuracies in all models, suggesting that color can possibly confound AI models. This also means that the algorithms learn from the lighter-weight grayscale images as well. This phenomenon can be explained by the fact that in some cases, when color may not be relevant to a classification problem, AI models work just as well as with grayscale images (or sometimes better)&#x000a0;(<xref rid="bib6" ref-type="bibr">&#x00108;ad&#x000ed;k 2008</xref>) (<xref rid="bib22" ref-type="bibr">Kanan and Cottrell 2012</xref>) (<xref rid="bib52" ref-type="bibr">Xie and Richmond 2018</xref>) (<xref rid="bib54" ref-type="bibr">Yohanandan et&#x000a0;al., 2018</xref>). In the case of insect morphology, pattern and shape can play a critical role in classification; for example, the very unique <inline-formula><mml:math id="M30" altimg="si27.gif"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>-shaped area of hairs on the thorax of <italic>B.&#x000a0;affinis</italic>. For detecting such markers, grayscale images may be sufficient. Also, AI models may be confused by the colors appearing in images, which is likely to happen in citizen-generated photos owing to diverse sources of background, inconsistencies in light, camera capabilities, and more. Grayscale images may overcome these issues, which aid in robust AI models for problems related to the analysis of insect morphology. Herein, it should be noted that converting one of the <italic>Mallophora</italic> images to grayscale turned an incorrect classification into a correct classification (<xref rid="fig5" ref-type="fig">Figures&#x000a0;5</xref>B and 5C).</p><p id="p0210">With respect to the t-SNE results (<xref rid="fig7" ref-type="fig">Figure&#x000a0;7</xref>), it is interesting that there was perfect clustering within each group, with no mismatched data points. Such clustering is surprising within the paraphyletic groups (Scarabaeidae non-mimics, Syrphidae wasp mimics) and especially the Lepidoptera non-mimics, which comprise five distinct families within four superfamilies. Furthermore, given the imperfect mimicry within Syrphidae (<xref rid="bib34" ref-type="bibr">Penney et&#x000a0;al., 2012</xref>), it is notable that there was such a clear separation of the bee mimic and wasp mimic clusters. This finding is inconsistent with the multi-model hypothesis (<xref rid="bib14" ref-type="bibr">Edmunds 2000</xref>), which would predict overlap owing to the imperfect mimicry of multiple models. This separation also corroborates the findings of <xref rid="bib34" ref-type="bibr">Penney et&#x000a0;al., (2012)</xref>, which found no syrphids intermediate in appearance between hymenopteran models. It is also intriguing that the pattern of clustering between these bee mimic groups&#x000a0;grossly corresponds to the evolutionary relationships, as denoted by the phylogenetic tree (<xref rid="fig7" ref-type="fig">Figure&#x000a0;7</xref>). Specifically, all within-clade clusters group together for Scarabaeidae, Lepidoptera, Syrphidae, Tachinidae, and even the Asilidae&#x000a0;+ Bombylidae clade, which represents the superfamily Asiloidea.</p><sec id="sec3.1"><title>Limitations of the study</title><p id="p0215">We would like to note that distance within the t-SNE plot cannot be used as an exact proxy for phenotypic disparity, owing to the probabilistic nature of the approach (<xref rid="bib47" ref-type="bibr">Wattenberg et&#x000a0;al., 2016</xref>), (<xref rid="bib44" ref-type="bibr">Van der Maaten and Hinton 2008</xref>). Phenotypic disparity is also an imperfect proxy for phylogenetic distance, especially in the case of convergent morphotypes. Lastly, one limitation of our bumble bee model is that all of the training images were of bees (bumble bees and non-bumble bee bees), with no training images of non-bees.</p></sec><sec id="sec3.2"><title>Conclusion</title><p id="p0220">By applying machine learning techniques within an evolutionary framework&#x02014;from our integration of t-SNE and phylogeny, to grayscale images and CAMs&#x02014;we can take the first step from &#x0201c;explainable AI&#x0201d; to &#x0201c;explainable mimicry.&#x0201d; This includes decoupling the role of color from pattern and elucidating diagnostic anatomical regions. We need to acknowledge, however, that the features learned by our deep learning models operating on images may have little to no functional significance in the evolutionary history of these insects. Nonetheless, these methods provide new avenues for addressing the long-standing challenge of &#x0201c;quantifying the extent of mimetic fidelity between mimics and models&#x0201d; (<xref rid="bib34" ref-type="bibr">Penney et&#x000a0;al., 2012</xref>). Furthermore, real-time deployment of the approaches herein&#x02014;such as providing immediate feedback and visualizations&#x02014;could enhance the operational efficiency of citizen scientist-driven identification of bees and their mimics, as well as help pique interest and engagement among the general public. For example, alerting users of critically endangered species such as <italic>B.&#x000a0;affinis</italic> could have a direct impact on sustaining efforts for conservation. Ultimately, with ever-expanding datasets of crowdsourced images, coupled with expected advances in computer vision techniques, foundational insights and applications are possible in the near future and beyond.</p></sec></sec><sec id="sec5"><title>STAR&#x02605;Methods</title><sec id="sec5.1"><title>Key resources table</title><p id="p0230">
<table-wrap position="float" id="undtbl1"><table frame="hsides" rules="groups"><thead><tr><th>REAGENT or RESOURCE</th><th>SOURCE</th><th>IDENTIFIER</th></tr></thead><tbody><tr><td colspan="3"><bold>Deposited data</bold></td></tr><tr><td colspan="3"><hr/></td></tr><tr><td>Training, validation and testing data</td><td>(<xref rid="bib1" ref-type="bibr">Bhuiyan, 2022</xref>)&#x0201c;Bee-Non-bee-Dataset&#x0201d;, Mendeley Data: <ext-link ext-link-type="doi" xlink:href="10.17632/jykt6862s4.1" id="intref0010">https://doi.org/10.17632/jykt6862s4.1</ext-link>.</td><td><ext-link ext-link-type="uri" xlink:href="https://data.mendeley.com/datasets/jykt6862s4/1" id="intref0015">https://data.mendeley.com/datasets/jykt6862s4/1</ext-link></td></tr><tr><td colspan="3"><hr/></td></tr><tr><td colspan="3"><bold>Software and algorithms</bold></td></tr><tr><td colspan="3"><hr/></td></tr><tr><td>Python</td><td>vanRossum, G., 1995. Python reference manual.&#x000a0;<italic>Department of Computer Science [CS]</italic>, (R 9525).</td><td><ext-link ext-link-type="uri" xlink:href="https://www.python.org/" id="intref0020">https://www.python.org/</ext-link></td></tr><tr><td>Tensorflow</td><td>Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M. and Kudlur, M., 2016. {TensorFlow}: a system for {Large-Scale} machine learning. In&#x000a0;<italic>12th USENIX symposium on operating systems design and implementation (OSDI 16)</italic>&#x000a0;(pp. 265&#x02013;283).</td><td><ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/" id="intref0025">https://www.tensorflow.org/</ext-link></td></tr><tr><td>OpenCV</td><td>OpenCV team</td><td><ext-link ext-link-type="uri" xlink:href="https://opencv.org/" id="intref0030">https://opencv.org/</ext-link></td></tr><tr><td>Matplotlib</td><td>J. Hunter, &#x0201c;Matplotlib: A 2D Graphics Environment&#x0201d; in <italic>Computing in Science &#x00026; Engineering</italic>, vol. 9, no. 03, pp. 90&#x02013;95, 2007. <ext-link ext-link-type="doi" xlink:href="10.1109/MCSE.2007.55" id="intref0035">https://doi.org/10.1109/MCSE.2007.55</ext-link></td><td><ext-link ext-link-type="uri" xlink:href="https://matplotlib.org/" id="intref0040">https://matplotlib.org/</ext-link></td></tr><tr><td>NumPy</td><td>Harris, C.R., Millman, K.J., van der Walt, S.J. et&#x000a0;al. Array programming with NumPy. Nature 585, 357&#x02013;362 (2020). <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41586-020-2649-2" id="intref0045">https://doi.org/10.1038/s41586-020-2649-2</ext-link></td><td><ext-link ext-link-type="uri" xlink:href="https://numpy.org/" id="intref0050">https://numpy.org/</ext-link></td></tr><tr><td>Code for this paper</td><td>(<xref rid="bib2" ref-type="bibr">Bhuiyan et al., 2022</xref>) Bee Classifier, Zenodo: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6965250" id="interref0010">https://doi.org/10.5281/zenodo.6965250</ext-link></td><td><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6965250" id="interref0015">https://doi.org/10.5281/zenodo.6965250</ext-link></td></tr></tbody></table></table-wrap>
</p></sec><sec id="sec5.2"><title>Resource availability</title><sec id="sec5.2.1"><title>Lead contact</title><p id="p0235">Requests for resources or information should be directed to the lead contact, Tanvir Bhuiyan (<ext-link ext-link-type="uri" xlink:href="mailto:bhuiyan@usf.edu" id="intref0060">bhuiyan@usf.edu</ext-link>).</p></sec><sec id="sec5.2.2"><title>Material availability</title><p id="p0240">No reagents were generated in the study.</p></sec></sec><sec id="sec5.3"><title>Method details</title><p id="p0245">Herein we designed two separate convolutional neural network (CNN) architectures for two problems: 1) classifying bees vs. other insects (including mimics) and 2) classifying bumble bees vs. non-bumble bees.</p><sec id="sec5.3.1"><title>Data pre-processing</title><p id="p0250">Our images were sourced exclusively from Research Grade observations on the citizen science platform iNaturalist. Images there are typically large in size, with a maximum of up to <inline-formula><mml:math id="M31" altimg="si28.gif"><mml:mrow><mml:mn>2,048</mml:mn></mml:mrow></mml:math></inline-formula> pixels in the longest dimension. Processing images of these sizes can be very complex and time-consuming. To speed up learning without compromising accuracy, we reduced the size of each image to <inline-formula><mml:math id="M32" altimg="si29.gif"><mml:mrow><mml:mn>1,024</mml:mn></mml:mrow></mml:math></inline-formula> pixels in the longest dimension. To evaluate the effect of color on classification accuracy, we trained our models on color images, grayscale images, and combination of color and grayscale images. The first dataset retained the original colors of the images from iNaturalist. For the grayscale versions, we converted all the images into grayscale using OpenCV (<xref rid="bib10" ref-type="bibr">Culjak et&#x000a0;al., 2012</xref>). In the third dataset, we combined all the color and grayscale versions of the images, thereby doubling the training dataset. We did the same for testing dataset&#x000a0;also. The procedure was executed for both problems. Note here that we did not do data augmentation of images in our dataset. There is a reason for this. Typically, data augmentation helps where the context of classification between two classes is very vivid and clear (for example, classifying a cat from a dog). In such cases, noise addition will not hurt the larger context of images used for classification. In our problem, the dataset was insects. And these insects have very fine-grained and minute signatures for classification. As such, adding artificial noise could change the overall context of images. Hence, we did not attempt data augmentation for images in this paper.</p></sec><sec id="sec5.3.2"><title>VGG16-based CNN for classifying bees from other insects</title><p id="p0255">Our VGG16-based model was pre-trained on the &#x0201c;ImageNet&#x0201d; dataset (<xref rid="bib12" ref-type="bibr">Deng et&#x000a0;al., 2009</xref>). Note that the VGG16 architecture consists of five blocks of convolutional layers, followed by three fully connected layers (<xref rid="bib41" ref-type="bibr">Simonyan and Zisserman 2015</xref>). It is an architecture that is simpler in size and complexity than most other standard CNN architectures. For our problem, after the five blocks of convolutional layers, we added one global pooling layer and four fully connected dense layers. Details of our architecture are shown in <xref rid="tbl6" ref-type="table">Table&#x000a0;6</xref>, where the first row represents the last layer of the base VGG16 architecture, up to which was fixed. For training, we did not change the pre-existing weights in the base architecture (i.e., we froze them), and we only trained those weights added after the fifth block for the first 50 epochs. Here, the weights for only the newly added layers were trained, with a rate of 0.005. Then, weights in the entire architecture were unfrozen and retrained again for 200 epochs, with a rate of 0.0005. <xref rid="tbl7" ref-type="table">Table&#x000a0;7</xref> presents the critical hyperparameters in our architecture during training and validation. The loss function is the binary cross entropy loss function, which is given by <inline-formula><mml:math id="M33" altimg="si30.gif"><mml:mrow><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x02212;</mml:mo><mml:mi mathvariant="italic">log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>p</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M34" altimg="si31.gif"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula> is the model estimated probability of the ground truth class, which we want to minimize during training and validation. We did monitor, but did not explicitly record decrease in the loss function; we stopped training when the loss function (and accuracy) saturated.</p></sec><sec id="sec5.3.3"><title>ResNet-101-based CNN for classifying bumble bees from other bees</title><p id="p0260">Classifying bumble bees from non-bumble bees is a more complex problem, since there are subtler difference between these two classes as compared to classifying bees from other insects. For this problem, the CNN architecture that worked best in our study was the more complex ResNet-101. Here also, our ResNet-101 model was not trained from scratch. We took the pre-trained version trained on &#x0201c;ImageNet&#x0201d; dataset for this model, with its base weights. ResNet-101 is a CNN with residual connections, wherein each layer, instead of feeding only into the next layer, also directly feeds into layers further down (<xref rid="bib19" ref-type="bibr">He et&#x000a0;al., 2016</xref>). This was done to specifically improve learning at later layers. Thus, this is a more complex architecture with 101 blocks of convolutional layers. This architecture provided optimal results in its current form; therefore, we did not change the architecture, but changed only the weights via training and validation. <xref rid="tbl9" ref-type="table">Table&#x000a0;9</xref> presents the critical hyperparameters in this architecture for training and validation. The loss function is once again the binary cross entropy loss function. We did monitor, but did not explicitly record decrease in the loss function; we stopped training when the loss function (and accuracy) saturated.</p></sec><sec id="sec5.3.4"><title>CAM</title><p id="p0265">To get a better understanding how our models interpret pixels in an image for classification, we adopted the technique of class activation maps (CAM) (<xref rid="bib55" ref-type="bibr">Zhou et&#x000a0;al., 2016</xref>). The CAM technique gives each pixel a weight which indicates the significance of that pixel toward classification. To execute the technique, we compute the output features generated at the final convolution layer of the CNN. Then, we traverse back in the architecture (at the conclusion of the last convolutional layer) to determine the weight (probability) of each pixel in the image that was used for classification. A higher weight for a pixel indicates a redder color in CAM, meaning that the particular pixel was a more significant factor in classification. Pixels with lower weight would appear comparatively bluer in the CAM technique, and these are pixels that were not dominant in classification. The CAM model was based on our VGG16 model that was trained on color images.</p></sec><sec id="sec5.3.5"><title>t-SNE</title><p id="p0270">t-Distributed Stochastic Neighbor Embedding (t-SNE) (<xref rid="bib44" ref-type="bibr">Van der Maaten and Hinton 2008</xref>) is an algorithm developed as an improvement over Stochastic Neighbor Embedding (<xref rid="bib20" ref-type="bibr">Hinton and Roweis 2002</xref>). t-SNE is&#x000a0;an unsupervised, non-linear technique for dimensionality reduction, primarily used for visualising high-dimensional data (in our case, RGB values from image pixels). In other words, t-SNE gives an intuition of how high-dimensional data points are related in low-dimensional space. Compared to many other non-parametric visualisation techniques (e.g., Sammon mapping, principal components analysis, isomap, locally linear embedding), t-SNE proved more robust and significantly more effective for high-dimensional data visualisation (<xref rid="bib8" ref-type="bibr">Chatzimparmpas et&#x000a0;al., 2020</xref>). t-SNE can be used for data-visualisation in a wide range of applications including biomedical signal processing, genomics, computer security research, bioinformatics, cancer research, and music analysis (<xref rid="bib8" ref-type="bibr">Chatzimparmpas et&#x000a0;al., 2020</xref>).</p><p id="p0275">To generate the t-SNE, we started with a base neural network architecture, from which the t-SNE algorithm runs a combination of two sequential phases. In the first phase, t-SNE builds up a probability distribution matrix for data points, which consists of the RGB values from the image pixels. Each pair of distinct data points are considered. For each pair, a probability value is generated. If there is a high level of similarity between the two objects in that pair, a large probability value is assigned, otherwise the probability value is small. In the second phase, t-SNE considers those data points in a lower dimensional space and generates another probability distribution following the similar procedure it did in the first phase. The algorithm then tries to minimize the loss or difference between the two probability distributions with respect to the locations on the map. To accomplish that, the algorithm calculates the Kullback-Leibler divergence (KL divergence) (<xref rid="bib35" ref-type="bibr">Perez-Cruz 2008</xref>) value and minimises it over several iterations.</p><p id="p0280">To visualize phenotype vis-a-vis phylogeny, we also plotted the data points from images of bee mimics and outgroups using the t-SNE algorithm on a 2D graph, and overlaid the phylogenetic tree that illustrates the evolutionary relationships. To build that graph, at first we trained a twelve-class VGG16-based classifier deep-learning model (similar architecture to that in <xref rid="tbl6" ref-type="table">Table&#x000a0;6</xref>) with 360 bee mimic images. We then extracted features from the final convolution layer for all 360 images. These features are a matrix of size <inline-formula><mml:math id="M35" altimg="si32.gif"><mml:mrow><mml:mn>14</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x000d7;</mml:mo><mml:mn>14</mml:mn><mml:mo linebreak="goodbreak" linebreakstyle="after">&#x000d7;</mml:mo><mml:mn>512</mml:mn></mml:mrow></mml:math></inline-formula>. We flattened the data to a 100,352 sized array for each image. Then we ran the t-SNE algorithm (per steps above) over the 360 flattened feature data, resulting in 2D coordinates for each image which were then color-coded by phylogenetic and mimetic group.</p></sec></sec><sec id="sec5.4"><title>Quantification and statistical analysis</title><p id="p0285">The number of images used in our study from iNaturalist was <inline-formula><mml:math id="M36" altimg="si20.gif"><mml:mrow><mml:mn>6,332</mml:mn></mml:mrow></mml:math></inline-formula> color images. These were also then converted to grayscale. Together, these images encompassed our dataset for training, validation and testing. We designed AI models for color only, grayscale only, and a combination of color and grayscale images. The percent of images used for training and validation was together was 80, while the remaining <inline-formula><mml:math id="M37" altimg="si23.gif"><mml:mrow><mml:mn>20</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math></inline-formula> of images was used for testing. Note that images used for testing were completely unseen by the AI models during training and validation.</p><p id="p0290">The overall metric used to assess classification is denoted as <italic>Accuracy</italic>, which is computed based on True Positives, True Negatives, False Positives and False Negatives, and is defined in <xref rid="fd1" ref-type="disp-formula">Equation&#x000a0;1</xref>. In addition, model fidelity was evaluated using Class Activation Maps, that highlight pixels with different colors based on the priority of those pixels for classification. Here, the warmer (i.e., more reddish) a pixel appears in the CAM, the higher is the priority of that pixel for classification. Ideally, we will see more red pixels in areas surrounding the insect anatomy if the model has high fidelity. We employed the t-Distributed Stochastic Neighbor Embedding (t-SNE) technique to study mimics. t-SNE is an unsupervised, non-linear technique for dimensionality reduction, primarily used for visualising high-dimensional data. We employed this method in our study to understand clustering of various mimic groups. Our study yielded perfect clustering within each group, as well as the clustering between groups that grossly corresponds to the phylogeny.</p></sec></sec></body><back><ref-list id="cebib0010"><title>References</title><ref id="bib1"><element-citation publication-type="book" id="sref1"><person-group person-group-type="author"><name><surname>Bhuiyan</surname><given-names>T.</given-names></name></person-group><part-title>Bee and non-bee dataset</part-title><year>2022</year><publisher-name>Mendeley Data</publisher-name><pub-id pub-id-type="doi">10.17632/jykt6862s4.1</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="book" id="sref2"><person-group person-group-type="author"><name><surname>Bhuiyan</surname><given-names>T.H.</given-names></name><name><surname>Carney</surname><given-names>R.M.</given-names></name><name><surname>Sriram</surname><given-names>C.</given-names></name></person-group><part-title>Bee classifier</part-title><year>2022</year><publisher-name>Zenodo</publisher-name><pub-id pub-id-type="doi">10.5281/zenodo.6965250</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal" id="sref3"><person-group person-group-type="author"><name><surname>Blaschke</surname><given-names>J.D.</given-names></name><name><surname>Stireman</surname><given-names>J.O.</given-names></name><name><surname>O&#x02019;hara</surname><given-names>J.E.</given-names></name><name><surname>Cerretti</surname><given-names>P.</given-names></name><name><surname>Moulton</surname><given-names>J.K.</given-names></name></person-group><article-title>Molecular phylogenetics and piercer evolution in the bug-killing flies (Diptera: Tachinidae: Phasiinae)</article-title><source>Syst. Entomol.</source><volume>43</volume><year>2018</year><fpage>218</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1111/syen.12272</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal" id="sref4"><person-group person-group-type="author"><name><surname>Bromley</surname><given-names>S.W.</given-names></name></person-group><article-title>Bee-killing robber flies</article-title><source>J.&#x000a0;N. Y. Entomol. Soc.</source><year>1930</year><fpage>159</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.2307/25004364</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal" id="sref5"><person-group person-group-type="author"><name><surname>Brower</surname><given-names>L.P.</given-names></name><name><surname>Westcott</surname><given-names>P.W.</given-names></name><name><surname>Westcott</surname><given-names>P.W.</given-names></name></person-group><article-title>Experimental studies of mimicry. 5. The reactions of toads (<italic>Bufo terrestris</italic>) to bumblebees (<italic>Bombus americanorum</italic>) and their robberfly mimics (<italic>Mallophora bomboides</italic>), with a discussion of aggressive mimicry</article-title><source>Am. Nat.</source><volume>94</volume><year>1960</year><fpage>343</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1086/282137</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book" id="sref6"><person-group person-group-type="author"><name><surname>&#x00108;ad&#x000ed;k</surname><given-names>M.</given-names></name></person-group><part-title>Perceptual evaluation of color-to-grayscale image conversions</part-title><source>Computer Graphics Forum</source><volume>27</volume><year>2008</year><publisher-name>Wiley Online Library</publisher-name><fpage>1745</fpage><lpage>1754</lpage><pub-id pub-id-type="doi">10.1111/j.1467-8659.2008.01319.x</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal" id="sref7"><person-group person-group-type="author"><name><surname>Carney</surname><given-names>R.</given-names></name><name><surname>Mapes</surname><given-names>C.</given-names></name><name><surname>Low</surname><given-names>R.</given-names></name><name><surname>Long</surname><given-names>A.</given-names></name><name><surname>Bowser</surname><given-names>A.</given-names></name><name><surname>Durieux</surname><given-names>D.</given-names></name><name><surname>Rivera</surname><given-names>K.</given-names></name><name><surname>Dekramanjian</surname><given-names>B.</given-names></name><name><surname>Bartumeus</surname><given-names>F.</given-names></name><name><surname>Guerrero</surname><given-names>D.</given-names></name><etal/></person-group><article-title>Integrating global citizen science platforms to enable next-generation surveillance of invasive and vector mosquitoes</article-title><source>Insects</source><volume>13</volume><year>2022</year><fpage>675</fpage><pub-id pub-id-type="doi">10.3390/insects13080675</pub-id><comment>ISSN 2075-4450</comment><pub-id pub-id-type="pmid">36005301</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal" id="sref8"><person-group person-group-type="author"><name><surname>Chatzimparmpas</surname><given-names>A.</given-names></name><name><surname>Martins</surname><given-names>R.M.</given-names></name><name><surname>Kerren</surname><given-names>A.</given-names></name></person-group><article-title>t-viSNE: Interactive assessment and interpretation of t-sne projections</article-title><source>IEEE Trans. Vis. Comput. Graph.</source><volume>26</volume><year>2020</year><fpage>2696</fpage><lpage>2714</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2020.2986996</pub-id><pub-id pub-id-type="pmid">32305922</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book" id="sref9"><person-group person-group-type="author"><name><surname>Crane</surname><given-names>E.</given-names></name></person-group><part-title>The World History of Beekeeping and Honey Hunting</part-title><year>1999</year><publisher-name>Routledge</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="book" id="sref10"><person-group person-group-type="author"><name><surname>Culjak</surname><given-names>I.</given-names></name><name><surname>Abram</surname><given-names>D.</given-names></name><name><surname>Pribanic</surname><given-names>T.</given-names></name><name><surname>Dzapo</surname><given-names>H.</given-names></name><name><surname>Cifrek</surname><given-names>M.</given-names></name></person-group><part-title>A brief introduction to OpenCV</part-title><year>2012</year><publisher-name>IEEE</publisher-name><fpage>1725</fpage><lpage>1730</lpage></element-citation></ref><ref id="bib11"><element-citation publication-type="book" id="sref11"><person-group person-group-type="author"><name><surname>Darwin</surname><given-names>C.</given-names></name></person-group><part-title>On the Origin of Species, 1859</part-title><year>2003</year><publisher-name>Routledge</publisher-name><pub-id pub-id-type="doi">10.4324/9780203509104</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book" id="sref12"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J.</given-names></name><name><surname>Dong</surname><given-names>W.</given-names></name><name><surname>Socher</surname><given-names>R.</given-names></name><name><surname>Li</surname><given-names>L.-J.</given-names></name><name><surname>Li</surname><given-names>K.</given-names></name><name><surname>Fei-Fei</surname><given-names>L.</given-names></name></person-group><part-title>Imagenet: a large-scale hierarchical image database</part-title><source>2009 IEEE conference on computer vision and pattern recognition</source><year>2009</year><publisher-name>IEEE</publisher-name><fpage>248</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal" id="sref13"><person-group person-group-type="author"><name><surname>De Nart</surname><given-names>D.</given-names></name><name><surname>Costa</surname><given-names>C.</given-names></name><name><surname>Di Prisco</surname><given-names>G.</given-names></name><name><surname>Carpana</surname><given-names>E.</given-names></name></person-group><article-title>Image recognition using convolutional&#x000a0;neural networks for classification of honey bee subspecies</article-title><source>Apidologie</source><volume>53</volume><year>2022</year><fpage>5</fpage><pub-id pub-id-type="doi">10.1007/s13592-022-00918-5</pub-id><comment>ISSN 1297-9678</comment></element-citation></ref><ref id="bib14"><element-citation publication-type="journal" id="sref14"><person-group person-group-type="author"><name><surname>Edmunds</surname><given-names>M.</given-names></name></person-group><article-title>Why are there good and poor mimics?</article-title><source>Biol. J. Linn. Soc. Lond.</source><volume>70</volume><year>2000</year><fpage>459</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1111/j.1095-8312.2000.tb01234.x</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book" id="sref15"><person-group person-group-type="author"><name><surname>Evenhuis</surname><given-names>N.L.</given-names></name><name><surname>Greathead</surname><given-names>D.J.</given-names></name></person-group><part-title>World Catalog of Bee Flies (Diptera: Bombyliidae)</part-title><year>1999</year><publisher-name>Backhuys Publishers</publisher-name></element-citation></ref><ref id="bib16"><element-citation publication-type="journal" id="sref16"><person-group person-group-type="author"><name><surname>Ezray</surname><given-names>B.D.</given-names></name><name><surname>Wham</surname><given-names>D.C.</given-names></name><name><surname>Hill</surname><given-names>C.E.</given-names></name><name><surname>Hines</surname><given-names>H.M.</given-names></name></person-group><article-title>Unsupervised machine learning reveals mimicry complexes in bumblebees occur along a perceptual continuum</article-title><source>Proc. Biol. Sci.</source><volume>286</volume><year>2019</year><object-id pub-id-type="publisher-id">20191501</object-id><pub-id pub-id-type="doi">10.1098/rspb.2019.1501</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal" id="sref17"><person-group person-group-type="author"><name><surname>Genise</surname><given-names>J.F.</given-names></name><name><surname>Bellosi</surname><given-names>E.S.</given-names></name><name><surname>Sarzetti</surname><given-names>L.C.</given-names></name><name><surname>Krause</surname><given-names>J.M.</given-names></name><name><surname>Dinghi</surname><given-names>P.A.</given-names></name><name><surname>S&#x000e1;nchez</surname><given-names>M.V.</given-names></name><name><surname>Umazano</surname><given-names>A.M.</given-names></name><name><surname>Puerta</surname><given-names>P.</given-names></name><name><surname>Cantil</surname><given-names>L.F.</given-names></name><name><surname>Jicha</surname><given-names>B.R.</given-names></name></person-group><article-title>Pablo Puerta, Liliana F Cantil, and Brian R Jicha. 100 ma sweat bee nests: early and rapid co-diversification of crown bees and flowering plants</article-title><source>PLoS One</source><volume>15</volume><year>2020</year><object-id pub-id-type="publisher-id">e0227789</object-id><pub-id pub-id-type="doi">10.1371/journal.pone.0227789</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal" id="sref18"><person-group person-group-type="author"><name><surname>Gunter</surname><given-names>N.L.</given-names></name><name><surname>Weir</surname><given-names>T.A.</given-names></name><name><surname>Slipinksi</surname><given-names>A.</given-names></name><name><surname>Bocak</surname><given-names>L.</given-names></name><name><surname>Cameron</surname><given-names>S.L.</given-names></name></person-group><article-title>If dung beetles (scarabaeidae: Scarabaeinae) arose in association with dinosaurs, did they also suffer a mass co-extinction at the K-Pg boundary?</article-title><source>PLoS One</source><volume>11</volume><year>2016</year><object-id pub-id-type="publisher-id">e0153570</object-id><pub-id pub-id-type="doi">10.1371/journal.pone.0153570</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book" id="sref19"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><part-title>Deep&#x000a0;residual learning for image recognition</part-title><source>Proceedings of the IEEE conference on computer&#x000a0;vision and pattern recognition</source><year>2016</year><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.48550/arXiv.1512.03385</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="book" id="sref20"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G.E.</given-names></name><name><surname>Roweis</surname><given-names>S.</given-names></name></person-group><part-title>Stochastic neighbor embedding</part-title><source>Advances in Neural Information Processing Systems</source><volume>15</volume><year>2002</year></element-citation></ref><ref id="bib29"><element-citation publication-type="other" id="sref29"><person-group person-group-type="author"><name><surname>iNaturalist</surname></name></person-group><article-title>Research Grade</article-title><ext-link ext-link-type="uri" xlink:href="https://www.inaturalist.org/posts/39072-research-grade" id="intref0075">https://www.inaturalist.org/posts/39072-research-grade</ext-link><year>2020</year></element-citation></ref><ref id="bib28"><mixed-citation publication-type="other" id="sref28">iNaturalist, n.d. <ext-link ext-link-type="uri" xlink:href="https://www.inaturalist.org/" id="interref0020">https://www.inaturalist.org/</ext-link>.</mixed-citation></ref><ref id="bib21"><element-citation publication-type="journal" id="sref21"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>Y.</given-names></name><name><surname>Lee</surname><given-names>Y.-H.</given-names></name><name><surname>Ansari</surname><given-names>I.</given-names></name><name><surname>Lee</surname><given-names>C.-H.</given-names></name></person-group><article-title>Real time hornet classification system based on deep learning</article-title><source>J.&#x000a0;IKEEE</source><volume>24</volume><year>2020</year><fpage>1141</fpage><lpage>1147</lpage><pub-id pub-id-type="doi">10.7471/ikeee.2020.24.4.1141</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal" id="sref22"><person-group person-group-type="author"><name><surname>Kanan</surname><given-names>C.</given-names></name><name><surname>Cottrell</surname><given-names>G.W.</given-names></name></person-group><article-title>Cottrell. Color-to-grayscale: does the method matter in image recognition?</article-title><source>PLoS One</source><volume>7</volume><year>2012</year><object-id pub-id-type="publisher-id">e29740&#x02013;7</object-id><pub-id pub-id-type="doi">10.1371/journal.pone.0029740</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal" id="sref23"><person-group person-group-type="author"><name><surname>Kasinathan</surname><given-names>T.</given-names></name><name><surname>Singaraju</surname><given-names>D.</given-names></name><name><surname>Uyyala</surname><given-names>S.R.</given-names></name></person-group><article-title>Insect classification and detection in field crops using modern machine learning techniques</article-title><source>Inf. Process. Agric.</source><volume>8</volume><year>2021</year><fpage>446</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1016/j.inpa.2020.09.006</pub-id><comment>ISSN 2214-3173</comment></element-citation></ref><ref id="bib24"><element-citation publication-type="book" id="sref24"><person-group person-group-type="author"><name><surname>Kirby</surname><given-names>W.</given-names></name></person-group><part-title>An Introduction to Entomology</part-title><year>1857</year></element-citation></ref><ref id="bib25"><element-citation publication-type="book" id="sref25"><person-group person-group-type="author"><name><surname>Koch</surname><given-names>J.</given-names></name></person-group><part-title>Bumble Bees of the Western United States</part-title><year>2012</year><publisher-name>US Department of Agriculture, Forest Service</publisher-name></element-citation></ref><ref id="bib26"><element-citation publication-type="journal" id="sref26"><person-group person-group-type="author"><name><surname>Linsley</surname><given-names>E.G.</given-names></name></person-group><article-title>Ethology of some bee-and wasp-killing robber flies of southeastern Arizona and western New Mexico (Diptera: Asilidae)</article-title><source>Univ. Calif. Publ. Entomol.</source><volume>16</volume><year>1960</year><fpage>357</fpage></element-citation></ref><ref id="bib27"><element-citation publication-type="book" id="sref27"><person-group person-group-type="author"><name><surname>Minakshi</surname><given-names>M.</given-names></name><name><surname>Bharti</surname><given-names>P.</given-names></name><name><surname>McClinton</surname><given-names>W.B.</given-names><suffix>III</suffix></name><name><surname>Mirzakhalov</surname><given-names>J.</given-names></name><name><surname>Carney</surname><given-names>R.M.</given-names></name><name><surname>Sriram</surname><given-names>C.</given-names></name></person-group><part-title>Automating the surveillance of mosquito vectors from trapped specimens using computer vision techniques</part-title><source>Proceedings of the 3rd ACM SIGCAS Conference on Computing and Sustainable Societies</source><year>2020</year><fpage>105</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1145/3378393.3402260</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="other" id="sref30"><person-group person-group-type="author"><name><surname>Nvidia</surname></name></person-group><article-title>Nvidia GeForce Titan x</article-title><ext-link ext-link-type="uri" xlink:href="https://www.nvidia.com/en-us/geforce/graphics-cards/geforce-gtx-titan-x" id="intref0080">https://www.nvidia.com/en-us/geforce/graphics-cards/geforce-gtx-titan-x</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="book" id="sref31"><person-group person-group-type="author"><name><surname>Osten-Sacken</surname><given-names>R.C.</given-names></name></person-group><part-title>On the Oxen-Born Bees of the Ancients (Bugonia) and Their Relation to <italic>Eristalis tenax</italic>, a Two-Winged Insect</part-title><year>1894</year><publisher-name>Kessinger Publishing</publisher-name></element-citation></ref><ref id="bib32"><element-citation publication-type="journal" id="sref32"><person-group person-group-type="author"><name><surname>Packard</surname><given-names>A.S.</given-names><suffix>Jr.</suffix></name></person-group><article-title>The parasites of the honey-bee</article-title><source>Am. Nat.</source><volume>2</volume><year>1868</year><fpage>195</fpage><lpage>205</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal" id="sref33"><person-group person-group-type="author"><name><surname>Pasteur</surname><given-names>G.</given-names></name></person-group><article-title>A classificatory review of mimicry systems</article-title><source>Annu. Rev. Ecol. Syst.</source><volume>13</volume><year>1982</year><fpage>169</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1146/annurev.es.13.110182.001125</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal" id="sref34"><person-group person-group-type="author"><name><surname>Penney</surname><given-names>H.D.</given-names></name><name><surname>Hassall</surname><given-names>C.</given-names></name><name><surname>Skevington</surname><given-names>J.H.</given-names></name><name><surname>Abbott</surname><given-names>K.R.</given-names></name><name><surname>Sherratt</surname><given-names>T.N.</given-names></name></person-group><article-title>A comparative analysis of the evolution of imperfect mimicry</article-title><source>Nature</source><volume>483</volume><year>2012</year><fpage>461</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1038/nature10961</pub-id><pub-id pub-id-type="pmid">22437614</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book" id="sref35"><person-group person-group-type="author"><name><surname>Perez-Cruz</surname><given-names>F.</given-names></name></person-group><part-title>Kullback-Leibler Divergence Estimation of Continuous Distributions</part-title><year>2008</year><publisher-name>IEEE</publisher-name><fpage>1666</fpage><lpage>1670</lpage><pub-id pub-id-type="doi">10.1109/ISIT.2008.4595271</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book" id="sref36"><person-group person-group-type="author"><name><surname>Poulton</surname><given-names>E.B.</given-names></name></person-group><part-title>The Colours of Animals: Their Meaning and Use, Especially Considered in the Case of Insects</part-title><year>1890</year><publisher-name>D. Appleton</publisher-name><pub-id pub-id-type="doi">10.1126/science.ns-16.407.286.a</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book" id="sref37"><person-group person-group-type="author"><name><surname>Powell</surname><given-names>J.A.</given-names></name></person-group><part-title>Lepidoptera: moths, butterflies</part-title><source>Encyclopedia of insects</source><year>2009</year><publisher-name>Elsevier</publisher-name><fpage>559</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-374144-8.00160-0</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal" id="sref38"><person-group person-group-type="author"><name><surname>Potts</surname><given-names>S.G.</given-names></name><name><surname>Biesmeijer</surname><given-names>J.C.</given-names></name><name><surname>Kremen</surname><given-names>C.</given-names></name><name><surname>Neumann</surname><given-names>P.</given-names></name><name><surname>Schweiger</surname><given-names>O.</given-names></name><name><surname>Kunin</surname><given-names>W.E.</given-names></name></person-group><article-title>Global pollinator declines: trends, impacts&#x000a0;and drivers</article-title><source>Trends Ecol. Evol.</source><volume>25</volume><year>2010</year><fpage>345</fpage><lpage>353</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2010.01.007</pub-id><pub-id pub-id-type="pmid">20188434</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal" id="sref39"><person-group person-group-type="author"><name><surname>Roffet-Salque</surname><given-names>M.</given-names></name><name><surname>Regert</surname><given-names>M.</given-names></name><name><surname>Evershed</surname><given-names>R.P.</given-names></name><name><surname>Outram</surname><given-names>A.K.</given-names></name><name><surname>Cramp</surname><given-names>L.J.E.</given-names></name><name><surname>Decavallas</surname><given-names>O.</given-names></name><name><surname>Dunne</surname><given-names>J.</given-names></name><name><surname>Gerbault</surname><given-names>P.</given-names></name><name><surname>Mileto</surname><given-names>S.</given-names></name><name><surname>Mirabaud</surname><given-names>S.</given-names></name><etal/></person-group><article-title>Widespread exploitation of the honeybee by early neolithic farmers</article-title><source>Nature</source><volume>527</volume><year>2015</year><fpage>226</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1038/nature15757</pub-id><pub-id pub-id-type="pmid">26560301</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book" id="sref40"><person-group person-group-type="author"><name><surname>Scholl</surname><given-names>P.J.</given-names></name><name><surname>Colwell</surname><given-names>D.D.</given-names></name><name><surname>Cepeda-Palacios</surname><given-names>R.</given-names></name></person-group><part-title>Myiasis (Muscoidea, Oestroidea)</part-title><source>Medical and veterinary entomology</source><year>2019</year><publisher-name>Elsevier</publisher-name><fpage>383</fpage><lpage>419</lpage></element-citation></ref><ref id="bib41"><element-citation publication-type="journal" id="sref41"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K.</given-names></name><name><surname>Zisserman</surname><given-names>A.</given-names></name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title><comment>Preprint at</comment><source>arXiv</source><year>2015</year><pub-id pub-id-type="doi">10.48550/arXiv.1409.1556</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal" id="sref42"><person-group person-group-type="author"><name><surname>Spiesman</surname><given-names>B.J.</given-names></name><name><surname>Gratton</surname><given-names>C.</given-names></name><name><surname>Hatfield</surname><given-names>R.G.</given-names></name><name><surname>Hsu</surname><given-names>W.H.</given-names></name><name><surname>Jepsen</surname><given-names>S.</given-names></name><name><surname>McCornack</surname><given-names>B.</given-names></name><name><surname>Patel</surname><given-names>K.</given-names></name><name><surname>Wang</surname><given-names>G.</given-names></name></person-group><article-title>Assessing the potential for deep learning and computer vision to identify bumble bee species from images</article-title><source>Sci. Rep.</source><volume>11</volume><year>2021</year><fpage>7580</fpage><pub-id pub-id-type="doi">10.1038/s41598-021-87210-1</pub-id><comment>ISSN 2045-2322</comment><pub-id pub-id-type="pmid">33828196</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book" id="sref43"><person-group person-group-type="author"><name><surname>Tsacas</surname><given-names>L.</given-names></name><name><surname>Chenon</surname><given-names>R.D.</given-names></name><name><surname>Coutin</surname><given-names>R.</given-names></name></person-group><part-title>Observations on larval parasitism of <italic>Hyperechia bomboides</italic></part-title><source>Annales de la Soci&#x000e9;t&#x000e9; entomologique de France</source><volume>6</volume><year>1970</year><publisher-name>SOC Entomologique France</publisher-name><fpage>493</fpage></element-citation></ref><ref id="bib44"><element-citation publication-type="journal" id="sref44"><person-group person-group-type="author"><name><surname>Van der Maaten</surname><given-names>L.</given-names></name><name><surname>Hinton</surname><given-names>G.</given-names></name></person-group><article-title>Visualizing data using t-SNE</article-title><source>J.&#x000a0;Mach. Learn. Res.</source><volume>9</volume><year>2008</year></element-citation></ref><ref id="bib45"><element-citation publication-type="journal" id="sref45"><person-group person-group-type="author"><name><surname>Venegas</surname><given-names>P.</given-names></name><name><surname>Calderon</surname><given-names>F.</given-names></name><name><surname>Riofr&#x000ed;o</surname><given-names>D.</given-names></name><name><surname>Ben&#x000ed;tez</surname><given-names>D.</given-names></name><name><surname>Ram&#x000f3;n</surname><given-names>G.</given-names></name><name><surname>Cisneros-Heredia</surname><given-names>D.</given-names></name><name><surname>Coimbra</surname><given-names>M.</given-names></name><name><surname>Rojo-&#x000c1;lvarez</surname><given-names>J.L.</given-names></name><name><surname>P&#x000e9;rez</surname><given-names>N.</given-names></name></person-group><article-title>Automatic ladybird beetle detection using deep-learning models</article-title><source>PLoS One</source><volume>16</volume><year>2021</year><object-id pub-id-type="publisher-id">e0253027&#x02013;21</object-id><pub-id pub-id-type="doi">10.1371/journal.pone.0253027</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book" id="sref46"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>A.R.</given-names></name></person-group><part-title>Contributions to the Theory of Natural Selection</part-title><year>1871</year><publisher-name>Macmillan and Company</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511693106</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal" id="sref47"><person-group person-group-type="author"><name><surname>Wattenberg</surname><given-names>M.</given-names></name><name><surname>Vi&#x000e9;gas</surname><given-names>F.</given-names></name><name><surname>Johnson</surname><given-names>I.</given-names></name></person-group><article-title>How to use t-SNE effectively</article-title><source>Distill</source><volume>1</volume><year>2016</year><fpage>e2</fpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal" id="sref48"><person-group person-group-type="author"><name><surname>Wham</surname><given-names>D.C.</given-names></name><name><surname>Ezray</surname><given-names>B.</given-names></name><name><surname>Hines</surname><given-names>H.M.</given-names></name></person-group><article-title>Measuring perceptual distance of organismal color pattern using the features of deep neural networks</article-title><comment>Preprint at</comment><source>bioRxiv</source><year>2019</year><fpage>736306</fpage><pub-id pub-id-type="doi">10.1101/736306</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal" id="sref49"><person-group person-group-type="author"><name><surname>Wiegmann</surname><given-names>B.M.</given-names></name><name><surname>Trautwein</surname><given-names>M.D.</given-names></name><name><surname>Kim</surname><given-names>J.-W.</given-names></name><name><surname>Cassel</surname><given-names>B.K.</given-names></name><name><surname>Bertone</surname><given-names>M.A.</given-names></name><name><surname>Winterton</surname><given-names>S.L.</given-names></name><name><surname>Yeates</surname><given-names>D.K.</given-names></name></person-group><article-title>Single-copy nuclear genes resolve the phylogeny of the holometabolous insects</article-title><source>BMC Biol.</source><volume>7</volume><year>2009</year><fpage>34</fpage><pub-id pub-id-type="doi">10.1186/1741-7007-7-34</pub-id><pub-id pub-id-type="pmid">19552814</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="book" id="sref50"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>P.H.</given-names></name><name><surname>Thorp</surname><given-names>R.W.</given-names></name><name><surname>Richardson</surname><given-names>L.L.</given-names></name><name><surname>Colla</surname><given-names>S.R.</given-names></name></person-group><part-title>Bumble bees of north America</part-title><source>Bumble Bees of North America</source><year>2014</year><publisher-name>Princeton University Press</publisher-name><pub-id pub-id-type="doi">10.1515/9781400851188</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal" id="sref51"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>M.</given-names></name><name><surname>Cao</surname><given-names>X.</given-names></name><name><surname>Guo</surname><given-names>S.</given-names></name></person-group><article-title>Accurate detection and tracking of ants in indoor and&#x000a0;outdoor environments</article-title><comment>Preprint at</comment><source>bioRxiv</source><year>2020</year><pub-id pub-id-type="doi">10.1101/2020.11.30.403816</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book" id="sref52"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y.</given-names></name><name><surname>Richmond</surname><given-names>D.</given-names></name></person-group><part-title>Pre-training on grayscale imagenet improves medical image classification</part-title><source>Proceedings of the European Conference on Computer Vision (ECCV) Workshops</source><year>2018</year></element-citation></ref><ref id="bib53"><element-citation publication-type="journal" id="sref53"><person-group person-group-type="author"><name><surname>Yeates</surname><given-names>D.K.</given-names></name><name><surname>Greathead</surname><given-names>D.</given-names></name></person-group><article-title>The evolutionary pattern of host use in the Bombyliidae (Diptera): a diverse family of parasitoid flies</article-title><source>Biol. J. Linn. Soc. Lond.</source><volume>60</volume><year>1997</year><fpage>149</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1111/j.1095-8312.1997.tb01490.x</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book" id="sref54"><person-group person-group-type="author"><name><surname>Yohanandan</surname><given-names>S.</given-names></name><name><surname>Song</surname><given-names>A.</given-names></name><name><surname>Dyer</surname><given-names>A.G.</given-names></name><name><surname>Tao</surname><given-names>D.</given-names></name></person-group><part-title>Saliency preservation in low-resolution grayscale images</part-title><source>Proceedings of the European Conference on Computer Vision</source><year>2018</year><publisher-name>ECCV</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="book" id="sref55"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>B.</given-names></name><name><surname>Khosla</surname><given-names>A.</given-names></name><name><surname>Lapedriza</surname><given-names>A.</given-names></name><name><surname>Oliva</surname><given-names>A.</given-names></name><name><surname>Torralba</surname><given-names>A.</given-names></name></person-group><part-title>Learning deep features for discriminative localization</part-title><source>2016 IEEE Conference on Computer Vision and Pattern Recognition</source><year>2016</year><publisher-name>CVPR</publisher-name><fpage>2921</fpage><lpage>2929</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.319</pub-id></element-citation></ref></ref-list><sec id="appsec2" sec-type="supplementary-material"><title>Supplemental information</title><p id="p0315">
<supplementary-material content-type="local-data" id="mmc1"><caption><title>Table&#x000a0;S1. Bee dataset, related to Table&#x000a0;2 and Table&#x000a0;3</title></caption><media xlink:href="mmc1.xlsx"/></supplementary-material>
<supplementary-material content-type="local-data" id="mmc2"><caption><title>Table&#x000a0;S2. Non-bee dataset, related to Table&#x000a0;1</title></caption><media xlink:href="mmc2.xlsx"/></supplementary-material>
<supplementary-material content-type="local-data" id="mmc3"><caption><title>Table&#x000a0;S3. Mimic dataset, related to Table&#x000a0;4</title></caption><media xlink:href="mmc3.xlsx"/></supplementary-material>
<supplementary-material content-type="local-data" id="mmc4"><caption><title>Table&#x000a0;S4. Species used in training, validation, related to Table&#x000a0;6</title></caption><media xlink:href="mmc4.xlsx"/></supplementary-material>
<supplementary-material content-type="local-data" id="mmc5"><caption><title>Table&#x000a0;S5. Species not used in training, validation, related to Table&#x000a0;6</title></caption><media xlink:href="mmc5.xlsx"/></supplementary-material>
<supplementary-material content-type="local-data" id="mmc6"><caption><title>Table&#x000a0;S6. Photographer credit, related to acknowledgment section</title></caption><media xlink:href="mmc6.xlsx"/></supplementary-material>
</p></sec><sec sec-type="data-availability" id="da0010"><title>Data and code availability</title><p id="p0030">
<list list-type="simple" id="ulist0015"><list-item id="u0030"><label>&#x02022;</label><p id="p0035">All data used in this study are available through Mendeley Data (<xref rid="bib1" ref-type="bibr">Bhuiyan, 2022</xref>).</p></list-item><list-item id="u0035"><label>&#x02022;</label><p id="p0040">All code used in this study is available through a Zenodo repository (<xref rid="bib2" ref-type="bibr">Bhuiyan et&#x000a0;al., 2022</xref>).</p></list-item><list-item id="u0040"><label>&#x02022;</label><p id="p0045">Other associated information is available in our Supplemental Information Excel files.</p></list-item></list>
</p></sec><ack id="ack0010"><title>Acknowledgments</title><p id="p0295">We would like to acknowledge the contributions of citizen scientists on the iNaturalist platform and in particular the photographs herein from Paul W. Sweet (psweet), Steve Kerr (steve_kerr), Bernd B&#x000e4;umler (berndtherat), Savvas Zafeiriou (savvaszafeiriou), Derek Hennen (derhennen), Steve Reekie (stevereekie1), John Witton (johnwitton), Anna Barr (darwanna), Matthew Wills (matthew_wills), Christine S-C Adamson (scibadger), Sue Gregoire (suegregoire), Greg Lasley (greglasley), Brian W (withy24269), ctdbryantx, Peter LaBelle (tapaculo99), Martin John Bishop (martinbishop), and dendzoscarab. See details in <xref rid="mmc6" ref-type="supplementary-material">Table&#x000a0;S6</xref>. This work was supported in part by the <funding-source id="gs1"><institution-wrap><institution-id institution-id-type="doi">10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source> under Grant No. IIS-2014547 to RMC and SC. Opinions, findings, and conclusions are those of the authors alone and do not necessarily reflect the views of the funding agency.</p><sec id="sec6"><title>Author contributions</title><p id="p0300">Conceptualization, R.M.C.; funding acquisition, R.M.C. and S.C.; investigation, R.M.C., T.B. and S.C.; visualization, T.B. and R.M.C.; writing&#x02014;original draft preparation, T.B., R.M.C. and S.C.; writing&#x02014;review and editing, T.B., R.M.C. and S.C.; methodology, T.B, R.M.C. and S.C., Software, T.B. and S.C. All authors have read and agreed to the published version of the article.</p></sec><sec sec-type="COI-statement" id="sec7"><title>Declaration of interests</title><p id="p0305">The authors declare no competing interests.</p></sec></ack><fn-group><fn id="appsec1" fn-type="supplementary-material"><p id="p0310">Supplemental information can be found online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.isci.2022.104924" id="intref0065">https://doi.org/10.1016/j.isci.2022.104924</ext-link>.</p></fn></fn-group></back></article>
