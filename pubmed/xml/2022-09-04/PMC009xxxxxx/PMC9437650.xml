<!DOCTYPE article
PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN" "JATS-archivearticle1-3-mathml3.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article"><?properties open_access?><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Transl Vis Sci Technol</journal-id><journal-id journal-id-type="iso-abbrev">Transl Vis Sci Technol</journal-id><journal-id journal-id-type="publisher-id">TVST</journal-id><journal-title-group><journal-title>Translational Vision Science &#x00026; Technology</journal-title></journal-title-group><issn pub-type="epub">2164-2591</issn><publisher><publisher-name>The Association for Research in Vision and Ophthalmology</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">36040250</article-id><article-id pub-id-type="pmc">PMC9437650</article-id><article-id pub-id-type="doi">10.1167/tvst.11.8.30</article-id><article-id pub-id-type="publisher-id">TVST-22-4494</article-id><article-categories><subj-group subj-group-type="heading"><subject>Artificial Intelligence</subject></subj-group><subj-group subj-group-type="category"><subject>Artificial Intelligence</subject></subj-group></article-categories><title-group><article-title>Development of Cumulative Order-Preserving Image Transformation Based Variational Autoencoder for Anterior Segment Optical Coherence Tomography Images</article-title><alt-title alt-title-type="runhead">Latent Space Representation of Anterior Segment OCT Images</alt-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Shon</surname><given-names>Kilhwan</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="aff4" ref-type="aff">
<sup>4</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Sung</surname><given-names>Kyung Rim</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Kwak</surname><given-names>Jiehoon</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Lee</surname><given-names>Joo Yeon</given-names></name><xref rid="aff3" ref-type="aff">
<sup>3</sup>
</xref><xref rid="aff4" ref-type="aff">
<sup>4</sup>
</xref></contrib><contrib contrib-type="author"><name><surname>Shin</surname><given-names>Joong Won</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref></contrib><aff id="aff1"><label>1</label>Department of Ophthalmology, Gangneung Asan Hospital, Gangneung, Korea</aff><aff id="aff2"><label>2</label>Department of Ophthalmology, College of Medicine, University of Ulsan, Asan Medical Center, Seoul, Korea</aff><aff id="aff3"><label>3</label>Camp 9 Orthopedic Clinic, Hwaseong-si, Gyeonggi-do, Korea</aff><aff id="aff4"><label>4</label>Asan Artificial Intelligence Institute, Hwaseong-si, Gyeonggi-do, Korea</aff></contrib-group><author-notes><corresp id="cor1">
<label>*</label>
<bold>Correspondence:</bold> Kyung Rim Sung, Department of Ophthalmology, University of Ulsan, College of Medicine, Asan Medical Center, 388-1 Pungnap-2-dong, Songpa-gu, Seoul, Korea. e-mail: <email>sungeye@gmail.com</email></corresp></author-notes><pub-date pub-type="epub"><day>30</day><month>8</month><year>2022</year></pub-date><pub-date pub-type="collection"><month>8</month><year>2022</year></pub-date><volume>11</volume><issue>8</issue><elocation-id>30</elocation-id><history><date date-type="accepted"><day>24</day><month>7</month><year>2022</year></date><date date-type="received"><day>10</day><month>2</month><year>2022</year></date></history><permissions><copyright-statement>Copyright 2022 The Authors</copyright-statement><copyright-year>2022</copyright-year><license><ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.</license-p></license></permissions><self-uri xlink:title="pdf" xlink:href="tvst-11-8-30.pdf"/><abstract><sec><title>Purpose</title><p>To develop a variational autoencoder (VAE) suitable for analysis of the latent structure of anterior segment optical coherence tomography (AS-OCT) images and to investigate possibilities of latent structure analysis of the AS-OCT images.</p></sec><sec><title>Methods</title><p>We retrospectively collected clinical data and AS-OCT images from 2111 eyes of 1261 participants from the ongoing Asan Glaucoma Progression Study. A specifically modified VAE was used to extract six symmetrical and one asymmetrical latent variable. A total of 1692 eyes of 1007 patients were used for training the model. Conventional measurements and latent variables were compared between 74 primary angle closure (PAC) and 51 primary angle closure glaucoma (PACG) eyes from validation set (419 eyes of 254 patients) that were not used for training.</p></sec><sec><title>Results</title><p>Among the symmetrical latent variables, the first three and the last demonstrated easily recognized features, anterior chamber area in &#x003b7;<sub>1</sub>, curvature of the cornea in &#x003b7;<sub>2</sub>, the pupil size in &#x003b7;<sub>3</sub> and corneal thickness in &#x003b7;<sub>6</sub>, whereas &#x003b7;<sub>4</sub> and &#x003b7;<sub>5</sub> were more complex aggregating complex interactions of multiple structures. Compared with PAC eyes, there was no difference in any of the conventional measurements in PACG eyes. However, values of &#x003b7;<sub>4</sub> were significantly different between the two groups, being smaller in the PACG group (<italic toggle="yes">P</italic> = 0.015).</p></sec><sec><title>Conclusions</title><p>VAE is a useful framework for analysis of the latent structure of AS-OCT. Latent structure analysis could be useful in capturing features not readily evident with conventional measures.</p></sec><sec><title>Translational Relevance</title><p>This study suggested that a deep learning-based latent space model can be applied for the analysis of AS-OCT images to find latent characteristics of the anterior segment of the eye.</p></sec></abstract><kwd-group><kwd>variational autoencoder</kwd><kwd>artificial intelligence</kwd><kwd>angle closure glaucoma</kwd><kwd>anterior segment optical coherence tomography</kwd></kwd-group><counts><page-count count="10"/></counts></article-meta></front><body><sec sec-type="intro" id="sec1"><title>Introduction</title><p>Although advances have been made in imaging techniques of the anterior segment, such as anterior segment optical coherence tomography (AS-OCT), the appropriate analysis of acquired high-resolution images has been limited by the lack of proper analytical tools. Conventional methods consist of manual measurement of hand-crafted features by the physician. The commonly used parameters are as follows; anterior chamber depth (ACD), width, and area (ACA), lens vault, angle recess area, angle opening distance, trabecular-iris-space area, and iris thickness.<xref rid="bib1" ref-type="bibr"><sup>1</sup></xref> Conventional analysis has been successfully utilized in a variety of tasks, including subclassification, monitoring intervention-induced changes, and describing dynamic and long-term processes in the anterior segment. Still, these parameters cannot inherently comprehend the whole image and have vulnerabilities when considering highly correlated parameters.<xref rid="bib2" ref-type="bibr"><sup>2</sup></xref><sup>&#x02013;</sup><xref rid="bib7" ref-type="bibr"><sup>7</sup></xref></p><p>Recent advances in deep learning technology provide a new approach to image data analysis. Several studies have proven that deep learning techniques are not only suitable for the analysis of AS-OCT images but can achieve accuracies comparable to human measurements.<xref rid="bib8" ref-type="bibr"><sup>8</sup></xref><sup>&#x02013;</sup><xref rid="bib12" ref-type="bibr"><sup>12</sup></xref> However, because these studies usually involve large-scale automated measurements of manually assigned labels, they are affected by all the limitations of manual measurements.</p><p>Latent space modeling is one of the machine learning approaches that could be applied to analyze high-dimensional data. An example of a latent space model (or latent variable model) used in the conventional context is factor analysis, such as the ones that have long been used in psychology.<xref rid="bib13" ref-type="bibr"><sup>13</sup></xref> In the field of computer vision, a family of latent space models called deep generative models has been intensively developed to analyze various images.<xref rid="bib14" ref-type="bibr"><sup>14</sup></xref> In our previous study, we have shown that a convolutional &#x003b2;-variational autoencoder (VAE) can be applied to the AS-OCT images to achieve a good disentangled latent space representation.<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref> Despite encouraging results, we also found shortcomings of a convolutional VAE framework, which motivated us to improve the model. The previous model had limited power in separating asymmetrical variance from symmetrical variance, which hampered the disentanglement of latent variables.</p><p>To overcome problems arising from asymmetricity, we have developed a new method inspired by spatial transformer networks.<xref rid="bib16" ref-type="bibr"><sup>16</sup></xref> In this new model, we have preserved the overall framework of VAE, but instead of convolution, an image warping technique we have named cumulative order-preserving image transforming network (COPIT) is used to reconstruct images from the latent space. COPIT was specifically developed and tailored for the current article's latent space representation of AS-OCT images. COPIT has been designed to have several properties: (1) the order of the <italic toggle="yes">x</italic> and <italic toggle="yes">y</italic> coordinates are preserved after transformation; (2) each latent variable defines a unique transformation; (3) multiple transformations can be combined into a single new transformation; and (4) each layer can be designed differently depending on the purposes. The entire network is based on the convolutional &#x003b2;-VAE with two modifications: (1) the convolutional decoder part is replaced by COPIT, and (2) the loss function has been modified to include a cosine similarity.</p></sec><sec sec-type="methods" id="sec2"><title>Methods</title><sec id="sec2-1"><title>Participants</title><p>We selected participants from the ongoing Asan Glaucoma Progression Study who have undergone an AS-OCT examination (Visante OCT, ver. 3.0; Carl Zeiss Meditec, Jena, Germany). From 2111 eyes of 1261 patients, we randomly assigned 80% of the patients to the training set (1692 eyes of 1007 patients) and the remaining 20% to the validation set (419 eyes of 254 patients), ensuring both eyes of the patient were assigned to the same group. Comparisons between conventional measurements and latent variables were made using patients from the validation set. A more detailed description of the population, including the clinical assessment, inclusion criteria, image acquisition, and demographics, has been included in our previous study.<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref></p><p>All procedures conformed to the Declaration of Helsinki, and this study was approved by the institutional review board of the Asan Medical Center, University of Ulsan, Seoul, Korea.</p></sec><sec id="sec2-2"><title>Image Preparation and Segmentation</title><p>Raw AS-OCT images of size 1200 &#x000d7; 1500 (<italic toggle="yes">H</italic> &#x000d7; <italic toggle="yes">W</italic>) pixels were center cropped to create a grayscale 512 &#x000d7; 1024 image, resized to 256 &#x000d7; 512 pixels. The segmentation was done in four steps: (1) resized images were manually segmented into three segments: the iris, the corneoscleral shell, and the anterior chamber by an experienced glaucoma specialist (KS); (2) 130 manually segmented images were used to train a modified U-net; (3) a trained modified U-net was used to segment remaining images; and (4) segmented images were aligned with rotation and translation using a spatial-transformer network.<xref rid="bib16" ref-type="bibr"><sup>16</sup></xref> A modified U-net is structurally identical to the original U-net but has been reduced in depth and has been adjusted to a different resolution.<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref><sup>,</sup><xref rid="bib17" ref-type="bibr"><sup>17</sup></xref> Segmented images were cropped to a size of 192 &#x000d7; 448, and left eyes were horizontally flipped.</p></sec><sec id="sec2-3"><title>Model Structure</title><p>Our model follows the general structure of VAE, but unlike the typical convolutional autoencoder, a decoder part was replaced with an image warping technique that was inspired by a spatial transformer network.<xref rid="bib16" ref-type="bibr"><sup>16</sup></xref> Compared to the original spatial transformer network, our model has the following major structural differences: (1) added a &#x0201c;reparameterization&#x0201d; part; (2) replaced a &#x0201c;grid generator&#x0201d; with our new COPIT; (3) replaced the sampling technique with a linearized multi-sampling technique proposed by Jiang et al.;<xref rid="bib18" ref-type="bibr"><sup>18</sup></xref> and (4) have modified the loss function.<xref rid="bib16" ref-type="bibr"><sup>16</sup></xref><sup>,</sup><xref rid="bib18" ref-type="bibr"><sup>18</sup></xref><sup>,</sup><xref rid="bib19" ref-type="bibr"><sup>19</sup></xref></p><p>The encoder (which corresponds to a &#x0201c;localization net&#x0201d; in the spatial transformer network) is identical to the model described in our previous study but was trained de novo.<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref> A reparameterization part and addition of Kullback-Leibler divergence (KLD) to the loss function were taken from makes our model a VAE.<xref rid="bib19" ref-type="bibr"><sup>19</sup></xref> The decoder part in our preceding work or a &#x0201c;grid generator&#x0201d; in the spatial transformer network has been replaced with a COPIT-based decoder specifically developed for the current research. In the COPIT, numbers are generated from reparametrized latent variables using fully connected layers, which are then fed into COPIT to generate a sampling grid. Then a transformed image is calculated from the standard image using a sampling grid (<xref rid="fig1" ref-type="fig">Fig. 1</xref>).</p><fig position="float" id="fig1"><label>Figure 1.</label><caption><p>Schematic diagram of convolutional VAE, spatial transformer and COPIT based VAE. In a convolutional VAE, convolutional encoder is used to generate means and standard deviations, from which latent variables are sampled, and then output image is generated from sampled latent variables through convolutional decoder (A). In a spatial transformer, convolutional localization net is used to generate latent variables which are used as transformation coefficients for generating sampling grids, and then output image is calculated using a sampling grid and the standard image (B). In our new model, latent variables are generated in the same way as in the convolutional VAE, but sampled in similar way as in the spatial transformer but grid generator has been replaced with COPIT which two versions - six symmetrical and one asymmetrical. Output image is calculated from final sampling grid and standard image using linearized multi-sampling (C).</p></caption><graphic xlink:href="tvst-11-8-30-f001" position="float"/></fig><p>To separate symmetrical variability from asymmetrical variability, we used two versions of COPIT layers: an asymmetrical layer and a symmetrical layer (the left and right sides of the grid are mirror images reflected over the <italic toggle="yes">y</italic> axis). All variationally inferred latent variables have been matched to the symmetrical layers, while one additional variable was matched with an asymmetrical layer. Also, because generating grids with a number of points equivalent to or larger than the number of pixels, in our case 192 &#x000d7; 448 = 86,016, is not only inefficient but might also cause instability, we have generated down-scaled grids from fully connected layers that were upsampled using bicubic interpolation. A detailed description of COPIT can be found in the <xref rid="tvst-11-8-30_s001" ref-type="supplementary-material">supplementary material</xref>.</p></sec><sec id="sec2-4"><title>The Loss Function</title><p>A latent space <italic toggle="yes">z</italic> in the VAE is variationally inferred such that a posterior <italic toggle="yes">p</italic><sub>&#x003b8;</sub>(<italic toggle="yes">z</italic>|<italic toggle="yes">x</italic>) is approximated to a prior <italic toggle="yes">p</italic><sub>&#x003b8;</sub>(<italic toggle="yes">z</italic>), which is usually defined as a Gaussian distribution <italic toggle="yes">N</italic>(0, <italic toggle="yes">I</italic>).<xref rid="bib20" ref-type="bibr"><sup>20</sup></xref> The distance between a posterior and a prior is measured with a KLD. However, KLD does not provide information regarding the similarity between latent variables. Hence, we have decided to add cosine similarities between all possible combinations of latent variables:
<disp-formula id="equ1"><label>(1)</label><mml:math id="math-dequ1" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x000b7;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02225;</mml:mo></mml:mrow><mml:mrow><mml:mo>&#x02225;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02225;</mml:mo><mml:mo>&#x02225;</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&#x02225;</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>In a special case where the data is centered at zero, cosine similarity is equivalent to Pearson's correlation coefficient.<xref rid="bib21" ref-type="bibr"><sup>21</sup></xref></p><p>With additional hyperparameter <italic toggle="yes">&#x003b3;</italic> and the similarity function <italic toggle="yes">Sim</italic>, our modified loss function VAE is given as:
<disp-formula id="equ2"><label>(2)</label><mml:math id="math-dequ2" overflow="scroll"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi>L</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>&#x003b8;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003d5;</mml:mi><mml:mo>,</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:mo>;</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mtd><mml:mtd><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mi>&#x003d5;</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mfenced close="]" open="[" separators=""><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mspace width="4pt"/></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>&#x003b8;</mml:mi></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>x</mml:mi><mml:mo>|</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd/><mml:mtd columnalign="left"><mml:mrow><mml:mo>+</mml:mo><mml:mi>&#x003b2;</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo></mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>&#x003d5;</mml:mi></mml:msub><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mi>z</mml:mi><mml:mo>|</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced><mml:mrow><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>&#x003b3;</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mfenced close=")" open="("><mml:mi>z</mml:mi></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <italic toggle="yes">x</italic> is an image in our case, <italic toggle="yes">q</italic><sub>&#x003d5;</sub>(<italic toggle="yes">z</italic>|<italic toggle="yes">x</italic>) is an estimated distribution of the latent space, <italic toggle="yes">p</italic><sub>&#x003b8;</sub>(x|z) is the likelihood of generating a true image, and <italic toggle="yes">D<sub>KL</sub></italic> is a KLD.<xref rid="bib22" ref-type="bibr"><sup>22</sup></xref></p></sec><sec id="sec2-5"><title>Adjusting Hyperparameters and Training</title><p>The number of symmetrical latent variables was set to 6 based on our previous work with an additional asymmetrical layer, whereas the values of &#x003b2; and &#x003b3; were both set at 5<sup>2</sup>, which yielded a comparable KLD to the convolutional VAE model presented in our previous work.<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref> The grids were scaled down by a factor of 16, 16, 16, 8, 4, 2 for symmetrical layers 1 to 6 and by a factor of 8 for the asymmetrical layer. For the calculation of the reconstruction loss, mean squared error has resulted in a shorter training time than binary cross-entropy. The layers were trained in three steps: (1) six symmetrical layers were trained sequentially (only one layer was trained at a time while the other layers were frozen) for 900 epochs, (2) the asymmetrical layer was trained for 50 epochs, and (3) all layers were simultaneously trained for 50 epochs.</p></sec><sec id="sec2-6"><title>Making Conventional Measurements and Calculating Latent Variables in Selected Eyes</title><p>Exclusively from patients in the validation set, we collected complete clinical information of patients diagnosed with primary angle closure (PAC) or primary angle closure glaucoma (PACG). All patients have undergone static and dynamic gonioscopy with Sussman 4-mirror gonioscope (Ocular Instruments, Bellevue, WA, USA) in a darkened room (0.5 cd/m<sup>2</sup>) by an experienced glaucoma specialist (K.R.S). PAC was diagnosed if the eyes had an occludable angle (pigmented posterior trabecular meshwork was not visible on nonindentation gonioscopy for at least 180&#x000b0; in the primary position) with signs indicating trabecular obstruction (elevated intraocular pressure, distortion of the radially orientated iris fibers, &#x0201c;glaukomflecken&#x0201d; lens opacities, excessive pigment deposition on the trabecular meshwork, or presence of peripheral anterior synechiae) but without any sign suggestive of glaucoma on optic disc examination and visual field tests. PAC eyes showing glaucomatous optic disc changes (neuroretinal rim thinning, disc excavation, or optic disc hemorrhage attributable to glaucoma) or a glaucomatous visual field change were classified as PACG. Eyes with a previous history suggesting acute angle closure attack have been excluded: (1) presenting with ocular or periocular pain, nausea or vomiting, or intermittent blurred vision with haloes; (2) those with a presenting intraocular pressure of more than 30 mm Hg; and (3) those who had experienced at least three of the following: conjunctival injections, corneal epithelial edema, mid-dilated unreactive pupil. If both eyes were eligible, we selected the right eye. As a result, 125 eyes of 125 patients, including 74 PAC eyes and 51 PACG eyes, were analyzed.</p><p>A single investigator (S.K), blinded to all information, assigned the scleral spur&#x02014;defined as the point showing a change in the curvature of the inner surface of the angled wall - and measured the ACD using calipers built-in in the software provided by the manufacturers.<xref rid="bib23" ref-type="bibr"><sup>23</sup></xref> Then, the software provided by the manufacturer automatically measured the scleral spur angle, angle opening distance at 500 &#x000b5;m and 750 &#x000b5;m, angle recess area at 500 &#x000b5;m and 750 &#x000b5;m, and the trabecular-iris space area at 500 &#x000b5;m and 750 &#x000b5;m. Additionally, the iris thickness at 750 &#x000b5;m from the scleral spur, iris curvature, ACD, anterior chamber width, ACA, and lens vault were measured using Fiji software, and pixel values were converted into real-world units by comparing pupil diameter measured with Fiji software and built-in and calipers.<xref rid="bib24" ref-type="bibr"><sup>24</sup></xref> More detailed descriptions of the measurement methods can be found in our previous works.<xref rid="bib25" ref-type="bibr"><sup>25</sup></xref><sup>&#x02013;</sup><xref rid="bib26" ref-type="bibr"><sup>27</sup></xref></p><p>Clinical information, including age, gender, axial length, baseline intraocular pressure, manual measurements, and values of latent variables derived from the neural network trained in previous steps were compared between the PAC and PACG eyes with Student's <italic toggle="yes">t</italic> test for continuous variables and the &#x003c7;<sup>2</sup> test for frequency variables using SAS 9.4 software (SAS Institute Inc., Cary, NC, USA). We have not collected detailed information from the training dataset due to the larger size of the dataset, but we assume the proportion of PAC/PACG eyes in the training dataset to be not statistically different because patients were randomly assigned training and validation datasets and demographics do not differ.<xref rid="bib15" ref-type="bibr"><sup>15</sup></xref></p></sec></sec><sec sec-type="results" id="sec3"><title>Results</title><sec id="sec3-1"><title>Exploration of the Latent Space</title><p>A satisfactory latent space disentanglement was achieved in our new method, such that the variables were discernible and appreciable in the visual analysis. Specifically, &#x003b7;<sub>1</sub> seems to represent an overall ACD and ACA, whereas &#x003b7;<sub>2</sub> seems to mainly represent the curvature of the cornea. The &#x003b7;<sub>3</sub> was associated with pupil size changes without any noticeable changes in the corneoscleral or lens contour. The &#x003b7;<sub>4</sub> was also related to the pupil size, but in contrast to &#x003b7;<sub>3</sub>, the following differences are worth mentioning: (1) the iris is thicker and more curved with a small pupil size (negative <italic toggle="yes">z</italic> value); (2) the iris became relatively flat when the pupil got larger (positive <italic toggle="yes">z</italic> value); and (3) there was an overt increase in the lens vault with a positive <italic toggle="yes">z</italic> value. Little perceivable changes were seen for the &#x003b7;<sub>5</sub> variable, but by creating a subtraction image with extreme <italic toggle="yes">z</italic> values, we can notice some interesting characteristics: (1) the iris is flatter at negative <italic toggle="yes">z</italic> values and more curved at positive <italic toggle="yes">z</italic> values; (2) at negative <italic toggle="yes">z</italic> values, the iris becomes thinner, but the peripheral part remains thick; (3) the anterior chamber gets narrower at negative <italic toggle="yes">z</italic> values and wider at positive <italic toggle="yes">z</italic> values; (4) the periphery of the anterior chamber becomes shallower at negative <italic toggle="yes">z</italic> values and deeper at positive <italic toggle="yes">z</italic> values; and (5) there is a subtle change in the corneal profile such that at negative <italic toggle="yes">z</italic> values, the central portion of the cornea is steeper whereas the peripheral portion of the cornea is flatter and thicker. The &#x003b7;<sub>6</sub> seems to be mainly related to the corneal thickness, whereas &#x003b7;<sub>A</sub> appears to represent the asymmetricity as intended (<xref rid="fig2" ref-type="fig">Fig. 2</xref> and <xref rid="fig3" ref-type="fig">Fig. 3</xref>).</p><fig position="float" id="fig2"><label>Figure 2.</label><caption><p>Visualization of the latent variables of the model. There are six symmetrical layers denoted <inline-formula><mml:math id="math-inTM0006" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mspace width="4.pt"/><mml:mtext>to</mml:mtext><mml:mspace width="4.pt"/><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and one asymmetric layer denoted <inline-formula><mml:math id="math-inTM0006a" overflow="scroll"><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula>. For asymmetric layers, <italic toggle="yes">z</italic> values of &#x02212;2.0, &#x02212;1.0, 0, 1.0, and 2.0 were used while for the asymmetric layer, <italic toggle="yes">z</italic> values have been reduced to &#x02212;1.0, &#x02212;0.5, 0, 0.5, and 1.0 for better interpretability.</p></caption><graphic xlink:href="tvst-11-8-30-f002" position="float"/></fig><fig position="float" id="fig3"><label>Figure 3.</label><caption><p>Subtraction image of <inline-formula><mml:math id="math-inTM0006b" overflow="scroll"><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>5</mml:mn></mml:msub></mml:math></inline-formula> for <italic toggle="yes">z</italic> values &#x02212;4 and 4. Areas that correspond to <inline-formula><mml:math id="math-inTM0006c" overflow="scroll"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> are color-coded in yellow tone while areas that correspond are color-coded in blue tone. Gray color represents an area of the cornea which is common to both <italic toggle="yes">z</italic> values and green color represents areas of iris common to both z values.</p></caption><graphic xlink:href="tvst-11-8-30-f003" position="float"/></fig></sec><sec id="sec3-2"><title>Conventional Measurements and Latent Variables for PAC and PACG Eyes</title><p>There was no statistically significant difference between PAC and PACG eyes for any of the conventional measurements, with angle recess area having the lowest <italic toggle="yes">P</italic> value (<italic toggle="yes">P</italic> = 0.116). Among the latent variables, PACG eyes have a smaller value of &#x003b7;<sub>4</sub> compared to PAC eyes (<italic toggle="yes">P</italic> = 0.015; <xref rid="tbl1" ref-type="table">Table</xref>). Values of the latent variables can be visualized with the network in several ways. When using mean values of &#x003b7;<sub>1</sub> to &#x003b7;<sub>6</sub> for PAC and PACG eyes, there is little difference in the cornea or lens between the groups. The size of the pupil and width of the anterior chamber was slightly smaller in the PACG group compared to the PAC group, which is consistent with the conventional measurements in the <xref rid="tbl1" ref-type="table">Table</xref> (the top row of <xref rid="fig4" ref-type="fig">Fig. 4</xref>). To enhance the difference, we upscaled the mean values of the latent variables for each group by a factor of three. The angle difference was more noticeable with the narrower angle in the PACG group compared to that of the PAC group (middle row of <xref rid="fig4" ref-type="fig">Fig. 4</xref>), although there was little difference for the cornea or lens. Because we can select certain latent variables, we have created a subtraction image using the mean values of &#x003b7;<sub>4</sub> for the PAC and PACG eyes multiplied by 3, whereas all other latent variables are kept constant at zero. Still, there is little difference in the angle, corneal contour, or anterior chamber width, although the lens vault is slightly larger in the PAC group (bottom row of <xref rid="fig4" ref-type="fig">Fig. 4</xref>).</p><table-wrap position="float" content-type="4col" id="tbl1"><label>Table.</label><caption><p>Conventional Measurements and Latent Variables for PAC and PACG Eyes</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col align="char" char="(" span="1"/><col align="char" char="(" span="1"/><col align="char" char="." span="1"/></colgroup><thead><tr><th rowspan="1" colspan="1"/><th align="center" rowspan="1" colspan="1">PAC (<italic toggle="yes">n</italic> = 74)</th><th align="center" rowspan="1" colspan="1">PACG (<italic toggle="yes">n</italic> = 51)</th><th align="center" rowspan="1" colspan="1">
<italic toggle="yes">P</italic> Value</th></tr></thead><tbody><tr><td rowspan="1" colspan="1">Age, years</td><td rowspan="1" colspan="1">61.8 (&#x000b1;9.3)</td><td rowspan="1" colspan="1">63 (&#x000b1;8.5)</td><td rowspan="1" colspan="1">0.473</td></tr><tr><td rowspan="1" colspan="1">Sex, % of females</td><td rowspan="1" colspan="1">62 (83.8%)</td><td rowspan="1" colspan="1">42 (82.4%)</td><td rowspan="1" colspan="1">0.833</td></tr><tr><td rowspan="1" colspan="1">Laterality, right</td><td rowspan="1" colspan="1">68 (91.9%)</td><td rowspan="1" colspan="1">43 (88.2%)</td><td rowspan="1" colspan="1">0.495</td></tr><tr><td rowspan="1" colspan="1">Axial length, mm</td><td rowspan="1" colspan="1">22.7 (&#x000b1;0.7)</td><td rowspan="1" colspan="1">22.7 (&#x000b1;0.6)</td><td rowspan="1" colspan="1">0.916</td></tr><tr><td rowspan="1" colspan="1">Baseline IOP, mmHg</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td rowspan="1" colspan="1">Pupil diameter, mm</td><td rowspan="1" colspan="1">4.27 (&#x000b1;0.87)</td><td rowspan="1" colspan="1">4.09 (&#x000b1;1.03)</td><td rowspan="1" colspan="1">0.278</td></tr><tr><td rowspan="1" colspan="1">ACD, mm</td><td rowspan="1" colspan="1">2.04 (&#x000b1;0.26)</td><td rowspan="1" colspan="1">2.03 (&#x000b1;0.26)</td><td rowspan="1" colspan="1">0.867</td></tr><tr><td rowspan="1" colspan="1">ACW, mm</td><td rowspan="1" colspan="1">11.82 (&#x000b1;0.54)</td><td rowspan="1" colspan="1">11.76 (&#x000b1;0.43)</td><td rowspan="1" colspan="1">0.543</td></tr><tr><td rowspan="1" colspan="1">ACA, mm<sup>2</sup></td><td rowspan="1" colspan="1">13.61 (&#x000b1;2.03)</td><td rowspan="1" colspan="1">13.01 (&#x000b1;2.12)</td><td rowspan="1" colspan="1">0.116</td></tr><tr><td rowspan="1" colspan="1">LV, mm</td><td rowspan="1" colspan="1">1.14 (&#x000b1;0.32)</td><td rowspan="1" colspan="1">1.09 (&#x000b1;0.29)</td><td rowspan="1" colspan="1">0.378</td></tr><tr><td rowspan="1" colspan="1">AOD500, &#x000b5;m</td><td rowspan="1" colspan="1">148.3 (&#x000b1;86.7)</td><td rowspan="1" colspan="1">128.8 (&#x000b1;82.3)</td><td rowspan="1" colspan="1">0.210</td></tr><tr><td rowspan="1" colspan="1">AOD750, &#x000b5;m</td><td rowspan="1" colspan="1">213.2 (&#x000b1;113.1)</td><td rowspan="1" colspan="1">188 (&#x000b1;112.3)</td><td rowspan="1" colspan="1">0.223</td></tr><tr><td rowspan="1" colspan="1">ARA500, mm<sup>2</sup></td><td rowspan="1" colspan="1">0.077 (&#x000b1;0.047)</td><td rowspan="1" colspan="1">0.070 (&#x000b1;0.039)</td><td rowspan="1" colspan="1">0.353</td></tr><tr><td rowspan="1" colspan="1">ARA750, mm<sup>2</sup></td><td rowspan="1" colspan="1">0.121 (&#x000b1;0.067)</td><td rowspan="1" colspan="1">0.108 (&#x000b1;0.060)</td><td rowspan="1" colspan="1">0.283</td></tr><tr><td rowspan="1" colspan="1">TISA500, mm<sup>2</sup></td><td rowspan="1" colspan="1">0.058 (&#x000b1;0.032)</td><td rowspan="1" colspan="1">0.053 (&#x000b1;0.028)</td><td rowspan="1" colspan="1">0.428</td></tr><tr><td rowspan="1" colspan="1">TISA750, mm<sup>2</sup></td><td rowspan="1" colspan="1">0.104 (&#x000b1;0.057)</td><td rowspan="1" colspan="1">0.092 (&#x000b1;0.050)</td><td rowspan="1" colspan="1">0.223</td></tr><tr><td rowspan="1" colspan="1">SSA, degrees</td><td rowspan="1" colspan="1">16.0 (&#x000b1;8.4)</td><td rowspan="1" colspan="1">14.0 (&#x000b1;8.3)</td><td rowspan="1" colspan="1">0.206</td></tr><tr><td rowspan="1" colspan="1">IT750, &#x000b5;m</td><td rowspan="1" colspan="1">0.432 (&#x000b1;0.094)</td><td rowspan="1" colspan="1">0.433 (&#x000b1;0.079)</td><td rowspan="1" colspan="1">0.942</td></tr><tr><td rowspan="1" colspan="1">Iris curvature, &#x000b5;m</td><td rowspan="1" colspan="1">0.18 (&#x000b1;0.073)</td><td rowspan="1" colspan="1">0.186 (&#x000b1;0.076)</td><td rowspan="1" colspan="1">0.773</td></tr><tr><td rowspan="1" colspan="1">&#x003b7;<sub>1</sub></td><td rowspan="1" colspan="1">0.499 (&#x000b1;0.548)</td><td rowspan="1" colspan="1">0.694 (&#x000b1;0.791)</td><td rowspan="1" colspan="1">0.105</td></tr><tr><td rowspan="1" colspan="1">&#x003b7;<sub>2</sub></td><td rowspan="1" colspan="1">0.155 (&#x000b1;0.867)</td><td rowspan="1" colspan="1">&#x02212;0.089 (&#x000b1;0.965)</td><td rowspan="1" colspan="1">0.142</td></tr><tr><td rowspan="1" colspan="1">&#x003b7;<sub>3</sub></td><td rowspan="1" colspan="1">0.034 (&#x000b1;0.947)</td><td rowspan="1" colspan="1">0.114 (&#x000b1;1.004)</td><td rowspan="1" colspan="1">0.650</td></tr><tr><td rowspan="1" colspan="1">&#x003b7;<sub>4</sub></td><td rowspan="1" colspan="1">0.412 (&#x000b1;0.872)</td><td rowspan="1" colspan="1">0.012 (&#x000b1;0.922)</td><td rowspan="1" colspan="1">
<bold>0.015</bold>
<xref rid="tb1fn1" ref-type="table-fn">
<bold>
<sup>*</sup>
</bold>
</xref>
</td></tr><tr><td rowspan="1" colspan="1">&#x003b7;<sub>5</sub></td><td rowspan="1" colspan="1">0.062 (&#x000b1;1.083)</td><td rowspan="1" colspan="1">0.035 (&#x000b1;0.856)</td><td rowspan="1" colspan="1">0.882</td></tr><tr><td rowspan="1" colspan="1">&#x003b7;<sub>6</sub></td><td rowspan="1" colspan="1">0.013 (&#x000b1;0.845)</td><td rowspan="1" colspan="1">&#x02212;0.130 (&#x000b1;1.02)</td><td rowspan="1" colspan="1">0.397</td></tr><tr><td rowspan="1" colspan="1">&#x003b7;<sub>A</sub></td><td rowspan="1" colspan="1">0.204 (&#x000b1;0.115)</td><td rowspan="1" colspan="1">0.199 (&#x000b1;0.13)</td><td rowspan="1" colspan="1">0.806</td></tr></tbody></table><table-wrap-foot><fn id="tb1fn1"><p>IOP = intraocular pressure; ACD = anterior chamber depth; ACW = anterior chamber width; ACA = anterior chamber area; LV = lens vault; AOD500 = angle opening distance at 500 &#x000b5;m from the scleral spur; AOD750 = angle opening distance at 750 &#x000b5;m from the scleral spur; ARA500 = angle recess area at 500 &#x000b5;m from the scleral spur; ARA750 = angle recess area at 750 &#x000b5;m from the scleral spur; TISA500 = trabecular-iris space area at 500 &#x000b5;m from the scleral spur; TISA750 = trabecular-iris space area at 750 &#x000b5;m from the scleral spur; SSA = scleral spur angle; IT750 = iris thickness at 750 &#x000b5;m from the scleral spur.</p></fn><fn id="tb1fn2"><label>*</label><p>Statistically significant at <italic toggle="yes">P</italic> = 0.05.</p></fn></table-wrap-foot></table-wrap><fig position="float" id="fig4"><label>Figure 4.</label><caption><p>Reconstructed images representing PAC and PACG eyes from selected mean values of latent variables of each group. Top row: mean values of <inline-formula><mml:math id="math-inTM0006d" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x0223c;</mml:mo><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, middle row: tripled mean values of <inline-formula><mml:math id="math-inTM0006e" overflow="scroll"><mml:mrow><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>&#x0223c;</mml:mo><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>6</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, bottom row: tripled mean values of <inline-formula><mml:math id="math-inTM0006f" overflow="scroll"><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math></inline-formula>. Left column: plain segmented images, middle column: difference map, right column: magnified difference map of the angle with approximate AOD500 marked with arrows. We can see that PACG eyes seem to have a narrower angle (top row, right column) which is more pronounced if we multiply latent variables by 3 (middle row, right column). However, there was no noticeable difference in the width of the angle regarding only <inline-formula><mml:math id="math-inTM0006g" overflow="scroll"><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math></inline-formula> (bottom row, right column). It implies that despite significant statistical difference between two disease groups in <inline-formula><mml:math id="math-inTM0006h" overflow="scroll"><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math></inline-formula>, narrower angle in PACG is not a direct result of <inline-formula><mml:math id="math-inTM0006i" overflow="scroll"><mml:msub><mml:mi>&#x003b7;</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math></inline-formula> but rather a result of combination all latent variables.</p></caption><graphic xlink:href="tvst-11-8-30-f004" position="float"/></fig></sec></sec><sec sec-type="discussion" id="sec4"><title>Discussion</title><p>Our model has successfully disentangled latent space with readily distinguishable main features for the first three latent variables: anterior chamber area for &#x003b7;<sub>1</sub>, the curvature of the cornea for &#x003b7;<sub>2</sub>, and pupil size for &#x003b7;<sub>3</sub>. &#x003b7;<sub>4</sub> and &#x003b7;<sub>5</sub> are more complex, with &#x003b7;<sub>4</sub> associated with at least three features: (1) pupil size, (2) curvature and thickness of the iris, and (3) the lens vault. The &#x003b7;<sub>5</sub> is the most complex, with combined changes of the iris profile, corneal profile, width of the anterior chamber, and depth of the peripheral anterior chamber. &#x003b7;<sub>6</sub> seems to be associated with corneal thickness (<xref rid="fig2" ref-type="fig">Figs. 2</xref> and <xref rid="fig3" ref-type="fig">3</xref>). Given that our model is unsupervised, good interpretability of certain latent variables (&#x003b7;<sub>1</sub>, &#x003b7;<sub>2</sub>, &#x003b7;<sub>3</sub>, and &#x003b7;<sub>6</sub>) is encouraging. However, some latent variables (&#x003b7;<sub>4</sub>, &#x003b7;<sub>5</sub>) are difficult to interpret, which implies complex interactions between AS-OCT features but also leaves room for further improvement of the model.</p><p>Comparing conventional measurements and latent variables of PAC and PACG eyes led to interesting results: there was no statistically significant difference in any of the conventional measurements, but the PACG eyes had smaller values of &#x003b7;<sub>4</sub> compared to PAC eyes (<italic toggle="yes">P</italic> = 0.015; <xref rid="tbl1" ref-type="table">Table</xref>). On the visualization of latent variables of the two groups, we noticed that PACG eyes seemed to have a narrower angle, which was more pronounced if the latent variables were multiplied by a factor of 3. However, despite the statistically significant difference of &#x003b7;<sub>4</sub> between disease groups, there was no noticeable difference in the width of the angle between groups if only &#x003b7;<sub>4</sub> was visualized (<xref rid="fig4" ref-type="fig">Fig. 4</xref>). Hence, if there is a difference in the angle between PAC and PACG eyes, it is not a direct result of &#x003b7;<sub>4</sub> but rather due to the combined action of all latent variables. Although current research is not sufficient to draw any conclusion regarding morphological differences between PAC and PACG eyes, such results are encouraging in that current research did not apply any disease labels during the training. Although we cannot derive any conclusion regarding the association between latent variables and disease mechanisms, we can postulate that there could be some complex interactions between conventional measurements that are not evident with conventional measurements but could be captured with deep learning techniques.</p><p>Compared to the convolutional VAE model we have presented in a previous study, our new COPIT-based model has shown similar but less blurred reconstructed images but vastly improved latent space disentanglement such that (1) every latent variable represents a unique feature, (2) features are easier to comprehend, and (3) other components of corneosclera, iris, and the lens remain relatively stable whereas the feature of the focus changes dramatically (<xref rid="fig2" ref-type="fig">Figs. 2</xref> and <xref rid="fig3" ref-type="fig">3</xref>). Such encouraging results could be achieved by implementing a design specifically tailored for the application. First, we have implemented sequential training, which provides some characteristics: (1) the latent variables are ordered such that the variance explained can be expected to be the largest for the first latent variable and decrease afterward, (2) the addition of cosine similarity to the loss function encourages latent factors are dissimilar, and (3) resulting latent space is easier to interpret to human eyes. Second, in our new model, every layer could be configured to have a different design. For example, we have separated the asymmetrical layer from the symmetrical layers, which can offer a clear advantage given that physiologically, no eye is symmetrical. Hence, we expect that separating the asymmetricity will help reduce confounding and enhance the extraction of clinically meaningful features in the symmetrical layers. Besides horizontal symmetricity, other restrictions and transformations such as affine transformation or thin-plate spline transformation are relatively easy to incorporate and can be applied on a per-layer basis if required.</p><p>Another useful characteristic of our new model compared to the convolutional VAE is better stability at extreme <italic toggle="yes">z</italic> values. This can be useful because scaling up the values of the latent variables can enhance the subtle changes to improve interpretability. However, after a certain point, the generated image becomes unnatural, limiting the range of usable <italic toggle="yes">z</italic> values. For example, at <italic toggle="yes">z</italic> values of &#x02212;4 and 4, which have been used in <xref rid="fig3" ref-type="fig">Figure&#x000a0;3</xref>, latent variables in the convolutional VAE generate somewhat broken images (<xref rid="fig5" ref-type="fig">Fig. 5</xref>).</p><fig position="float" id="fig5"><label>Figure 5.</label><caption><p>Latent variables of convolutional VAE presented in our previous paper at more extreme <italic toggle="yes">z</italic> values.</p></caption><graphic xlink:href="tvst-11-8-30-f005" position="float"/></fig><p>We believe that in the near future, deep learning techniques will be more commonly applied in the field of glaucoma, including AS-OCT image analysis. Many deep learning techniques involve the dimension reduction stage, which is related to the latent space. For example, deep clustering or longitudinal analyses can use latent space. For such strategies to be more effective, an understanding of latent structure is essential. We hope our study could promote understanding of the latent structure of AS-OCT images and provide a basis for future deep learning studies.</p><p>AS-OCT poses an important limitation to our analysis. Inadequate tissue resolution makes it difficult to delineate the exact border between the cornea and the iris and the location of the scleral spur in certain eyes with a closed angle. AS-OCT has limited penetration, limiting visualization of the posterior surface of the iris and the ciliary body, whereas in some eyes without cataracts, the lens is so transparent that some parts of the anterior capsule are not visualized. We expect those limitations to be overcome with newer technologies. The model also has limitations inherent in generative models, including a requirement for manual tailoring of the hyperparameters. In the process of tailoring, knowledge of developers about the subject, AS-OCT images, in our case, get involved. Because there is no universally accepted feature to qualitatively assess the degree of disentanglement for AS-OCT images, the process is highly subjective. The same limitation applies when interpreting the results, especially latent variables that capture complex interactions of various features that are difficult to interpret.</p><p>Both training and validation data were derived from the same dataset, which does not include various ethnic groups, and for a machine learning study, the sample size is small. Hence, we expect the generalizability of our analysis to be limited, and the results should be assumed to be dependent on a specific dataset we have used. Comparisons between PAC and PACG eyes have all the limitations inherent in the retrospective design of the study.</p><p>Nonetheless, we have shown that an unsupervised neural network can achieve good results for the analysis of the latent structure of the AS-OCT. Also, our results suggest that latent space analysis can be useful for capturing a combination of features not readily represented with conventional measurements due to their complex interactions.</p></sec><sec sec-type="supplementary-material"><title>Supplementary Material</title><supplementary-material id="tvst-11-8-30_s001" position="float" content-type="local-data"><caption><title>Supplement 1</title></caption><media xlink:href="tvst-11-8-30_s001.pdf" id="d64e1244" position="anchor"/></supplementary-material></sec></body><back><ack><title>Acknowledgments</title><p>Supported by a grant (2018-0500) from the Asan Institute for Life Sciences, Asan Medical Center, Seoul, South Korea.</p><p content-type="COI-statement">Disclosure: <bold>K. Shon,</bold> (N); <bold>K.R. Sung,</bold> (N); <bold>J. Kwak,</bold> (N); <bold>J.Y. Lee,</bold> (N); <bold>J.W. Shin,</bold> (N)</p></ack><ref-list><title>References</title><ref id="bib1"><label>1.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Triolo</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Barboni</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Savini</surname>
<given-names>G</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>The use of anterior-segment optical-coherence tomography for the assessment of the iridocorneal angle and its alterations: update and current evidence</article-title>. <source><italic toggle="yes">J Clin Med</italic></source>. <year>2021</year>; <volume>10</volume>: <fpage>231</fpage>.</mixed-citation></ref><ref id="bib2"><label>2.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Baek</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Sung</surname>
<given-names>KR</given-names></string-name>, <string-name><surname>Sun</surname>
<given-names>JH</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>A hierarchical cluster analysis of primary angle closure classification using anterior segment optical coherence tomography parameters</article-title>. <source><italic toggle="yes">Invest Ophthalmol Vis Sci</italic></source>. <year>2013</year>; <volume>54</volume>: <fpage>848</fpage>&#x02013;<lpage>853</lpage>.<pub-id pub-id-type="pmid">23299485</pub-id></mixed-citation></ref><ref id="bib3"><label>3.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kwon</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Sung</surname>
<given-names>KR</given-names></string-name>, <string-name><surname>Han</surname>
<given-names>S</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Subclassification of primary angle closure using anterior segment optical coherence tomography and ultrasound biomicroscopic parameters</article-title>. <source><italic toggle="yes">Ophthalmology</italic></source>. <year>2017</year>; <volume>124</volume>: <fpage>1039</fpage>&#x02013;<lpage>1047</lpage>.<pub-id pub-id-type="pmid">28385302</pub-id></mixed-citation></ref><ref id="bib4"><label>4.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Han</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Sung</surname>
<given-names>KR</given-names></string-name>, <string-name><surname>Lee</surname>
<given-names>KS</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Outcomes of laser peripheral iridotomy in angle closure subgroups according to anterior segment optical coherence tomography parameters</article-title>. <source><italic toggle="yes">Invest Ophthalmol Vis Sci</italic></source>. <year>2014</year>; <volume>55</volume>: <fpage>6795</fpage>&#x02013;<lpage>6801</lpage>.<pub-id pub-id-type="pmid">25249604</pub-id></mixed-citation></ref><ref id="bib5"><label>5.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lee</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Sung</surname>
<given-names>KR</given-names></string-name>, <string-name><surname>Na</surname>
<given-names>JH</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Dynamic changes in anterior segment (AS) parameters in eyes with primary angle closure (PAC) and PAC glaucoma and open-angle eyes assessed using as optical coherence tomography</article-title>. <source><italic toggle="yes">Invest Ophthalmol Vis Sci</italic></source>. <year>2012</year>; <volume>53</volume>: <fpage>693</fpage>&#x02013;<lpage>697</lpage>.<pub-id pub-id-type="pmid">22222269</pub-id></mixed-citation></ref><ref id="bib6"><label>6.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Kwon</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Sung</surname>
<given-names>KR</given-names></string-name>, <string-name><surname>Han</surname>
<given-names>S.</given-names></string-name></person-group>
<article-title>Long-term changes in anterior segment characteristics of eyes with different primary angle-closure mechanisms</article-title>. <source><italic toggle="yes">Am J Ophthalmol</italic></source>. <year>2018</year>; <volume>191</volume>: <fpage>54</fpage>&#x02013;<lpage>63</lpage>.<pub-id pub-id-type="pmid">29655644</pub-id></mixed-citation></ref><ref id="bib7"><label>7.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Moghimi</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Torkashvand</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Mohammadi</surname>
<given-names>M</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Classification of primary angle closure spectrum with hierarchical cluster analysis</article-title>. <source><italic toggle="yes">PLoS One</italic></source>. <year>2018</year>; <volume>13</volume>(<issue>7</issue>): <fpage>e0199157</fpage>.<pub-id pub-id-type="pmid">30036362</pub-id></mixed-citation></ref><ref id="bib8"><label>8.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Xu</surname>
<given-names>BY</given-names></string-name>, <string-name><surname>Chiang</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Pardeshi</surname>
<given-names>AA</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Deep neural network for scleral spur detection in anterior segment OCT images: the Chinese American eye study</article-title>. <source><italic toggle="yes">Transl Vis Sci Technol</italic></source>. <year>2020</year>; <volume>9</volume>(<issue>2</issue>): <fpage>1</fpage>&#x02013;<lpage>10</lpage>.</mixed-citation></ref><ref id="bib9"><label>9.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Wanichwecharungruang</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Kaothanthong</surname>
<given-names>N</given-names></string-name>, <string-name><surname>Pattanapongpaiboon</surname>
<given-names>W</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Deep learning for anterior segment optical coherence tomography to predict the presence of plateau iris</article-title>. <source><italic toggle="yes">Transl Vis Sci Technol</italic></source>. <year>2021</year>; <volume>10</volume>(<issue>1</issue>): <fpage>1</fpage>&#x02013;<lpage>10</lpage>.</mixed-citation></ref><ref id="bib10"><label>10.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Fu</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Baskaran</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Xu</surname>
<given-names>Y</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>A deep learning system for automated angle-closure detection in anterior segment optical coherence tomography images</article-title>. <source><italic toggle="yes">Am J Ophthalmol</italic></source>. <year>2019</year>; <volume>203</volume>: <fpage>37</fpage>&#x02013;<lpage>45</lpage>.<pub-id pub-id-type="pmid">30849350</pub-id></mixed-citation></ref><ref id="bib11"><label>11.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Hao</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Zhao</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Yan</surname>
<given-names>Q</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Angle-closure assessment in anterior segment OCT images via deep learning</article-title>. <source><italic toggle="yes">Med Image Anal</italic></source>. <year>2021</year>; <volume>69</volume>: <fpage>101956</fpage>.<pub-id pub-id-type="pmid">33550010</pub-id></mixed-citation></ref><ref id="bib12"><label>12.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Pham</surname>
<given-names>TH</given-names></string-name>, <string-name><surname>Devalla</surname>
<given-names>SK</given-names></string-name>, <string-name><surname>Ang</surname>
<given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Deep learning algorithms to isolate and quantify the structures of the anterior segment in optical coherence tomography images</article-title>. <source><italic toggle="yes">Br J Ophthalmol</italic></source>. <year>2021</year>; <volume>105</volume>: <fpage>1231</fpage>&#x02013;<lpage>1237</lpage>.<pub-id pub-id-type="pmid">32980820</pub-id></mixed-citation></ref><ref id="bib13"><label>13.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bollen</surname>
<given-names>KA.</given-names></string-name>
</person-group>
<article-title>Latent variable in psychology and social sciences</article-title>. <source><italic toggle="yes">Ann Rev Psychol</italic></source>. <year>2002</year>; <volume>53</volume>: <fpage>605</fpage>&#x02013;<lpage>634</lpage>.<pub-id pub-id-type="pmid">11752498</pub-id></mixed-citation></ref><ref id="bib14"><label>14.</label><mixed-citation publication-type="book">
<person-group person-group-type="author">
<string-name>
<surname>Turhan</surname>
<given-names>CG</given-names></string-name>, <string-name><surname>Bilge</surname>
<given-names>HS.</given-names></string-name></person-group>
<article-title>Recent trends in deep generative models: a review</article-title>. In: <source><italic toggle="yes">2018 3rd International Conference on Computer Science and Engineering (UBMK)</italic></source>. <publisher-name>IEEE</publisher-name>; <year>2018</year>: <fpage>574</fpage>&#x02013;<lpage>579</lpage>.</mixed-citation></ref><ref id="bib15"><label>15.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Shon</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Sung</surname>
<given-names>KR</given-names></string-name>, <string-name><surname>Kwak</surname>
<given-names>J</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Development of a &#x003b2;-variational autoencoder for disentangled latent space representation of anterior segment optical coherence tomography images</article-title>. <source><italic toggle="yes">Transl Vis Sci Technol</italic></source>. <year>2022</year>; <volume>11</volume>(<issue>2</issue>): <fpage>11</fpage>.</mixed-citation></ref><ref id="bib16"><label>16.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Jaderberg</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Simonyan</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Zisserman</surname>
<given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Spatial transformer networks</article-title>. <source><italic toggle="yes">Adv Neural Inf Process Syst</italic></source>. <year>2015</year>; <volume>2015</volume>: <fpage>2017</fpage>&#x02013;<lpage>2025</lpage>.</mixed-citation></ref><ref id="bib17"><label>17.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Ronneberger</surname>
<given-names>O</given-names></string-name>, <string-name><surname>Fischer</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Brox</surname>
<given-names>T.</given-names></string-name></person-group>
<article-title>U-Net: convolutional networks for biomedical image segmentation</article-title>. <source><italic toggle="yes">IEEE Access</italic></source>. <year>2015</year>; <volume>9</volume>: <fpage>16591</fpage>&#x02013;<lpage>16603</lpage>.</mixed-citation></ref><ref id="bib18"><label>18.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Jiang</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Sun</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Tagliasacchi</surname>
<given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Linearized multi-sampling for differentiable image transformation</article-title>. In: <source>Proceedings of the IEEE/CVF International Conference on Computer Vision</source>. <year>2019</year>: <fpage>2988</fpage>&#x02013;<lpage>2997</lpage>.</mixed-citation></ref><ref id="bib19"><label>19.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Kingma</surname>
<given-names>DP</given-names></string-name>, <string-name><surname>Welling</surname>
<given-names>M.</given-names></string-name></person-group>
<article-title>Auto-encoding variational bayes</article-title>. <source><italic toggle="yes">2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings</italic></source>. <year>2014</year>;(<issue>Ml</issue>): <fpage>1</fpage>&#x02013;<lpage>14</lpage>.</mixed-citation></ref><ref id="bib20"><label>20.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Kingma</surname>
<given-names>DP</given-names></string-name>, <string-name><surname>Welling</surname>
<given-names>M.</given-names></string-name></person-group>
<article-title>Auto-encoding variational Bayes</article-title>. <source><italic toggle="yes">2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings</italic></source>. <year>2014</year>: <fpage>1</fpage>&#x02013;<lpage>14</lpage>.</mixed-citation></ref><ref id="bib21"><label>21.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>van Dongen</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Enright</surname>
<given-names>AJ.</given-names></string-name></person-group>
<article-title>Metric distances derived from cosine similarity and Pearson and Spearman correlations</article-title>. arXiv preprint <ext-link xlink:href="http://doi.org/arXiv:1208.3145" ext-link-type="uri">arXiv:1208.3145</ext-link>. Accessed April 4, 2012.</mixed-citation></ref><ref id="bib22"><label>22.</label><mixed-citation publication-type="other">
<person-group person-group-type="author">
<string-name>
<surname>Higgins</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Matthey</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Pal</surname>
<given-names>A</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Beta-VAE: learning basic visual concepts with a constrained variational framework</article-title>. Available at: <ext-link xlink:href="https://openreview.net/forum?id=Sy2fzU9gl" ext-link-type="uri">https://openreview.net/forum?id=Sy2fzU9gl</ext-link>.</mixed-citation></ref><ref id="bib23"><label>23.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Sakata</surname>
<given-names>LM</given-names></string-name>, <string-name><surname>Lavanya</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Friedman</surname>
<given-names>DS</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Assessment of the scleral spur in anterior segment optical coherence tomography images</article-title>. <source><italic toggle="yes">Arch Ophthalmol</italic></source>. <year>2008</year>; <volume>126</volume>: <fpage>181</fpage>&#x02013;<lpage>185</lpage>.<pub-id pub-id-type="pmid">18268207</pub-id></mixed-citation></ref><ref id="bib24"><label>24.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Schindelin</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Arganda-Carreras</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Frise</surname>
<given-names>E</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Fiji: an open-source platform for biological-image analysis</article-title>. <source><italic toggle="yes">Nat Methods</italic></source>. <year>2012</year>; <volume>9</volume>: <fpage>676</fpage>&#x02013;<lpage>682</lpage>.<pub-id pub-id-type="pmid">22743772</pub-id></mixed-citation></ref><ref id="bib25"><label>25.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lee</surname>
<given-names>KS</given-names></string-name>, <string-name><surname>Sung</surname>
<given-names>KR</given-names></string-name>, <string-name><surname>Kang</surname>
<given-names>SY</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Residual anterior chamber angle closure in narrow-angle eyes following laser peripheral iridotomy: anterior segment optical coherence tomography quantitative study</article-title>. <source><italic toggle="yes">Jpn J Ophthalmol</italic></source>. <year>2011</year>; <volume>55</volume>: <fpage>213</fpage>&#x02013;<lpage>219</lpage>.<pub-id pub-id-type="pmid">21559907</pub-id></mixed-citation></ref><ref id="bib26"><label>26.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Lee</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Sung</surname>
<given-names>KR</given-names></string-name>, <string-name><surname>Na</surname>
<given-names>JH</given-names></string-name>, <etal>et al</etal>.</person-group>
<article-title>Dynamic changes in anterior segment (AS) parameters in eyes with primary angle closure (PAC) and PAC glaucoma and open-angle eyes assessed using as optical coherence tomography</article-title>. <source><italic toggle="yes">Invest Ophthalmol Vis Sci</italic></source>. <year>2012</year>; <volume>53</volume>: <fpage>693</fpage>&#x02013;<lpage>697</lpage>.<pub-id pub-id-type="pmid">22222269</pub-id></mixed-citation></ref><ref id="bib27"><label>27.</label><mixed-citation publication-type="journal">
<person-group person-group-type="author">
<string-name>
<surname>Bookstein</surname>
<given-names>FL.</given-names></string-name>
</person-group>
<article-title>Principal Warps: Thin-Plate Splines and the Decomposition of Deformations</article-title>. <source><italic toggle="yes">IEEE Trans Pattern Anal Mach Intell</italic></source>. <year>1989</year>; <volume>11</volume>: <fpage>567</fpage>&#x02013;<lpage>585</lpage>.</mixed-citation></ref></ref-list></back></article>
